### FILE: .\condensator.py ###
import os
from pathlib import Path

def write_codebase_to_file(root_dir, output_file):
    with open(output_file, 'w') as outfile:
        for subdir, _, files in os.walk(root_dir):
            for file in files:
                if file.endswith('.py'):
                    file_path = os.path.join(subdir, file)
                    print(file_path)
                    outfile.write(f"### FILE: {file_path} ###\n")
                    with open(file_path, 'r') as infile:
                        outfile.write(infile.read())
                        outfile.write("\n\n")

if __name__ == "__main__":
    root_directory = '.'  # Replace with the path to your codebase
    output_filename = 'condensed_codebase.txt'  # Replace with your desired output file name
    write_codebase_to_file(root_directory, output_filename)
    print(f"Codebase has been written to {output_filename}")

### FILE: .\docs\source\conf.py ###
# Configuration file for the Sphinx documentation builder.
#
# For the full list of built-in configuration values, see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html

# -- Project information -----------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information

from dotenv import load_dotenv
import os
import sys
sys.path.insert(0, os.path.abspath('../src'))
load_dotenv()

project = 'open-mimic-iii'
copyright = '2024, Amadou Wolfgang Cisse'
author = 'Amadou Wolfgang Cisse'
release = '-'

# -- General configuration ---------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration

extensions = [
    "sphinx.ext.autodoc",
    "sphinx.ext.napoleon",
    "sphinx.ext.viewcode",
]

templates_path = ['_templates']
exclude_patterns = [
]

exclude_patterns = ["../src/managers.py"]

# -- Options for HTML output -------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output

html_theme = 'sphinx_rtd_theme'
html_static_path = ['_static']


### FILE: .\entry_point\main.py ###
import os
import argparse
from src import CaseHandler

os.environ["DEBUG"] = "1"

if __name__ == "__main__":
    case = CaseHandler()
    parser = argparse.ArgumentParser(
        description='Run the MIMIC pipeline with configuration from the directory configuration.'
        ' The model configuration need to contain a data_config.json, '
        'pipeline_config.json and model_config.json')
    parser.add_argument('path',
                        type=str,
                        help='Absolute path to the directory containint the config.json.')
    args = parser.parse_args()
    case.run(args.path)
    #


### FILE: .\examples\example_utils.py ###
from utils import *
from datasets import ProcessedSetReader
from typing import Dict, List, Tuple
from examples.settings import YERVA_SPLIT
from datasets.readers import SplitSetReader


def print_split_info(reader: ProcessedSetReader, split_set: Dict[str, List[str]]):
    train_subjects = split_set["train"]
    val_subjects = split_set["val"]
    test_subjects = split_set["test"]
    train_percent = len(train_subjects) / len(reader.subject_ids) * 100
    val_percent = len(val_subjects) / len(reader.subject_ids) * 100
    test_percent = len(test_subjects) / len(reader.subject_ids) * 100

    max_len = max(len(train_subjects), len(val_subjects), len(test_subjects))
    width = len(str(max_len))

    info_io(f"Train: {len(train_subjects):<{width}} ({train_percent:0.2f}%) \n"
            f"Val:   {len(val_subjects):<{width}} ({val_percent:0.2f}%) \n"
            f"Test:  {len(test_subjects):<{width}} ({test_percent:0.2f}%)")


def benchmark_split_subjects() -> Tuple[List[int], List[int]]:
    print("Test and validation sets found...")
    test_subjects = pd.read_csv(Path(YERVA_SPLIT, "testset.csv"), header=None)
    test_subjects.columns = ["subjects", "affiliation"]
    test_subjects = test_subjects[test_subjects["affiliation"] == 1]["subjects"].astype(
        int).tolist()

    val_subjects = pd.read_csv(Path(YERVA_SPLIT, "valset.csv"), header=None)
    val_subjects.columns = ["subjects", "affiliation"]
    val_subjects = val_subjects[val_subjects["affiliation"] == 1]["subjects"].astype(int).tolist()
    return test_subjects, val_subjects


def benchmark_split_reader(reader: ProcessedSetReader, test_subjects: List[str],
                           val_subjects: List[str]) -> SplitSetReader:
    # Split data as in original set
    test_subjects = list(set(reader.subject_ids) & set(test_subjects))
    val_subjects = list(set(reader.subject_ids) & set(val_subjects))
    train_subjects = list(set(reader.subject_ids) - set(test_subjects) - set(val_subjects))

    split_sets = {"test": test_subjects, "val": val_subjects, "train": train_subjects}
    split_reader = SplitSetReader(reader.root_path, split_sets)

    # Print result
    print_split_info(reader, split_sets)
    return split_reader


### FILE: .\examples\settings.py ###
import os
from river import optim
from pathlib import Path
from dotenv import load_dotenv

__all__ = [
    "TEMP_DIR", "EXAMPLE_DIR", "EXAMPLE_DATA_DEMO", "EXAMPLE_DATA", "TEMP_DIR", "YERVA_SPLIT",
    "LOG_METRICS", "NETWORK_METRICS", "BENCHMARK_MODEL", "LOG_REG_PARAMS", "STANDARD_LSTM_PARAMS",
    "STANDARD_LSTM_DS_PARAMS", "CHANNEL_WISE_LSTM_PARAMS"
]

load_dotenv()

EXAMPLE_DIR = Path(os.getenv("EXAMPLES"))
EXAMPLE_DATA = Path(EXAMPLE_DIR, "data")
EXAMPLE_DATA_DEMO = Path(
    EXAMPLE_DATA,
    "physionet.org",
    "files",
    "mimiciii-demo",
    "1.4",
)
TEMP_DIR = Path(EXAMPLE_DATA, "temp")
YERVA_SPLIT = Path(EXAMPLE_DIR, "yerva_nn_benchmark", "data_split")
LOG_METRICS = {
    "PHENO": ["micro_roc_auc", "macro_roc_auc"],
    "DECOMP": ["roc_auc", "pr_auc", "classification_report"],
    "IHM": ["roc_auc", "pr_auc", "classification_report"],
    "LOS": ["cohen_kappa", "mae", "los_classification_report"]
}
LOG_REG_PARAMS = {
    "PHENO": {
        "l2": 0.1,
    },
    "DECOMP": {
        "l2": 0.001,
    },
    "IHM": {
        "l2": 0.001,
    },
    "LOS": {
        "optimizer": optim.SGD(lr=0.00001),
    }
}
NETWORK_METRICS = {
    "PHENO": ["micro_roc_auc", "macro_roc_auc"],
    "DECOMP": [
        "roc_auc",
        "pr_auc",
    ],
    "IHM": ["roc_auc", "pr_auc"],
    "LOS": ["cohen_kappa", "mae"]
}
STANDARD_LSTM_PARAMS = {
    "PHENO": {
        "model": {
            "input_dim": 59,
            "dropout": 0.3,
            "layer_size": 256,
            "depth": 1,
            "final_activation": "sigmoid",
            "output_dim": 25
        },
        "generator_options": {
            "batch_size": 8
        },
        "compile_options": {
            "loss": "binary_crossentropy",
            "optimizer": "adam"
        },
        "training": {
            "epochs": 100
        }
    },
    "DECOMP": {
        "model": {
            "input_dim": 59,
            "final_activation": "sigmoid",
            "layer_size": 128,
            "depth": 1
        },
        "compile_options": {
            "loss": "binary_crossentropy",
            "optimizer": "adam"
        },
        "generator_options": {
            "batch_size": 8,
        },
        "training": {
            "epochs": 100
        }
    },
    "IHM": {
        "model": {
            "input_dim": 59,
            "dropout": 0.3,
            "final_activation": "sigmoid",
            "output_dim": 1,
            "layer_size": 16,
            "depth": 2
        },
        "compile_options": {
            "loss": "binary_crossentropy",
            "optimizer": "adam"
        },
        "generator_options": {
            "batch_size": 8,
        },
        "training": {
            "epochs": 100
        }
    },
    "LOS": {
        "model": {
            "input_dim": 59,
            "dropout": 0.3,
            "layer_size": 64,
            "depth": 1,
            "final_activation": "softmax",  # if partition is none then relu
            "output_dim": 10  # if partition is none then only 1
        },
        "compile_options": {
            "loss":
                "sparse_categorical_crossentropy",  # mean_squared_logarithmic_error if partition is None
            "optimizer": "adam"
        },
        "generator_options": {
            "batch_size": 8,
            "partition":
                "custom"  # Can be custom, log and None and has effects on the bining of the target 
        },
        "training": {
            "epochs": 100
        }
    }
}

STANDARD_LSTM_DS_PARAMS = {
    "PHENO": {
        "dim": 256,
        "depth": 1,
        "batch_size": 8,
        "dropout": 0.3,
        "target_repl_coef": 0.5
    },
    "DECOMP": {
        "dim": 128,
        "depth": 1,
        "batch_size": 8,
        "dropout": 0.3,
        "deep_supervision": True
    },
    "IHM": {
        "dim": 32,
        "depth": 1,
        "batch_size": 8,
        "dropout": 0.3,
        "target_repl_coef": 0.5
    },
    "LOS": {
        "dim": 128,
        "depth": 1,
        "batch_size": 8,
        "dropout": 0.3,
        "partition": "custom",
        "deep_supervision": True
    }
}

CHANNEL_WISE_LSTM_PARAMS = {
    "PHENO": {
        "dim": 16,
        "depth": 1,
        "batch_size": 8,
        "dropout": 0.3,
        "size_coef": 8.0
    },
    "DECOMP": {
        "dim": 16,
        "depth": 1,
        "batch_size": 8,
        "size_coef": 4.0
    },
    "IHM": {
        "dim": 8,
        "depth": 1,
        "batch_size": 8,
        "dropout": 0.3,
        "size_coef": 4.0
    },
    "LOS": {
        "dim": 16,
        "depth": 1,
        "batch_size": 8,
        "size_coef": 8.0,
        "partition": "custom"
    }
}

BENCHMARK_MODEL = Path(EXAMPLE_DATA, "benchmark_models")


### FILE: .\examples\etc\convert_columns.py ###
import pandas as pd
from datasets.mimic_utils import upper_case_column_names
from tests.settings import *

for csv in TEST_DATA_DEMO.iterdir():
    if csv.is_dir() or csv.suffix != ".csv":
        continue

    df = pd.read_csv(csv,
                     dtype={
                         "ROW_ID": 'Int64',
                         "ICUSTAY_ID": 'Int64',
                         "HADM_ID": 'Int64',
                         "SUBJECT_ID": 'Int64',
                         "row_id": 'Int64',
                         "icustay_id": 'Int64',
                         "hadm_id": 'Int64',
                         "subject_id": 'Int64'
                     },
                     low_memory=False)
    df = upper_case_column_names(df)
    df.to_csv(csv, index=False)


### FILE: .\examples\yerva_nn_benchmark\__init__.py ###
import argparse
import datasets
import pandas as pd
from pathlib import Path
from datasets.readers import SplitSetReader
from typing import List
from utils.IO import *
from examples.settings import *
from examples.example_utils import benchmark_split_reader, benchmark_split_subjects
from examples.yerva_nn_benchmark.scripts.logistic_regression import run_log_reg
from examples.yerva_nn_benchmark.scripts.lstm import run_standard_lstm
# from .decomp import logistic_regression, lstm_channel_wise, lstm, river_models
# from .ihm import logistic_regression, lstm_channel_wise, lstm, river_models
# from .los import logistic_regression, lstm_channel_wise, lstm, river_models
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Creation of the original Yerva NN benchmark"
                                     "We also added some models for the fun of it :). "
                                     "Multitasking will be added during future maintenance.")
    parser.add_argument('--mimic_dir',
                        type=str,
                        default=EXAMPLE_DATA,
                        help="Path to your MIMIC-III dataset installation. "
                        "If not provided, the downloaded example dataset"
                        " will be used but the results will be invalid.")

    parser.add_argument('--tasks',
                        type=List[str],
                        default=["IHM", "LOS", "DECOMP", "PHENO"],
                        help="This is the list of tasks for which the benchmark is to be created. ")

    parser.add_argument(
        '--models',
        type=List[str],
        default=["logistic_regression", "lstm_channel_wise", "lstm", "river_models"],
        help="This is the list of models for which the benchmark is tÃ³ be created. ")

    args, _ = parser.parse_known_args()

    if not Path(YERVA_SPLIT, "testset.csv").is_file() or not Path(YERVA_SPLIT,
                                                                  "valset.csv").is_file():
        raise FileNotFoundError(
            f"The testset.csv or valset.csv file is missing. Please run the bash script "
            f"from examples/etc/setup.sh or .ps1"
            f"\nExpected location was {YERVA_SPLIT}")
    else:
        # These are fetched from the original github https://github.com/YerevaNN/mimic3-benchmarks
        test_subjects, val_subjects = benchmark_split_subjects()

    for task_name in args.tasks:
        info_io(f"Creating benchmark for task {task_name}")
        if not set(["lstm_channel_wise", "lstm"]) - set(args.models):
            reader = datasets.load_data(chunksize=75836,
                                        source_path=EXAMPLE_DATA_DEMO,
                                        storage_path=TEMP_DIR,
                                        discretize=True,
                                        time_step_size=1.0,
                                        start_at_zero=True,
                                        impute_strategy='previous',
                                        task=task_name)

            info_io(f"Splitting data for task {task_name}", level=0)

            split_reader = benchmark_split_reader(reader, test_subjects, val_subjects)

            if "lstm" in args.models:
                storage_path = Path(BENCHMARK_MODEL, task_name, "lstm")
                storage_path.mkdir(parents=True, exist_ok=True)

                run_standard_lstm(task_name=task_name,
                                  reader=split_reader,
                                  storage_path=storage_path,
                                  metrics=NETWORK_METRICS[task_name],
                                  params=STANDARD_LSTM_PARAMS[task_name])

        # Classical classifiers
        if False and not set(["logistic_regression", "river_models"]) - set(args.models):
            reader = datasets.load_data(chunksize=75836,
                                        source_path=EXAMPLE_DATA_DEMO,
                                        storage_path=TEMP_DIR,
                                        engineer=True,
                                        task=task_name)
            info_io(f"Splitting data for task {task_name}", level=0)

            split_reader = benchmark_split_reader(reader, test_subjects, val_subjects)

            if "logistic_regression" in args.models:
                storage_path = Path(BENCHMARK_MODEL, task_name, "logistic_regression")
                storage_path.mkdir(parents=True, exist_ok=True)

                run_log_reg(task_name=task_name,
                            reader=split_reader,
                            storage_path=storage_path,
                            metrics=LOG_METRICS[task_name],
                            params=LOG_REG_PARAMS[task_name])
            pass


### FILE: .\examples\yerva_nn_benchmark\scripts\logistic_regression.py ###
import datasets
from river import multiclass
from pathlib import Path
from datasets.readers import SplitSetReader
from models.stream.linear_model import LogisticRegression, LinearRegression, MultiOutputLogisticRegression
from pipelines.stream import RiverPipeline


def run_log_reg(task_name: str, reader: SplitSetReader, storage_path: Path, metrics: list,
                params: dict):
    if task_name == "LOS":
        run_linear_reg(reader=reader, storage_path=storage_path, metrics=metrics, params=params)
    elif task_name in ["DECOMP", "IHM"]:
        run_binary_log_reg(reader=reader, storage_path=storage_path, metrics=metrics, params=params)
    elif task_name == "PHENO":
        run_multioutput_log_reg(reader=reader,
                                storage_path=storage_path,
                                metrics=metrics,
                                params=params)


def run_binary_log_reg(reader: SplitSetReader, storage_path: Path, metrics: list, params: dict):
    model = LogisticRegression(metrics=metrics, **params)
    pipe = RiverPipeline(storage_path=storage_path, reader=reader, model=model)
    pipe.fit(no_subdirs=True)
    pipe.test()


def run_multioutput_log_reg(reader: SplitSetReader, storage_path: Path, metrics: list,
                            params: dict):
    model = MultiOutputLogisticRegression(metrics=metrics, **params)
    pipe = RiverPipeline(storage_path=storage_path, reader=reader, model=model)
    pipe.fit(no_subdirs=True)
    pipe.test()


def run_linear_reg(reader: SplitSetReader, storage_path: Path, metrics: list, params: dict):
    model = LinearRegression(metrics=metrics, **params)
    pipe = RiverPipeline(storage_path=storage_path, reader=reader, model=model)
    pipe.fit(no_subdirs=True)
    pipe.test()


### FILE: .\examples\yerva_nn_benchmark\scripts\lstm.py ###
from models.pytorch.lstm import LSTMNetwork
from tests.settings import *
from pipelines.pytorch import TorchPipeline
from pathlib import Path
from datasets.readers import SplitSetReader
from copy import deepcopy


def run_standard_lstm(task_name: str, reader: SplitSetReader, storage_path: Path, metrics: list,
                      params: dict):

    params = deepcopy(params)

    model = LSTMNetwork(**params.pop("model"))
    training_params = params.pop("training")
    params["compile_options"].update({"metrics": metrics})
    pipe = TorchPipeline(storage_path=storage_path, reader=reader, model=model,
                         **params).fit(no_subdirs=True, **training_params)


def run_binary_lstm(reader: SplitSetReader, storage_path: Path, metrics: list, params: dict):
    params = deepcopy(params)
    model = LSTMNetwork(**params.pop("model"))
    training_params = params.pop("training")
    if "compile_options" in params:
        params["compile_options"].update(params.pop("compile_options"))
    else:
        params["compile_options"] = params.pop("compile_options")
    pipe = TorchPipeline(storage_path=storage_path, reader=reader, model=model,
                         **params).fit(no_subdirs=True, **training_params)


def run_multilabel_lstm(reader: SplitSetReader, storage_path: Path, metrics: list, params: dict):
    ...


def run_regression_lstm(reader: SplitSetReader, storage_path: Path, metrics: list, params: dict):
    ...


### FILE: .\examples\yerva_nn_benchmark\scripts\lstm_channel_wise.py ###


### FILE: .\examples\yerva_nn_benchmark\scripts\river_models.py ###


### FILE: .\examples\yerva_nn_benchmark\scripts\__init__.py ###


### FILE: .\model_templates\__init__.py ###


### FILE: .\src\handlers.py ###


'''
import datasets
from datasets.readers import ProcessedSetReader
from pathlib import Path
from utils import load_json, write_json
from utils.IO import info_io, error_io
from sklearn.linear_model import LogisticRegression, SGDClassifier
from models.sklearn.standard.linear_models import StandardLogReg
from sklearn.ensemble import RandomForestClassifier
from models.tf2.lstm import LSTMNetwork
from models.tf2.logistic_regression import IncrementalLogReg as IncrementalLogRegTF2
from models.sklearn.incremental.linear_models import IncrementalLogRegSKLearn
from pipelines.nn import MIMICPipeline as MIMICNNPipeline
from pipelines.regression import MIMICPipeline as MIMICRegPipeline
from tensorflow.keras.metrics import AUC
from copy import deepcopy


class MultiCaseHandler(object):
    """_summary_
    """

    def __init__(self):
        """_summary_
        """
        pass

    def run(path: Path):
        """_summary_

        Args:
            path (Path): _description_
        """
        directories = list(path.iterdir())
        for directory in directories:
            case = CaseHandler()
            case.run(directory)


class AbstractCaseHandler(object):
    """_summary_
    """

    def __init__(self) -> None:
        """_summary_

        Args:
            case_folder (Path): _description_
        """
        #TODO! unused
        self.regression_models = ["sgd_classifier", "logistic_regression", "random_forest"]
        self.neural_network_models = ["lstm"]
        self.subcase_configs = list()

    def read_case_config(self, case_config: dict, case_folder: Path):
        """_summary_

        Args:
            case_folder (Path): _description_
        """
        # Constants
        name = case_config["name"]
        task = case_config["task"]
        frame_work = case_config["pipeline_config"]["framework"]
        model_type = case_config["model_type"]

        # Compose default pipeline config
        case_config["pipeline_config"].update({
            "model_name": name,
            "root_path": case_folder,
        })
        if task in ["IHM", "DECOMP"]:
            case_config["pipeline_config"]["output_type"] = "sparse"

        # Compose default data config
        case_config["data_config"] = deepcopy(case_config["data_config"])
        if not "extracted" == Path(case_config["data_config"]["storage_path"]).name:
            case_config["data_config"].update({
                "storage_path": str(Path(case_config["data_config"]["storage_path"], "extracted")),
                "task": task
            })

        # Compose defualt config
        default_config = {
            "name": name,
            "model_type": model_type,
            "data_config": case_config["data_config"],
            "model_config": case_config["model_config"],
            "pipeline_config": case_config["pipeline_config"],
        }

        case_config = self.custom_configs(default_config, case_config)

        return default_config, model_type, frame_work, task, name, case_folder

    def write_case_folder(self):
        """_summary_
        """

    def run(self, case_folder: Path):
        """_summary_

        Args:
            case_folder (Path): _description_
        """
        case_config_data = load_json(Path(case_folder, "config.json"))
        if "subcases" in case_config_data:
            for data in case_config_data["subcases"]:
                (config, \
                model_type, \
                framework, \
                task, \
                name, \
                root_path) = self.read_case_config(data, case_folder)

                info_io(f"Loading model config from: {str(Path(root_path, name))}")

                case_path = Path(root_path, name, task)
                case_path.mkdir(parents=True, exist_ok=True)

                write_json(Path(case_path, "config.json"), config)
                if model_type in self.regression_models:
                    self.regression(config, framework, model_type, task)
                elif model_type in self.neural_network_models:
                    self.neural_network(config, framework, model_type, task)
                else:
                    raise ValueError(
                        f"Model type needs to be in {', '.join(str(x) for x in list([*self.regression_models, *self.neural_network_models]))}"
                    )
        else:
            (config, \
            model_type, \
            framework, \
            task, \
            name, \
            root_path) = self.read_case_config(case_config_data, case_folder)

            info_io(f"Loading model config from: {str(Path(root_path))}")

            case_path = Path(root_path)
            case_path.mkdir(parents=True, exist_ok=True)

            write_json(Path(case_path, "config.json"), config)
            print(config)
            if model_type in self.regression_models:
                self.regression(config, framework, model_type, task)
            elif model_type in self.neural_network_models:
                self.neural_network(config, framework, model_type, task)
            else:
                raise ValueError(
                    f"Model type needs to be in {', '.join(str(x) for x in list([*self.regression_models, *self.neural_network_models]))}"
                )

            # except Exception as e:
            #    print(e)

    def regression(self, config, framework, model_type):
        """_summary_

        Raises:
            NotImplementedError: _description_
        """
        raise NotImplementedError()

    def neural_network(self, config, framework, model_type):
        """_summary_

        Raises:
            NotImplementedError: _description_
        """
        raise NotImplementedError()

    def custom_configs(self, config: dict) -> dict:
        return config


class CaseHandler(AbstractCaseHandler):
    """_summary_

    Args:
        AbstractCaseHandler (_type_): _description_
    """

    def __init__(self) -> None:
        """_summary_

        Args:
            case_folder (Path): _description_
        """
        super().__init__()

    def custom_configs(self, config: dict, config_json: dict) -> dict:
        """_summary_

        Args:
            config (dict): _description_
            config_json (dict): _description_

        Returns:
            dict: _description_
        """
        config["pipeline_config"]["task"] = config_json["task"]
        config["task"] = config_json["task"]

        return config

    def regression(self, config, framework, model_type, task):
        """_summary_
        """
        regression_models = {
            "tf2": {
                "logistic_regression": IncrementalLogRegTF2
            },
            "sklearn": {
                "incremental": {
                    "sgd_classifier": SGDClassifier,
                    "logistic_regression": IncrementalLogRegSKLearn,
                },
                "standard": {
                    "random_forest": RandomForestClassifier,
                    "logistic_regression": StandardLogReg
                }
            }
        }

        if framework == "sklearn":
            model_switch = regression_models["sklearn"]
            if task in ["IHM", "PHENO"]:
                model_switch = model_switch["standard"]
            else:
                model_switch = model_switch["incremental"]
            try:
                model = model_switch[model_type](task=task,
                                                 random_state=42,
                                                 **config["model_config"])
            except KeyError:
                raise ValueError(
                    f"For the framework {framework}, only the models \"{', '.join(str(x) for x in model_switch)}\" are available."
                )

        else:
            model_switch = regression_models["tf2"]
            try:
                model = model_switch[model_type](task=task,
                                                 input_dim=714,
                                                 random_state=42,
                                                 **config["model_config"])
            except KeyError:
                raise ValueError(
                    f"For the framework {framework}, only the models \"{', '.join(str(x) for x in model_switch)}\" are available."
                )
        config["data_config"].update({"preprocess": True, "engineer": True})

        self.run_case(MIMICRegPipeline(model, **config["pipeline_config"]), config["data_config"])

    def neural_network(self, config, framework, model_type):
        """_summary_
        """
        custom_objects = {"auc_2": AUC(curve='ROC'), "auc_3": AUC(curve='PR')}
        neural_network_models = {"lstm": LSTMNetwork}

        model = neural_network_models[model_type](**config["model_config"])
        config["data_config"].update({"preprocess": True})
        self.run_case(
            MIMICNNPipeline(model, custom_objects=custom_objects, **config["pipeline_config"]),
            config["data_config"])

    def run_case(self, pipeline, data_config):
        """_summary_

        Args:
            pipeline (_type_): _description_
        """
        try:
            if not "chunksize" in data_config.keys():
                timeseries, \
                episodic_data, \
                subject_diagnoses, \
                subject_icu_history = datasets.load_data(**data_config)

                pipeline.fit(timeseries=timeseries,
                             episodic_data=episodic_data,
                             subject_diagnoses=subject_diagnoses,
                             subject_icu_history=subject_icu_history)
            else:
                data_path = datasets.load_data(**data_config)
                pipeline.fit(data_path=data_path)
        except Exception as e:
            error_io("Encountered exception:")
            error_io(e)
            error_io("Case is finalized and shut down!")

'''

### FILE: .\src\managers.py ###
import shutil
import numpy as np
import tensorflow as tf
from pathlib import Path
from utils.IO import *
from utils import load_json, update_json
from tensorflow.keras import Model


class AbstractCheckpointManager(object):
    """_summary_
    """

    def __init__(self, directory):
        """_summary_
        """
        if isinstance(directory, str):
            self.directory = Path(directory)
        else:
            self.directory = directory

        self.custom_objects = []

    @property
    def latest(self):
        """_summary_
        """
        return self.latest_epoch()

    def load_model(self):
        """_summary_
        """
        model_path = Path(self.directory, f"cp-{self.latest:04d}.ckpt")
        info_io(f"Loading model from epoch {self.latest}.")
        model = tf.keras.models.load_model(model_path, self.custom_objects)

        return model

    def load_weights(self, model: Model):
        """_summary_

        Args:
            model (_type_): _description_
        """
        latest_cp_name = tf.train.latest_checkpoint(self.directory)
        model.load_weights(latest_cp_name)
        return model

    def latest_epoch(self):
        """_summary_

        Returns:
            _type_: _description_
        """
        raise NotImplementedError("This is an abstract class!")

    def clean_directory(self, best_epoch: int, keep_latest: bool = True):
        """_summary_

        Args:
            best_epoch (int): _description_
            keep_latest (bool, optional): _description_. Defaults to True.
        """
        raise NotImplementedError("This is an abstract class!")

    def is_empty(self):
        """_summary_
        """
        if self.latest == 0:
            return True
        return False


class CheckpointManager(AbstractCheckpointManager):
    """_summary_
    """

    def __init__(self, directory, train_epochs, custom_objects):
        """_summary_

        Args:
            directory (_type_): _description_
            train_epochs (_type_): _description_
            custom_objects (_type_): _description_
        """
        super().__init__(directory)
        self.epochs = train_epochs
        self.custom_objects = custom_objects

    def latest_epoch(self):
        """_summary_

        Returns:
            _type_: _description_
        """

        check_point_epochs = [
            i for i in range(self.epochs + 1) for folder in self.directory.iterdir()
            if f"{i:04d}" in folder.name
        ]

        if check_point_epochs:
            return max(check_point_epochs)

        return 0

    def clean_directory(self, best_epoch: int, keep_latest: bool = True):
        """_summary_

        Args:
            best_epoch (int): _description_
            keep_latest (bool, optional): _description_. Defaults to True.
        """

        [
            shutil.rmtree(folder)
            for i in range(self.epochs + 1)
            for folder in self.directory.iterdir()
            if f"{i:04d}" in folder.name and ((i != self.epochs) or not keep_latest) and
            (i != best_epoch) and (".ckpt" in folder.name)
        ]


class ReducedCheckpointManager(AbstractCheckpointManager):

    def __init__(self, directory):
        """_summary_

        Args:
            directory (_type_): _description_
        """
        super().__init__(directory)

    def latest_epoch(self):
        """_summary_

        Returns:
            _type_: _description_
        """
        items = [item for item in self.directory.iterdir()]
        check_point_epochs = [
            i for i in range(len(items)) for folder in items if f"{i:04d}" in folder.name
        ]

        if check_point_epochs:
            return max(check_point_epochs)

        return 0

    def clean_directory(self, best_epoch: int):
        """_summary_

        Args:
            best_epoch (int): _description_
            keep_latest (bool, optional): _description_. Defaults to True.
        """
        items = [item for item in self.directory.iterdir()]
        [
            shutil.rmtree(folder)
            for i in range(len(items))
            for folder in items
            if f"{i:04d}" in folder.name and (i != best_epoch) and (".ckpt" in folder.name)
        ]


class HistoryManager():
    """_summary_
    """

    def __init__(self, directory):
        """_summary_

        Args:
            directory (_type_): _description_
        """
        self.directory = directory
        self.history_file = Path(directory, "history.json")

    @property
    def history(self):
        self._history = load_json(self.history_file)
        return self._history

    @history.setter
    def history(self, value):
        self._history = value

    @property
    def best(self):
        """_summary_
        """
        if "val_loss" in self.history.keys():
            return min(self.history["val_loss"]), np.argmin(self.history["val_loss"]) + 1
        return None, None

    def update(self, items: dict):
        """_summary_v

        Args:
            items (dict): _description_
        """
        self._history = update_json(self.history_file, items)

    def is_finished(self):
        if "finished" in self.history.keys():
            return True
        return False

    def finished(self):
        """_summary_
        """
        self.update({'finished': True})

### FILE: .\src\settings.py ###
import json
import os
import bisect
import numpy as np
from pathlib import Path
from dotenv import load_dotenv
from utils.IO import *

load_dotenv(verbose=False)

__all__ = [
    'TASK_NAMES', 'DATASET_SETTINGS', 'DECOMP_SETTINGS', 'LOS_SETTINGS', 'PHENOT_SETTINGS',
    'IHM_SETTINGS', 'TEXT_METRICS'
]

TASK_NAMES = ["DECOMP", "LOS", "PHENO", "IHM"]
TEXT_METRICS = ["classification_report", "confusion_matrix"]

with Path(os.getenv("CONFIG"), "datasets.json").open() as file:
    DATASET_SETTINGS = json.load(file)
    DECOMP_SETTINGS = DATASET_SETTINGS["DECOMP"]
    LOS_SETTINGS = DATASET_SETTINGS["LOS"]
    PHENOT_SETTINGS = DATASET_SETTINGS["PHENO"]
    IHM_SETTINGS = DATASET_SETTINGS["IHM"]


### FILE: .\src\storable.py ###
"""Provides a storable dataclass with some custom dictionary operations for trackers.
Attempted thread safety but didn't work out too well. Use an external lock. 
Designed to work multiple processes.
"""
import shelve
import multiprocess as mp
from copy import deepcopy
from pathlib import Path
from typing import Any


class SimpleProperty(object):
    """
    A simple descriptor class for storing and retrieving property values.
    """

    def __init__(self, name: str, default: Any = 0):
        self._name = name
        self._default = default

    def __get__(self, instance, owner) -> Any:
        return instance._progress.get(self._name, self._default)

    def __set__(self, instance, value: Any):
        instance._progress[self._name] = value
        instance._write({self._name: value})


class FloatPropert(SimpleProperty):
    """
    A descriptor class for storing and retrieving float property values.
    """

    def __init__(self, name: str, default: float):
        super().__init__(name, default)

    def __get__(self, instance, owner) -> float:
        return super().__get__(instance, owner)

    def __set__(self, instance, value: float):
        super().__set__(instance, value)


class BoolProperty(SimpleProperty):
    """
    A descriptor class for storing and retrieving bool property values.
    """

    def __init__(self, name: str, default: bool):
        super().__init__(name, default)

    def __get__(self, instance, owner) -> bool:
        return super().__get__(instance, owner)

    def __set__(self, instance, value: bool):
        super().__set__(instance, value)


class IntProperty(SimpleProperty):
    """
    A descriptor class for storing and retrieving int property values.
    """

    def __init__(self, name: str, default: int):
        super().__init__(name, default)

    def __get__(self, instance, owner) -> int:
        return super().__get__(instance, owner)

    def __set__(self, instance, value: int):
        super().__set__(instance, value)


class ProxyDictionary(dict):
    """
    A dictionary subclass that allows for callback on modification.
    """

    def __init__(self,
                 name: str,
                 default: dict = None,
                 store_total: bool = False,
                 write_callback=None,
                 *args,
                 **kwargs):
        super().__init__(*args, **kwargs)
        self._name = name
        self._store_total = store_total
        self._on_modified = write_callback
        if store_total:
            if "total" not in default:
                # If the dict is created from scratch, total is created in the first update
                # After that it is assumed that total is correctly updated
                self._is_initial = False
                self["total"] = 0
            else:
                self._is_initial = True

            if not self._is_initial:
                self._on_modified(self._name, self, *args, **kwargs)
        self.update(default)
        self._is_initial = False
        return

    def _update_total(self, key, value):
        if self._store_total:
            if isinstance(value, dict):
                if not self._is_initial:
                    if key not in self or isinstance(self[key], (int, float)):
                        if len(value):
                            orig_value = self.get(key, 0)
                            super().__setitem__(key, dict())
                            self[key]["total"] = sum(
                                [stay_data for stay, stay_data in value.items() if stay != "total"])
                            self["total"] += self[key]["total"] - orig_value
                    elif len(value):
                        length = self[key].get("total", 0)
                        new_length = sum([
                            stay_data for stay, stay_data in self[key].items() if stay != "total"
                        ]) + length
                        self[key]["total"] = new_length
                        if not self._is_initial:
                            self["total"] += new_length - length

            elif isinstance(value, (int, float)):
                if not (key == "total" or self._is_initial):
                    orig_value = self.get(key, 0)
                    self["total"] += value - orig_value

    def __iadd__(self, other, *args, **kwargs):
        if set(other.keys()) - set(self.keys()):
            raise ValueError(
                f"Keys in the dictionary must match to __iadd__! stored_keys: {list(self.keys())}; added_keys: {list(other.keys())}"
            )
        for key, value in other.items():
            self._update_total(key, value)
        for key in other.keys():
            self[key] += other[key]
        if self._on_modified:
            self._on_modified(self._name, self, *args, **kwargs)
        return self

    def __setitem__(self, key, value):
        self._update_total(key, value)
        if self._store_total and isinstance(value, dict) and key in self:
            self[key].update(value)
        else:
            super().__setitem__(key, value)
        if self._on_modified:
            self._on_modified(self._name, self)
        return

    def __delitem__(self, key):
        super().__delitem__(key)
        if self._on_modified:
            self._on_modified(self._name, self)

    def update(self, other, *args, **kwargs):
        """
        Update the dictionary with the values from another dictionary.

        Args:
            *args: Variable length argument list.
            **kwargs: Arbitrary keyword arguments.
        """
        for key, value in other.items():
            if key in self and isinstance(self[key], dict) and isinstance(value, dict):
                self._update_total(key, value)
                self._recursive_update(self[key], value)
            else:
                self[key] = value

        if self._on_modified:
            self._on_modified(self._name, self)
        return

    def _recursive_update(self, dictionary, update):
        """
        Recursively update a dictionary with the values from another dictionary.

        Args:
            d (dict): The dictionary to update.
            u (dict): The dictionary with the new values.
        """
        for key, value in update.items():
            if isinstance(dictionary.get(key), dict) and isinstance(update[key], dict):
                self._update_total(key, value)
                self._recursive_update(dictionary[key], update[key])
            else:
                dictionary[key] = update[key]


class ProxyList(list):
    """
    A list subclass that allows for callback on modification.
    """

    def __init__(self, name, initial=None, write_callback=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._name = name
        if initial is None:
            initial = []
        self._on_modified = write_callback
        self.extend(initial)

    def append(self, item):
        super().append(item)
        if self._on_modified:
            self._on_modified(self._name, self)

    def extend(self, items):
        super().extend(items)
        if self._on_modified:
            self._on_modified(self._name, self)

    def insert(self, index, item):
        super().insert(index, item)
        if self._on_modified:
            self._on_modified(self._name, self)

    def remove(self, item):
        super().remove(item)
        if self._on_modified:
            self._on_modified(self._name, self)

    def pop(self, index=-1):
        item = super().pop(index)
        if self._on_modified:
            self._on_modified(self._name, self)
        return item

    def clear(self):
        super().clear()
        if self._on_modified:
            self._on_modified(self._name, self)

    def __setitem__(self, index, value):
        super().__setitem__(index, value)
        if self._on_modified:
            self._on_modified(self._name, self)

    def __delitem__(self, index):
        super().__delitem__(index)
        if self._on_modified:
            self._on_modified(self._name, self)


class ListProperty:
    """
    A descriptor class for storing and retrieving list property values.
    """

    def __init__(self, name: str, default=None, write_callback=None):
        self._name = name
        self._write_callback = write_callback
        self._default = ProxyList(name=name, initial=default, write_callback=write_callback)

    def __get__(self, instance, owner):
        if instance is not None:
            if self._name not in instance.__dict__:
                instance.__dict__[self._name] = ProxyList(
                    name=self._name,
                    initial=self._default,
                    write_callback=lambda n, x: self.__set__(instance, x))
            return instance.__dict__[self._name]
        else:
            return self._default

    def __set__(self, instance, value):
        if isinstance(value, list):
            value = ProxyList(name=self._name, initial=value, write_callback=self._write_callback)
        instance.__dict__[self._name] = value
        if self._write_callback:
            self._write_callback(self._name, value)


class DictionaryProperty(object):
    """
    A descriptor class for storing and retrieving dictionary property values.
    """

    def __init__(self, name: str, default: dict = {}, store_total=False, write_callback=None):
        self._name = name
        self._store_total = store_total
        self._write_callback = (write_callback if store_total else None)
        self._default = ProxyDictionary(self._name,
                                        default,
                                        write_callback=write_callback,
                                        store_total=self._store_total)

    def __get__(self, instance, owner) -> dict:
        if instance is not None:
            instance._progress[self._name] = instance._read(self._name)
            return ProxyDictionary(self._name,
                                   instance._progress.get(self._name, self._default),
                                   write_callback=lambda n, x: self.__set__(instance, x),
                                   store_total=self._store_total)
        elif owner is not None:
            return deepcopy(owner._originals.get(self._name, self._default))

    def __set__(self, instance, value: dict):
        if isinstance(value, ProxyDictionary):
            value = dict(value)
        elif self._store_total:
            value = dict(
                ProxyDictionary(self._name,
                                value,
                                store_total=True,
                                write_callback=self._write_callback))
        instance._progress[self._name] = value
        instance._write({self._name: value})


def storable(cls):
    """
    A class decorator that adds persistence functionality to a class.

    Args:
        cls: The class to decorate.

    Returns:
        The decorated class.
    """
    # When you think about it this is highly illegal but the only way of keeping them cls attributes unchanged between instantiations.
    # Find a better way if you can, you are the real hero.
    originals = {
        name: deepcopy(attr)
        for name, attr in vars(cls).items()
        if not name.startswith("_") and isinstance(attr, (int, float, bool, dict, list, type(None)))
    }
    setattr(cls, '_originals', originals)
    original_init = cls.__init__

    def __init__(self, *args, **kwargs):
        """
        Initialize the storable class.

        Args:
            *args: Variable length argument list.
            **kwargs: Arbitrary keyword arguments.
        """
        self._lock = mp.Lock()
        self._progress = {}
        for name, original_value in cls._originals.items():
            setattr(cls, name, deepcopy(original_value))

        if "storage_path" in kwargs:
            self._path = Path(kwargs.pop('storage_path'))
        elif args:
            self._path = Path(args[0])
            args = tuple(args[1:])
        else:
            raise ValueError(
                "No storage path provided to storable. Either provide as first positional argument or with the storage_path keyword argument."
            )

        self._path.parent.mkdir(exist_ok=True, parents=True)

        self._save_frequency = kwargs.pop('save_frequency', 1)
        self._access_count = 0

        # Load or initialize progress
        if Path(self._path.parent, f"{self._path.name}.dat").is_file():
            self._progress = self._read()
            self._wrap_attributes()
        else:
            self._wrap_attributes()
            self._write(self._progress)

        original_init(self, *args, **kwargs)

    def _write(self, items: dict):
        """
        Write the current state of the progress to the file.

        Args:
            items (dict): The dictionary containing the progress items.
        """
        if self._lock is not None:
            self._lock.acquire()
        self._access_count += 1
        with shelve.open(str(self._path)) as db:
            for key, value in items.items():
                # Make sure no Property types are written back
                if isinstance(value, ProxyDictionary):
                    db[key] = dict(value)
                elif isinstance(value, ProxyList):
                    db[key] = list(value)
                else:
                    db[key] = value
        if self._lock is not None:
            self._lock.release()

    def _read(self, key=None):
        """
        Read the progress from the file.

        Returns:
            dict: The progress dictionary.
        """
        if self._lock is not None:
            self._lock.acquire()

        with shelve.open(str(self._path)) as db:

            def _read_value(key):
                value = db[key]
                default_value = getattr(cls, key)
                # These types are miscast by shelve
                if isinstance(default_value, bool):
                    return bool(value)
                elif isinstance(default_value, int):
                    return int(value)
                else:
                    return value

            if key is None:
                ret = dict()
                for key in db.keys():
                    ret[key] = _read_value(key)
            else:
                ret = _read_value(key)

        if self._lock is not None:
            self._lock.release()

        return ret

    def __setstate__(self, state):
        self.__dict__.update(state)
        self._wrap_attributes()

    cls.__setstate__ = __setstate__
    cls.__init__ = __init__
    cls._write = _write
    cls._read = _read

    # Attribute wrapping logic
    def _wrap_attributes(self):
        """
        Wrap the attributes of the class with the appropriate descriptors.
        """

        for name, attr in vars(cls).items():
            if (isinstance(attr, (int, float, bool, dict, list)) or
                    attr is None) and not name.startswith("_"):
                attr = self._progress.get(name, attr)

                def _write_callback(name, value):
                    # Read before you _write like github
                    self._progress[name] = value
                    self._write({name: value})

                if isinstance(attr, dict):
                    # Store total is a mess but necessary for preprocessing trackers
                    store_total = getattr(cls, "_store_total", False)
                    setattr(
                        cls, name,
                        DictionaryProperty(name, attr, store_total, write_callback=_write_callback))
                elif isinstance(attr, list):
                    setattr(cls, name, ListProperty(name, attr, write_callback=_write_callback))
                elif isinstance(attr, bool):
                    setattr(cls, name, BoolProperty(name, attr))
                elif isinstance(attr, float):
                    setattr(cls, name, FloatPropert(name, attr))
                else:
                    setattr(cls, name, IntProperty(name, attr))
                self._progress[name] = attr

    cls._wrap_attributes = _wrap_attributes

    return cls


### FILE: .\src\visualization.py ###
import matplotlib.pyplot as plt
import os
import numpy as np
import pandas as pd
import pdb
import seaborn as sn
import tensorflow as tf
from pathlib import Path
from utils import make_prediction_vector


def plot_fourier_transform(series, years):
    """_summary_

    Args:
        series (_type_): _description_
    """
    fast_fourier_transform = tf.signal.rfft(series)
    f_per_dataset = np.arange(0, len(fast_fourier_transform))

    f_per_year = f_per_dataset / years
    print(max(np.abs(fast_fourier_transform)))
    # pdb.set_trace()
    plt.step(f_per_year, np.abs(fast_fourier_transform))
    plt.xscale('log')
    plt.ylim(0, max(np.abs(fast_fourier_transform)) / 2)
    plt.xlim([0.1, max(plt.xlim())])
    plt.xticks([1, 12, 52.2, 365.2524, 365.2524 * 2, 365.2524 * 4, 365.2524 * 24],
               labels=['1/Year', '1/Month', '1/Week', '1/day', '1/12', '1/6', '1/hour'])
    _ = plt.xlabel('Frequency (log scale)')
    plt.title("Fast Fourier Transform")


def _subplot_generator(data_df, name, features, layout, type, save=False):
    """
    This function generates subplots of the specified type with the specified parameters.

    Parameters:
        data_df:    data frame containing the data which is to be plotted.
        name:       name under which to save the plot
        features:   columns from the frame which are to be plotted
        layout:     of the plot in rows x columns
        type:       plot type
    
    Returns:
        fig:    matplotlib generated figure obj
        axs:    obj 
    """

    if layout[0] * layout[1] < len(features):
        print(
            f"Layout not valid, there are {len(features)} columns within the data frame and only {layout[0] * layout[1]} subplots!"
        )
        return

    if not name:
        name = "Subplots"
    if layout[1] > len(features):
        ncols = len(features)
    else:
        ncols = layout[1]
    nrows = int(np.ceil(len(features) / layout[1]))
    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 7, nrows * 3.6))
    for index, feature in enumerate(features):
        rows = int(np.floor(index / layout[1]))
        cols = index % layout[1]
        if nrows == ncols == 1:
            axs.plot(data_df[feature])
            axs.set_title(feature)
        elif nrows == 1:
            getattr(axs[cols], type)(data_df[feature])
            axs[cols].set_title(feature)
        else:
            getattr(axs[rows, cols], type)(data_df[feature])
            axs[rows, cols].set_title(feature)

    if save:
        fig.savefig(Path(plot_location, f"{name}.png"), dpi=200)

    return fig, axs


def plot(data_df, name="", features=None, subplots=True, layout=(27, 2), save=False):
    """
    Plots against index.

    Parameters:
        data_df:    data frame containing the data which is to be plotted.
        name:       name under which to save the plot
        features:   columns from the frame which are to be plotted
        layout:     of the plot in rows x columns
    
    Returns:
        fig:    matplotlib generated figure obj
        axs:    obj 
    """
    plt.clf()
    # implement something to adjust layout ratio if fewer features where passed
    if isinstance(data_df, pd.core.series.Series):
        features = [data_df.name]
        is_series = True
        subplots = False
    elif not features:
        features = data_df.columns

    if subplots:
        return _subplot_generator(data_df, name, features, layout, 'plot')

    else:
        for feature in features:
            plt.clf()
            if is_series:
                time_serie = data_df
            else:
                time_serie = data_df[feature].copy()
            time_serie.plot(figsize=(15, 10))
            if save:
                plt.savefig(Path(plot_location, f"{name}_{feature}.png"), dpi=200)

    return plt.gcf()


def acorr(data_df, name="", features=None, subplots=True, layout=(27, 2), maxlags=100, save=False):
    """
    Autocorrelation plots.

    Parameters:
        data_df:    data frame containing the data which is to be plotted.
        name:       name under which to save the plot
        features:   columns from the frame which are to be plotted
        layout:     of the plot in rows x columns
    
    Returns:
        fig:    matplotlib generated figure obj
        axs:    obj 
    """

    plt.clf()

    if subplots:
        if not features:
            features = data_df.columns
        return _subplot_generator(data_df, name, features, layout, 'acorr')
    else:
        if not features:
            features = data_df.columns

        for feature in features:
            plt.acorr(data_df[feature], maxlags=maxlags)

            if save:
                plt.savefig(Path(plot_location, f"{feature}.png"), dpi=200)

            plt.clf()

        return None


def hist(data_df, name="", features=None, subplots=True, layout=(27, 2), save=False):
    """
    Histogram plots.

    Parameters:
        data_df:    data frame containing the data which is to be plotted.
        name:       name under which to save the plot
        features:   columns from the frame which are to be plotted
        layout:     of the plot in rows x columns
    
    Returns:
        fig:    matplotlib generated figure obj
        axs:    obj 
    """
    plt.clf()
    if not features:
        features = data_df.columns
    if subplots:
        return _subplot_generator(data_df, name, features, layout, 'hist')
    else:
        if not features:
            features = data_df.columns

        for feature in features:
            plt.hist(data_df[feature])
            if save:
                plt.savefig(Path(plot_location, f"{feature}.png"), dpi=200)
            plt.clf()

        return None


def corrMatrix(data_df, features=None):
    """
    Correlation plots.

    Parameters:
        data_df:    data frame containing the data which is to be plotted.
        features:   columns from the frame which are to be plotted
    
    Returns:
        plt:    matplotlib generated pyplot obj
    """
    plt.clf()
    if not features:
        features = data_df.columns

    data = data_df[features]
    corrMatrix = data.corr()
    size = (np.max([0.4 * len(features), 5]), np.max([0.4 * len(features), 5]))
    fig, axs = plt.subplots(1, 1, figsize=size)
    axs = sn.heatmap(corrMatrix, annot=False)
    plt.subplots_adjust(bottom=0.35)
    plt.subplots_adjust(left=0.45)
    plt.savefig(Path(plot_location, "corrMatrix.png"), dpi=200)
    return plt.gcf()


def make_sample_plot(model, generator, folder=None, batches=20, title="", bin_averages=None):
    """
    """
    y_pred, y_true = make_prediction_vector(model=model,
                                            generator=generator,
                                            batches=batches,
                                            bin_averages=bin_averages)

    fig, ax = plt.subplots()
    pd.DataFrame(y_true, columns=['y_true']).plot(ax=ax, ylabel="load")
    pd.DataFrame(y_pred.reshape(-1, 1), columns=['y_pred']).plot(color="r", ax=ax)

    if title:
        plt.title(title)

    if folder:
        plt.savefig(Path(folder, f"{title}_sample.png"))

    return fig


def make_sample_subplot(model,
                        generators,
                        titles=None,
                        folder=None,
                        batches=20,
                        title="",
                        bin_averages=None,
                        layout=None):
    """
    """
    if not isinstance(generators, list):
        generators = [generators]

    if not layout:
        layout = (len(generators), 1)

    if not titles:
        titles = [None] * len(generators)

    fig, ax = plt.subplots(*layout)
    plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.4)

    for index, (generator, title) in enumerate(zip(generators, titles)):
        cur_ax = get_ax(ax, index, layout)

        y_pred, y_true = make_prediction_vector(model=model,
                                                generator=generator,
                                                batches=batches,
                                                bin_averages=bin_averages)

        pd.DataFrame(y_true.reshape(-1, 1), columns=['y_true']).plot(ax=cur_ax, ylabel="load")
        pd.DataFrame(y_pred.reshape(-1, 1), columns=['y_pred']).plot(color="r", ax=cur_ax)

        if title:
            cur_ax.set_title(title)

    if folder:
        plt.savefig(Path(folder, f"{title}_sample.png"))

    return fig


def get_ax(ax, index, layout):
    """_summary_

    Args:
        ax (_type_): _description_
        index (_type_): _description_
        layout (_type_): _description_
    """
    if layout[1] == 1:
        rows = int(np.floor(index / layout[1]))
        return ax[rows]
    else:
        rows = int(np.floor(index / layout[1]))
        cols = index % layout[1]
        return ax[rows, cols]


def make_history_plot(history, folder=None, title=None, train_key="loss", val_key="val_loss"):
    """
    """
    fig, ax = plt.subplots()
    pd.DataFrame(history[train_key], columns=[train_key]).plot(ax=ax, ylabel="y_true")
    if val_key in history.keys():
        pd.DataFrame(history[val_key], columns=[val_key]).plot(color="r", ax=ax, ylabel="y_pred")

    if title:
        plt.title(title)

    if folder:
        plt.savefig(Path(folder, "loss.png"))

    return plt


def make_history_plots(history, train_keys, val_keys, folder=None, titles=None):
    """
    """
    layout = (len(train_keys), 1)

    fig, ax = plt.subplots(*layout)
    plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.4)

    if not titles:
        titles = [None] * len(train_keys)

    for index, (train_key, val_key, title) in enumerate(zip(train_keys, val_keys, titles)):
        cur_ax = get_ax(ax, index, layout)
        pd.DataFrame(history[train_key], columns=[train_key]).plot(ax=cur_ax, ylabel="y_true")
        pd.DataFrame(history[val_key], columns=[val_key]).plot(color="r",
                                                               ax=cur_ax,
                                                               ylabel="y_pred")

        if title:
            cur_ax.set_title(title)

    if folder:
        plt.savefig(Path(folder, "loss.png"))

    return fig


### FILE: .\src\datasets\discretizing.py ###
import random
import os
import pandas as pd
from copy import deepcopy
from itertools import chain
from typing import Dict
from pathlib import Path
from utils.IO import *
from utils import dict_subset
from pathos.multiprocessing import cpu_count, Pool
from datasets.processors.discretizers import MIMICDiscretizer
from .trackers import PreprocessingTracker
from .readers import ExtractedSetReader, ProcessedSetReader
from .mimic_utils import copy_subject_info

__all__ = ["compact_discretization", "iterative_discretization"]


def compact_discretization(X_subject: Dict[str, Dict[str, pd.DataFrame]],
                           y_subject: Dict[str, Dict[str, pd.DataFrame]],
                           task: str,
                           subject_ids: list = None,
                           num_subjects: int = None,
                           time_step_size: float = 1.0,
                           impute_strategy: str = "previous",
                           mode: str = "legacy",
                           start_at_zero: bool = True,
                           eps: float = 1e-6,
                           storage_path: Path = None,
                           source_path: Path = None) -> Dict[str, Dict[str, pd.DataFrame]]:
    """_summary_

    Args:
        timeseries (pd.DataFrame): _description_
        episodic_data (pd.DataFrame): _description_
        subject_diagnoses (pd.DataFrame): _description_
        subject_icu_history (pd.DataFrame): _description_

    Returns:
        _type_: _description_
    """

    tracker = PreprocessingTracker(storage_path=Path(storage_path, "progress"),
                                   num_subjects=num_subjects,
                                   subject_ids=subject_ids,
                                   time_step_size=time_step_size,
                                   impute_strategy=impute_strategy,
                                   start_at_zero=start_at_zero,
                                   mode=mode)

    copy_subject_info(source_path, storage_path)

    if tracker.is_finished:
        info_io(f"Compact discretization finalized in directory:\n{str(storage_path)}")
        if num_subjects is not None:
            subject_ids = random.sample(tracker.subject_ids, k=num_subjects)
        return ProcessedSetReader(root_path=storage_path, subject_ids=subject_ids,
                                  set_index=False).read_samples(read_ids=True)

    info_io(f"Compact Discretization: {task}", level=0)
    discretizer = MIMICDiscretizer(task=task,
                                   storage_path=storage_path,
                                   tracker=tracker,
                                   time_step_size=time_step_size,
                                   impute_strategy=impute_strategy,
                                   start_at_zero=start_at_zero,
                                   mode=mode,
                                   eps=eps,
                                   verbose=True)

    subject_ids, excluded_subject_ids = get_subject_ids(num_subjects=num_subjects,
                                                        subject_ids=subject_ids,
                                                        all_subjects=X_subject.keys())
    missing_subjects = 0
    if num_subjects is not None:
        X_discretized = dict()
        y_discretized = dict()
        while not len(X_discretized) == num_subjects:
            curr_X_subject = dict_subset(X_subject, subject_ids)
            curr_y_subject = dict_subset(y_subject, subject_ids)

            X, y = discretizer.transform(curr_X_subject, curr_y_subject)
            X_discretized.update(X)
            y_discretized.update(y)
            it_missing_subjects = set(X.keys()) - set(subject_ids)
            subject_ids, excluded_subject_ids = get_subject_ids(num_subjects=num_subjects -
                                                                len(X_discretized),
                                                                subject_ids=None,
                                                                all_subjects=excluded_subject_ids)
            if it_missing_subjects:
                missing_subjects += len(it_missing_subjects)
                debug_io(f"Missing subjects are: {*it_missing_subjects,}")
            if not subject_ids:
                break
            if len(X_discretized) == num_subjects:
                debug_io(f"Missing {len(X_discretized) - num_subjects} subjects.")
                debug_io(f"Unprocessable subjects are: {*it_missing_subjects,}")

    else:
        X_subject = dict_subset(X_subject, subject_ids)
        y_subject = dict_subset(y_subject, subject_ids)

        (X_discretized, y_discretized) = discretizer.transform(X_subject, y_subject)
    if storage_path is not None:
        discretizer.save_data()
        info_io(f"Finalized discretization for {task} in directory:\n{str(storage_path)}")
    else:
        info_io(f"Finalized discretization for {task}.")
    # TODO! this doesn't work, reimagine
    # if missing_subjects:
    #     warn_io(f"The subject target was not reached, missing {missing_subjects} subjects.")
    tracker.is_finished = True
    return {"X": X_discretized, "y": y_discretized}


def iterative_discretization(reader: ExtractedSetReader,
                             task: str,
                             subject_ids: list = None,
                             num_subjects: int = None,
                             time_step_size: float = 1.0,
                             impute_strategy: str = "previous",
                             mode: str = "legacy",
                             start_at_zero: bool = True,
                             eps: float = 1e-6,
                             storage_path: Path = None) -> ProcessedSetReader:
    """_summary_

    Args:
        timeseries (pd.DataFrame): _description_
        episodic_data (pd.DataFrame): _description_
        subject_diagnoses (pd.DataFrame): _description_
        subject_icu_history (pd.DataFrame): _description_

    Returns:
        _type_: _description_
    """
    original_subject_ids = deepcopy(subject_ids)
    tracker = PreprocessingTracker(storage_path=Path(storage_path, "progress"),
                                   num_subjects=num_subjects,
                                   subject_ids=subject_ids,
                                   time_step_size=time_step_size,
                                   impute_strategy=impute_strategy,
                                   start_at_zero=start_at_zero,
                                   mode=mode)

    copy_subject_info(reader.root_path, storage_path)

    discretizer = MIMICDiscretizer(reader=reader,
                                   task=task,
                                   storage_path=storage_path,
                                   tracker=tracker,
                                   time_step_size=time_step_size,
                                   impute_strategy=impute_strategy,
                                   start_at_zero=start_at_zero,
                                   mode=mode,
                                   eps=eps,
                                   verbose=False)

    if tracker.is_finished:
        info_io(f"Data discretization for {task} is already in directory:\n{str(storage_path)}.")
        if num_subjects is not None:
            subject_ids = random.sample(tracker.subject_ids, k=num_subjects)
        return ProcessedSetReader(storage_path, subject_ids=subject_ids, set_index=False)

    info_io(f"Iterative Discretization: {task}", level=0)
    info_io(f"Discretizing for task {task}.")

    # Tracking info
    n_discretizer_subjects = len(tracker.subject_ids)
    n_discretizer_stays = len(tracker.stay_ids)
    n_discretizer_samples = tracker.samples

    # Parallel processing logic
    # discretizer_pr = discretizer

    def discretize_subject(subject_id: str):
        """_summary_
        """
        _, tracking_infos = discretizer_pr.transform_subject(subject_id)

        if tracking_infos:
            discretizer_pr.save_data([subject_id])
            return subject_id, tracking_infos

        return subject_id, None

    def init(discretizer: MIMICDiscretizer):
        global discretizer_pr
        discretizer_pr = discretizer

    subject_ids, excluded_subject_ids = get_subject_ids(num_subjects=num_subjects,
                                                        subject_ids=subject_ids,
                                                        all_subjects=discretizer.subjects,
                                                        discretizer_subjects=tracker.subject_ids)
    # for subject_id in subject_ids:
    #     discretize_subject(subject_id)
    info_io(f"Discretizing task data:\n"
            f"Discretize subjects: {n_discretizer_subjects}\n"
            f"Discretize stays: {n_discretizer_stays}\n"
            f"Discretize samples: {n_discretizer_samples}\n"
            f"Skipped subjects: {0}")

    # Start the run
    with Pool(cpu_count() - 1, initializer=init, initargs=(discretizer,)) as pool:
        res = pool.imap_unordered(discretize_subject, subject_ids, chunksize=500)

        empty_subjects = 0
        missing_subjects = 0
        while True:
            try:
                subject_id, tracker_data = next(res)
                if tracker_data is None:
                    empty_subjects += 1
                    # Add new samples if to meet the num subjects target
                    if num_subjects is None:
                        continue
                    debug_io(f"Missing subject is: {subject_id}")
                    try:
                        subj = excluded_subject_ids.pop()
                        res = chain(res, [pool.apply_async(discretize_subject, args=(subj,)).get()])
                    except IndexError:
                        missing_subjects += 1
                        debug_io(
                            f"Could not replace missing subject. Excluded subjects is: {excluded_subject_ids}"
                        )
                else:
                    n_discretizer_subjects += 1
                    n_discretizer_stays += len(tracker_data) - 1
                    n_discretizer_samples += tracker_data["total"]

                info_io(
                    f"Discretizing timeseries data:\n"
                    f"Discretized subjects: {n_discretizer_subjects}\n"
                    f"Discretized stays: {n_discretizer_stays}\n"
                    f"Discretized samples: {n_discretizer_samples}\n"
                    f"Skipped subjects: {empty_subjects}",
                    flush_block=(True and not int(os.getenv("DEBUG", 0))))
            except StopIteration as e:
                tracker.is_finished = True
                info_io(f"Finalized for task {task} in directory:\n{str(storage_path)}")
                if num_subjects is not None and missing_subjects:
                    warn_io(
                        f"The subject target was not reached, missing {missing_subjects} subjects.")
                break

    if original_subject_ids is not None:
        original_subject_ids = list(set(original_subject_ids) & set(tracker.subject_ids))
    return ProcessedSetReader(storage_path, subject_ids=original_subject_ids, set_index=False)


def get_subject_ids(num_subjects: int,
                    subject_ids: list,
                    all_subjects: list,
                    discretizer_subjects: list = list()):
    remaining_subject_ids = list(set(all_subjects) - set(discretizer_subjects))
    # Select subjects to process logic
    n_processed_subjects = len(discretizer_subjects)
    if num_subjects is not None:
        num_subjects = max(num_subjects - n_processed_subjects, 0)
        selected_subjects_ids = random.sample(remaining_subject_ids, k=num_subjects)
        remaining_subject_ids = list(set(remaining_subject_ids) - set(selected_subjects_ids))
        random.shuffle(remaining_subject_ids)
    elif subject_ids is not None:
        unknown_subjects = set(subject_ids) - set(all_subjects)
        if unknown_subjects:
            warn_io(f"Unknown subjects: {*unknown_subjects,}")
        selected_subjects_ids = list(set(subject_ids) & set(all_subjects))
        remaining_subject_ids = list(set(remaining_subject_ids) - set(selected_subjects_ids))
    else:
        selected_subjects_ids = remaining_subject_ids
    return selected_subjects_ids, remaining_subject_ids


### FILE: .\src\datasets\mimic_utils.py ###
"""
Collection of utility functions for the MIMIC-III dataset extraction process.

YerevaNN/mimic3-benchmarks

Functions
---------
- copy_subject_info(source_path, storage_path)
    Copy subject information from source path to storage path.
- get_samples_per_df(event_frames, num_samples)
    Get the number of samples per DataFrame based on the specified number of samples.
- convert_dtype_value(value, dtype)
    Convert a value to the specified dtype.
- convert_dtype_dict(dtypes, add_lower=True)
    Convert a dictionary of column names to dtype strings to a dictionary of column names to dtype objects.
- clean_chartevents_util(chartevents)
    Clean the chartevents DataFrame based on the timeseries column.
- get_static_value(timeseries, variable)
    Get the first non-null value of a specified variable from the time series data.
- upper_case_column_names(frame)
    Convert the column names to upper case for consistency.
- convert_to_numpy_types(frame)
    Convert the dtypes to numpy types for consistency.
- read_varmap_csv(resource_folder)
    Read the variable map CSV file from the specified resource folder.
- _clean_height(df)
    Convert height from inches to centimeters.
- _clean_systolic_bp(df)
    Filter out systolic blood pressure only.
- _clean_diastolic_bp(df)
    Filter out diastolic blood pressure only.
- _clean_capilary_rr(df)
    Categorize capillary refill rate.
- _clean_fraction_inspired_o2(df)
    Map fraction of inspired oxygen values to correct scale.
- _clean_laboratory_values(df)
    Clean laboratory values by removing non-numeric entries.
- _clean_o2sat(df)
    Scale oxygen saturation values to correct range.
- _clean_temperature(df)
    Map Fahrenheit temperatures to Celsius.
- _clean_weight(df)
    Convert weight values to kilograms.
- _clean_respiratory_rate(df)
    Transform respiratory rate values from greater than 60 to 60.
"""

import numpy as np
import pandas as pd
import os
import re
import shutil
from pathlib import Path
from typing import Dict
from utils.IO import *
from settings import *


def copy_subject_info(source_path: Path, storage_path: Path):
    """
    Copy subject information from source path to storage path.

    Parameters
    ----------
    source_path : Path
        The path to the source directory containing the subject information file.
    storage_path : Path
        The path to the target directory where the subject information file will be copied.
    """
    if source_path is None:
        if storage_path is not None:
            warn_io("No source path provided for subject information. Skipping copy.")
        return
    if not storage_path.is_dir():
        storage_path.mkdir(parents=True, exist_ok=True)
    source_file = Path(source_path, "subject_info.csv")
    target_file = Path(storage_path, "subject_info.csv")
    shutil.copy(str(source_file), str(target_file))


def get_samples_per_df(event_frames: Dict[str, pd.DataFrame], num_samples: int):
    """
    Get the number of samples per DataFrame based on the specified number of samples.

    Parameters
    ----------
    event_frames : Dict[str, pd.DataFrame]
        A dictionary where the keys are event types and the values are DataFrames containing event data.
    num_samples : int
        The total number of samples to distribute across the DataFrames.

    Returns
    -------
    sampled_dfs : Dict[str, pd.DataFrame]
        A dictionary with the same keys as `event_frames`, where each value is a sampled DataFrame.
    subject_events_per_df : Dict[str, int]
        A dictionary with the number of events per DataFrame.
    samples_per_df : Dict[str, int]
        A dictionary with the number of samples allocated to each DataFrame.
    """
    total_length = sum(len(df) for df in event_frames.values())
    samples_per_df = {
        event_types: int((len(df) / total_length) * num_samples)
        for event_types, df in event_frames.items()
    }

    # Adjust for rounding errors if necessary (simple method shown here)
    samples_adjusted = num_samples - sum(samples_per_df.values())
    for name in samples_per_df:
        if samples_adjusted <= 0:
            break
        samples_per_df[name] += 1
        samples_adjusted -= 1

    sampled_dfs = {
        event_types: event_frames[event_types][event_frames[event_types]["CHARTTIME"].isin(
            event_frames[event_types]["CHARTTIME"].unique()[:samples])]
        if len(event_frames[event_types]) >= samples else event_frames[event_types]
        for event_types, samples in samples_per_df.items()
    }

    subject_events_per_df = {
        event_types: len(samples) for event_types, samples in sampled_dfs.items()
    }

    if not sum([len(frames) for frames in sampled_dfs.values()]):
        raise RuntimeError(
            "Sample limit compliance subsampling produced empty dataframe. Source code is erroneous!"
        )

    return sampled_dfs, subject_events_per_df, samples_per_df


def convert_dtype_value(value, dtype: str):
    """
    Convert a value to the specified dtype.

    Parameters
    ----------
    value : any
        The value to be converted.
    dtype : str
        The target data type. Dtype can be one of "Int8", "Int16", "Int32", "Int64", "str", "float".

    Returns
    -------
    any
        The value converted to the specified data type.
    """
    dtype_mapping = {
        "Int8": np.int8,
        "Int16": np.int16,
        "Int32": np.int32,
        "Int64": np.int64,
        "str": str,
        "float": float,
        "float64": np.float64,
        "float32": np.float32,
        "object": lambda x: x
    }
    return dtype_mapping[dtype](value)


def convert_dtype_dict(dtypes: dict, add_lower=True) -> dict:
    """
    Convert a dictionary of column names to dtype strings to a dictionary of column names to dtype objects.

    Parameters
    ----------
    dtypes : dict
        Column name to dtype mapping. Dtype can be one of "Int8", "Int16", "Int32", "Int64", "str", "float".
    add_lower : bool, optional
        Whether to add lower case column names as well, by default True.

    Returns
    -------
    dict
        A dictionary with column name to dtype object mapping.
    """
    dtype_mapping = {
        "Int8": pd.Int8Dtype(),
        "Int16": pd.Int16Dtype(),
        "Int32": pd.Int32Dtype(),
        "Int64": pd.Int64Dtype(),
        "str": pd.StringDtype(),
        "float": float,
        "float64": pd.Float64Dtype(),
        "float32": pd.Float32Dtype(),
        "object": "object"
    }
    dtype_dict = {
        column: dtype_mapping[type_identifyer] for column, type_identifyer in dtypes.items()
    }
    if add_lower:
        dtype_dict.update({
            column.lower(): dtype_mapping[type_identifyer]
            for column, type_identifyer in dtypes.items()
        })
    return dtype_dict


def clean_chartevents_util(chartevents: pd.DataFrame):
    """
    Clean the chartevents DataFrame based on the timeseries column.

    Parameters
    ----------
    chartevents : pd.DataFrame
        DataFrame containing chart events.

    Returns
    -------
    pd.DataFrame
        Cleaned DataFrame.
    """
    function_switch = DATASET_SETTINGS["CHARTEVENTS"]["clean"]
    for variable_name, function_identifier in function_switch.items():
        index = (chartevents.VARIABLE == variable_name)
        try:
            chartevents.loc[index, 'VALUE'] = globals()[function_identifier](chartevents.loc[index])
        except Exception as exp:
            print("Exception in clean_events function", function_identifier, ": ", exp)
            print("number of rows:", np.sum(index))
            print("values:", chartevents.loc[index])
            raise exp

    return chartevents.loc[chartevents.VALUE.notnull()]


def get_static_value(timeseries: pd.DataFrame, variable: str):
    """
    Get the first non-null value of a specified variable from the time series data.

    Parameters
    ----------
    timeseries : pd.DataFrame
        DataFrame containing the time series data.
    variable : str
        The variable to get the static value for.

    Returns
    -------
    any
        The first non-null value of the specified variable.
    """

    index = timeseries[variable].notnull()

    if index.any():
        loc = np.where(index)[0][0]
        return timeseries[variable].iloc[loc]

    return np.nan


def upper_case_column_names(frame: pd.DataFrame) -> pd.DataFrame:
    """
    Convert the column names to upper case for consistency.

    Parameters
    ----------
    frame : pd.DataFrame
        Target DataFrame.

    Returns
    -------
    pd.DataFrame
        DataFrame with upper-cased column names.
    """
    frame.columns = frame.columns.str.upper()

    return frame


def convert_to_numpy_types(frame: pd.DataFrame) -> pd.DataFrame:
    """
    Convert the dtypes to numpy types for consistency.

    Parameters
    ----------
    frame : pd.DataFrame
        Target DataFrame.

    Returns
    -------
    pd.DataFrame
        DataFrame with numpy dtypes.
    """
    for col in frame.columns:
        # Convert pandas "Int64" (or similar) types to "int64"
        if pd.api.types.is_integer_dtype(frame[col]):
            frame[col] = frame[col].astype('int64', errors='ignore')
        # Convert pandas "boolean" type to NumPy "bool"
        elif pd.api.types.is_bool_dtype(frame[col]):
            frame[col] = frame[col].astype('bool', errors='ignore')
        # Convert pandas "string" type to NumPy "object"
        elif pd.api.types.is_string_dtype(frame[col]):
            frame[col] = frame[col].astype('object', errors='ignore')
        # Convert pandas "Float64" (or similar) types to "float64"
        elif pd.api.types.is_float_dtype(frame[col]):
            frame[col] = frame[col].astype('float64', errors='ignore')
    return frame


def _clean_height(df: pd.DataFrame) -> pd.Series:
    """
    Convert height from inches to centimeters.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing height values.

    Returns
    -------
    pd.Series
        Series with converted height values.
    """
    value = df.VALUE.astype(float).copy()

    def get_measurment_type(string):
        """
        """
        return 'in' in string.lower()

    index = df.VALUEUOM.fillna('').apply(get_measurment_type) | df.MIMIC_LABEL.apply(
        get_measurment_type)
    value.loc[index] = np.round(value[index] * 2.54)
    return value


def _clean_systolic_bp(df: pd.DataFrame) -> pd.Series:
    """
    Filter out systolic blood pressure only.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing blood pressure values.

    Returns
    -------
    pd.Series
        Series with systolic blood pressure values.
    """
    value = df.VALUE.astype(str).copy()
    index = value.apply(lambda string: '/' in string)
    value.loc[index] = value[index].apply(lambda string: re.match('^(\d+)/(\d+)$', string).group(1))
    return value.astype(float)


def _clean_diastolic_bp(df: pd.DataFrame) -> pd.Series:
    """
    Filter out diastolic blood pressure only.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing blood pressure values.

    Returns
    -------
    pd.Series
        Series with diastolic blood pressure values.
    """
    value = df.VALUE.astype(str).copy()
    index = value.apply(lambda string: '/' in string)
    value.loc[index] = value[index].apply(lambda string: re.match('^(\d+)/(\d+)$', string).group(2))
    return value.astype(float)


def _clean_capilary_rr(df: pd.DataFrame) -> pd.Series:
    """
    Categorize capillary refill rate.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing capillary refill rate values.

    Returns
    -------
    pd.Series
        Series with categorized capillary refill rate values.
    """
    df = df.copy()
    value = pd.Series(np.zeros(df.shape[0]), index=df.index).copy()
    value.loc[:] = np.nan

    df['VALUE'] = df.VALUE.astype(str)

    value.loc[(df.VALUE == 'Normal <3 secs') | (df.VALUE == 'Brisk')] = 0
    value.loc[(df.VALUE == 'Abnormal >3 secs') | (df.VALUE == 'Delayed')] = 1
    return value


def _clean_fraction_inspired_o2(df: pd.DataFrame) -> pd.Series:
    """
    Map fraction of inspired oxygen values to correct scale.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing fraction of inspired oxygen values.

    Returns
    -------
    pd.Series
        Series with mapped fraction of inspired oxygen values.
    """
    value = df.VALUE.astype(float).copy()

    # Check wheather value is string
    is_str = np.array(map(lambda x: type(x) == str, list(df.VALUE)), dtype=bool)

    def get_measurment_type(string):
        """
        torr is equal to mmHg
        """
        return 'torr' not in string.lower()

    index = df.VALUEUOM.fillna('').apply(get_measurment_type) & (is_str | (~is_str & (value > 1.0)))

    value.loc[index] = value[index] / 100.

    return value


def _clean_laboratory_values(df: pd.DataFrame) -> pd.Series:
    """
    Clean laboratory values by removing non-numeric entries.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing laboratory values.

    Returns
    -------
    pd.Series
        Series with cleaned laboratory values.
    """
    value = df.VALUE.copy()
    index = value.apply(
        lambda string: type(string) is str and not re.match('^(\d+(\.\d*)?|\.\d+)$', string))
    value.loc[index] = np.nan
    return value.astype(float)


def _clean_o2sat(df: pd.DataFrame) -> pd.Series:
    """
    Scale oxygen saturation values to correct range.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing oxygen saturation values.

    Returns
    -------
    pd.Series
        Series with scaled oxygen saturation values.
    """
    # change "ERROR" to NaN
    value = df.VALUE.copy()
    index = value.apply(
        lambda string: type(string) is str and not re.match('^(\d+(\.\d*)?|\.\d+)$', string))
    value.loc[index] = np.nan
    value = value.astype(float)

    # Scale values
    index = (value <= 1)
    value.loc[index] = value[index] * 100.
    return value


def _clean_temperature(df: pd.DataFrame) -> pd.Series:
    """
    Map Fahrenheit temperatures to Celsius.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing temperature values.

    Returns
    -------
    pd.Series
        Series with converted temperature values.
    """
    value = df.VALUE.astype(float).copy()

    def get_measurment_type(string):
        """
        """
        return 'F' in string

    index = df.VALUEUOM.fillna('').apply(get_measurment_type) | df.MIMIC_LABEL.apply(
        get_measurment_type) | (value >= 79)
    value.loc[index] = (value[index] - 32) * 5. / 9
    return value


def _clean_weight(df: pd.DataFrame) -> pd.Series:
    """
    Convert weight values to kilograms.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing weight values.

    Returns
    -------
    pd.Series
        Series with converted weight values.
    """
    value = df.VALUE.astype(float).copy()

    def get_measurment_type(string):
        """
        """
        return 'oz' in string

    # ounces
    index = df.VALUEUOM.fillna('').apply(get_measurment_type) | df.MIMIC_LABEL.apply(
        get_measurment_type)
    value.loc[index] = value[index] / 16.

    def get_measurment_type(string):
        """
        """
        return 'lb' in string

    # pounds
    index = index | df.VALUEUOM.fillna('').apply(get_measurment_type) | df.MIMIC_LABEL.apply(
        get_measurment_type)
    value.loc[index] = value[index] * 0.453592
    return value


def _clean_respiratory_rate(df: pd.DataFrame) -> pd.Series:
    """
    Transform respiratory rate values from greater than 60 to 60.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing respiratory rate values.

    Returns
    -------
    pd.Series
        Series with cleaned respiratory rate values.
    """
    value = df.VALUE
    value = value.replace('>60/min retracts', 60)
    value = value.replace('>60/minute', 60)

    return value


def read_varmap_csv(resource_folder: Path):
    """
    Read the variable map CSV file from the specified resource folder.

    Parameters
    ----------
    resource_folder : Path
        The path to the resource folder containing the CSV file.

    Returns
    -------
    pd.DataFrame
        DataFrame containing the variable map.
    """
    csv_settings = DATASET_SETTINGS["varmap"]
    # Load the resource map
    varmap_df = pd.read_csv(Path(resource_folder, "itemid_to_variable_map.csv"),
                            index_col=None,
                            dtype=convert_dtype_dict(csv_settings["dtype"]))

    # Impute empty to string
    varmap_df = varmap_df.fillna('').astype(str)

    # Cast columns
    varmap_df['COUNT'] = varmap_df.COUNT.astype(int)
    varmap_df['ITEMID'] = varmap_df.ITEMID.astype(int)

    # Remove unlabeled and not occuring phenotypes and make sure only variables with ready status
    varmap_df = varmap_df.loc[(varmap_df['LEVEL2'] != '') & (varmap_df['COUNT'] > 0)]
    varmap_df = varmap_df.loc[(varmap_df.STATUS == 'ready')]

    # Get subdf
    varmap_df = varmap_df[['LEVEL2', 'ITEMID', 'MIMIC LABEL']].set_index('ITEMID')
    name_equivalences = {'LEVEL2': 'VARIABLE', 'MIMIC LABEL': 'MIMIC_LABEL'}
    varmap_df = varmap_df.rename(columns=name_equivalences)

    return varmap_df


### FILE: .\src\datasets\readers.py ###
"""
Dataset Reader Module
=====================

This module provides classes and methods for reading and handling dataset files. 

Classes
-------

- AbstractReader: A base reader class for datasets, providing methods to handle and sample subject directories.
- ExtractedSetReader: A reader for extracted datasets, providing methods to read various types of data including timeseries, episodic data, events, diagnoses, and ICU history.
- ProcessedSetReader: A reader for processed datasets, providing methods to read samples and individual subject data.
- EventReader: A reader for event data from CHARTEVENTS, OUTPUTEVENTS, LABEVENTS, providing methods to read data either in chunks or in a single shot.
- SplitSetReader: A reader for datasets split into training, validation, and test sets, providing access to each split.

References
----------
- YerevaNN/mimic3-benchmarks: https://github.com/YerevaNN/mimic3-benchmarks
"""
import random
import re
import os
import threading
import pandas as pd
import numpy as np
from pathlib import Path
from collections.abc import Iterable
from copy import deepcopy
from utils.IO import *
from settings import *
from .mimic_utils import upper_case_column_names, convert_dtype_dict, read_varmap_csv
from .trackers import ExtractionTracker
from typing import List, Union, Dict

__all__ = ["ExtractedSetReader", "ProcessedSetReader", "EventReader", "SplitSetReader"]


class AbstractReader(object):
    """
    A base reader class for datasets, providing methods to handle and sample subject directories.

    Parameters
    ----------
    root_path : Path
        The root directory path containing subject folders.
    subject_ids : list of int, optional
        List of subject IDs to read. If None, reads all subject directories in the root_path.

    Raises
    ------
    ValueError
        If the specified subject IDs do not have existing directories.

    Examples
    --------
    >>> root_path = Path("/path/to/data")
    >>> reader = AbstractReader(root_path, subject_ids=[10006, 10011, 10019])
    >>> reader.subject_ids
    [10006, 10011, 10019]
    """

    def __init__(self, root_path: Path, subject_ids: List[int] = None) -> None:
        self._root_path = (root_path if isinstance(root_path, Path) else Path(root_path))

        if subject_ids is None:
            self._subject_folders = [
                folder for folder in self._root_path.iterdir()
                if folder.is_dir() and folder.name.isnumeric()
            ]
            self._update_self = True
        elif not subject_ids:
            warn_io("List of subjects passed to mimic dataset reader is empty!")
            self._update_self = False
            self._subject_folders = []
        else:
            self._update_self = False
            if all([Path(str(folder)).is_dir() for folder in subject_ids]):
                self._subject_folders = subject_ids
            elif all([Path(self._root_path, str(folder)).is_dir() for folder in subject_ids]):
                self._subject_folders = [
                    Path(self._root_path, str(folder)) for folder in subject_ids
                ]
            else:
                raise ValueError(
                    f"The following subject do not have existing directories: "
                    f"{*[ Path(str(folder)).name for folder in subject_ids if not (Path(self._root_path, str(folder)).is_dir() or Path(str(folder)).is_dir())],}"
                )

    def _update(self):
        """
        Update the list of subject folders. Does not update if subject IDs were specified on creation.
        """
        # Doesn't update if subject_ids specified on creation
        if self._update_self:
            self._subject_folders = [
                folder for folder in self._root_path.iterdir()
                if folder.is_dir() and folder.name.isnumeric()
            ]

    def _cast_dir_path(self, dir_path: Union[Path, str, int]) -> Path:
        """
        Cast the directory path to a Path object and ensure it is relative to the root path.
        """
        if isinstance(dir_path, int):
            dir_path = Path(str(dir_path))
        elif isinstance(dir_path, str):
            dir_path = Path(dir_path)
        if not dir_path.is_relative_to(self._root_path):
            dir_path = Path(self._root_path, dir_path)
        return dir_path

    def _cast_subject_ids(self, subject_ids: Union[List[str], List[int], np.ndarray]) -> List[int]:
        """
        Cast the subject IDs to a list of integers.
        """
        if subject_ids is None:
            return None
        return [int(subject_id) for subject_id in subject_ids]

    def _sample_ids(self, subject_ids: list, num_subjects: int, seed: int = 42):
        """
        Sample a specified number of subject IDs.
        """
        # Subject ids overwrites num subjects
        random.seed(seed)
        self._update()
        if subject_ids is not None:
            return subject_ids
        if num_subjects is not None:
            random.seed(seed)
            return random.sample(self.subject_ids, num_subjects)
        return self._subject_folders

    @property
    def root_path(self) -> Path:
        """
        Get the root directory path.

        Returns
        -------
        Path
            The root directory path.

        Examples
        --------
        >>> reader.root_path
        PosixPath('/path/to/data')
        """
        return self._root_path

    @property
    def subject_ids(self) -> List[int]:
        """
        Get the list of subject IDs either past as parameter or located in the directory.

        Returns
        -------
        List[int]
            The list of subject IDs.

        Examples
        --------
        >>> reader.subject_ids
        [10006, 10011, 10019]
        """
        return [int(folder.name) for folder in self._subject_folders]

    def _init_returns(self, file_types: tuple, read_ids: bool = True):
        """
        Initialize a dictionary or list to store the data to be read, depending on read IDs.
        """
        return {file_type: {} if read_ids else [] for file_type in file_types}


class ExtractedSetReader(AbstractReader):
    """
    A reader for extracted datasets, providing methods to read various types of data including
    timeseries, episodic data, events, diagnoses, and ICU history.

    Parameters
    ----------
    root_path : Path
        The root directory path containing subject folders.
    subject_ids : list of int, optional
        List of subject IDs to read. If None, reads all subjects in the root_path.
    num_samples : int, optional
        Number of samples to read. If None, reads all available samples.

    Examples
    --------
    >>> root_path = Path("/path/to/data")
    >>> reader = ExtractedSetReader(root_path, subject_ids=[10006, 10011, 10019])
    >>> timeseries = reader.read_timeseries(num_subjects=2)
    >>> episodic_data = reader.read_episodic_data(subject_ids=[10006, 10011])
    """

    convert_datetime = ["INTIME", "CHARTTIME", "OUTTIME", "ADMITTIME", "DISCHTIME", "DEATHTIME"]

    def __init__(self, root_path: Path, subject_ids: list = None, num_samples: int = None) -> None:
        """_summary_

        Args:
            root_path (Path): _description_
            subject_folders (list, optional): _description_. Defaults to None.
        """

        self._file_types = ("timeseries", "episodic_data", "subject_events", "subject_diagnoses",
                            "subject_icu_history")
        # Maps from file type to expected index name
        self._index_name_mapping = dict(zip(self._file_types[1:], ["Icustay", None, None, None]))
        # Maps from file type to dtypes
        self._dtypes = {
            file_type: DATASET_SETTINGS[file_index]["dtype"]
            for file_type, file_index in zip(self._file_types, [
                "timeseries",
                "episodic_data",
                "subject_events",
                "diagnosis",
                "icu_history",
            ])
        }
        self._convert_datetime = {
            "subject_icu_history": DATASET_SETTINGS["icu_history"]["convert_datetime"],
            "subject_events": DATASET_SETTINGS["subject_events"]["convert_datetime"]
        }
        super().__init__(root_path, subject_ids)

    def read_csv(self, path: Path, dtypes: tuple = None) -> pd.DataFrame:
        """
        Read a CSV file into a pandas DataFrame, converting specified columns to datetime.

        Parameters
        ----------
        path : Path
            Absolute or relative path to the CSV file.
        dtypes : tuple, optional
            Data type(s) to apply to either the whole dataset or individual columns.

        Returns
        -------
        pd.DataFrame
            The dataframe read from the specified location.

        Examples
        --------
        >>> df = reader.read_csv(Path("/path/to/file.csv"))
        >>> df.head()
        """
        file_path = Path(path)
        if not file_path.is_relative_to(self._root_path):
            file_path = Path(self._root_path, file_path)

        if not file_path.is_file():
            warn_io(f"File path {str(file_path)} does not exist!")
            return pd.DataFrame()
        try:
            df = pd.read_csv(file_path, dtype=dtypes, low_memory=False)
        except TypeError as error:
            error_io(f"Can't fit the integer range into requested dtype. Pandas error: {error}",
                     TypeError)

        df = upper_case_column_names(df)

        for column in set(df.columns) & set(self.convert_datetime):
            df[column] = pd.to_datetime(df[column], errors="coerce")

        return df

    def read_subject(self,
                     dir_path: Union[Path, int, str],
                     read_ids: bool = False,
                     file_type_keys: bool = True,
                     file_types: tuple = None):
        """
        Read data for a single subject for specified directory or subject ID.

        Parameters
        ----------
        dir_path : Union[Path, int, str]
            The directory path to read the subject data from.
        read_ids : bool, optional
            Whether to read IDs. Defaults to False.
        file_type_keys : bool, optional
            Whether to use file type keys in the returned dictionary. Defaults to True.
        file_types : tuple, optional
            The types of files to read. If None, reads all file types.

        Returns
        -------
        dict
            Dictionary containing the data read for the subject.

        Examples
        --------
        >>> subject_data = reader.read_subject(dir_path="10019", read_ids=True)
        >>> subject_data["timeseries"].head()
        """
        dir_path = self._cast_dir_path(dir_path)

        if file_types is None:
            file_types = self._file_types
        else:
            if not (isinstance(file_types, Iterable) and not isinstance(file_types, str)):
                raise ValueError(f'file_types must be a iterable but is {type(file_types)}')

        return_data = dict() if file_type_keys else list()

        if not self._check_subject_dir(dir_path,
                                       [file for file in file_types if not file == "timeseries"]):
            return {}

        for filename in file_types:
            if filename == "timeseries":
                if file_type_keys:
                    return_data["timeseries"] = self._get_timeseries(dir_path, read_ids)
                else:
                    return_data.append(self._get_timeseries(dir_path, read_ids))
            else:
                if file_type_keys:
                    return_data[filename] = self._read_file(filename, dir_path)
                else:
                    return_data.append(self._read_file(filename, dir_path))
        if not len(return_data):
            warn_io(f"Directory {str(dir_path)} does not exist!")
        return return_data

    def read_subjects(self,
                      subject_ids: Union[List[str], List[int], None] = None,
                      num_subjects: int = None,
                      read_ids: bool = False,
                      file_type_keys: bool = True,
                      seed: int = 42):
        """
        Read data for multiple subjects, with file keys being one of timeseries, episodic_data, subject_events,
        diagnosis or icu_history.

        Parameters
        ----------
        subject_ids : Union[List[str], List[int]], optional
            List of subject IDs to read. If None, reads all subjects.
        num_subjects : int, optional
            Number of subjects to read. If None, reads all available subjects.
        read_ids : bool, optional
            Whether to read IDs. Defaults to False.
        file_type_keys : bool, optional
            Whether to use file type keys in the returned dictionary. Defaults to True.
        seed : int, optional
            Random seed for reproducibility. Default is 42.

        Returns
        -------
        dict
            Dictionary containing the data read for the subjects.

        Examples
        --------
        >>> subjects_data = reader.read_subjects(subject_ids=[10006, 10011], read_ids=True)
        >>> subjects_data[10006]["episodic_data"].head()
        >>> subjects_data[10006]["timeseries"][2246713].head()
        >>> # Or as list
        >>> subjects_data = reader.read_subjects(num_subjects=2)
        >>> subjects_data[0]["timeseries"][0].head()
        """
        subject_ids = self._cast_subject_ids(subject_ids)

        if subject_ids is not None and num_subjects is not None:
            raise ValueError("Only one of subject_ids or num_subjects can be specified!")

        subject_ids = self._sample_ids(subject_ids, num_subjects, seed)

        if read_ids:
            return_data = dict()
            for subject_id in subject_ids:
                subject_path = Path(self._root_path, str(subject_id))
                if not subject_path.is_dir():
                    continue
                subject_id = int(subject_path.name)
                return_data[subject_id] = self.read_subject(dir_path=Path(subject_path),
                                                            file_type_keys=file_type_keys,
                                                            file_types=self._file_types,
                                                            read_ids=read_ids)
            assert all([len(subject) for subject in return_data.values()])
            return return_data

        # without ids
        return_data = list()
        for subject_path in subject_ids:
            return_data.append(
                self.read_subject(dir_path=Path(subject_path),
                                  file_types=self._file_types,
                                  file_type_keys=file_type_keys,
                                  read_ids=read_ids))
        return return_data

    def read_timeseries(self,
                        num_subjects: int = None,
                        subject_ids: int = None,
                        read_ids: bool = False,
                        seed: int = 42):
        """
        Read timeseries data for specified subjects.

        Parameters
        ----------
        num_subjects : int, optional
            Number of subjects to read. Default is None.
        subject_ids : int, optional
            List of subject IDs to read. Default is None.
        read_ids : bool, optional
            Whether to read IDs. Default is False.
        seed : int, optional
            Random seed for reproducibility. Default is 42.

        Returns
        -------
        dict
            Dictionary containing the timeseries data for the subjects.

        Examples
        --------
        >>> timeseries_data = reader.read_timeseries(num_subjects=2, read_ids=True)
        >>> timeseries_data[10006][244351].head()
        >>> # Or as list
        >>> timeseries_data = reader.read_timeseries(subject_ids=[10006, 10011])
        >>> timeseries_data[0].head()
        """
        return self._read_filetype("timeseries", num_subjects, subject_ids, read_ids, seed)

    def read_episodic_data(self,
                           num_subjects: int = None,
                           subject_ids: int = None,
                           read_ids: bool = False,
                           seed: int = 42):
        """
        Read episodic data for specified subjects.

        Parameters
        ----------
        num_subjects : int, optional
            Number of subjects to read. Default is None.
        subject_ids : int, optional
            List of subject IDs to read. Default is None.
        read_ids : bool, optional
            Whether to read IDs. Default is False.
        seed : int, optional
            Random seed for reproducibility. Default is 42.

        Returns
        -------
        dict
            Dictionary containing the episodic data for the subjects.

        Examples
        --------
        >>> episodic_data = reader.read_episodic_data(subject_ids=[10006, 10011])
        >>> episodic_data[10006].head()
        """
        return self._read_filetype("episodic_data", num_subjects, subject_ids, read_ids, seed)

    def read_events(self,
                    num_subjects: int = None,
                    subject_ids: int = None,
                    read_ids: bool = False,
                    seed: int = 42):
        """
        Read event data for specified subjects.

        Parameters
        ----------
        num_subjects : int, optional
            Number of subjects to read. Default is None.
        subject_ids : int, optional
            List of subject IDs to read. Default is None.
        read_ids : bool, optional
            Whether to read IDs. Default is False.
        seed : int, optional
            Random seed for reproducibility. Default is 42.

        Returns
        -------
        dict
            Dictionary containing the event data for the subjects.

        Examples
        --------
        >>> events_data = reader.read_events(subject_ids=[10006, 10011])
        >>> events_data[10006].head()
        """
        return self._read_filetype("subject_events", num_subjects, subject_ids, read_ids, seed)

    def read_diagnoses(self,
                       num_subjects: int = None,
                       subject_ids: int = None,
                       read_ids: bool = False,
                       seed: int = 42):
        """
        Read diagnosis data for specified subjects.

        Parameters
        ----------
        num_subjects : int, optional
            Number of subjects to read. Default is None.
        subject_ids : int, optional
            List of subject IDs to read. Default is None.
        read_ids : bool, optional
            Whether to read IDs. Default is False.
        seed : int, optional
            Random seed for reproducibility. Default is 42.

        Returns
        -------
        dict
            Dictionary containing the diagnosis data for the subjects.

        Examples
        --------
        >>> diagnoses_data = reader.read_diagnoses(subject_ids=[10006, 10011])
        >>> diagnoses_data[10006].head()
        """
        return self._read_filetype("subject_diagnoses", num_subjects, subject_ids, read_ids, seed)

    def read_icu_history(self,
                         num_subjects: int = None,
                         subject_ids: int = None,
                         read_ids: bool = False,
                         seed: int = 42):
        """
        Read ICU history data for specified subjects.

        Parameters
        ----------
        num_subjects : int, optional
            Number of subjects to read. Default is None.
        subject_ids : int, optional
            List of subject IDs to read. Default is None.
        read_ids : bool, optional
            Whether to read IDs. Default is False.
        seed : int, optional
            Random seed for reproducibility. Default is 42.

        Returns
        -------
        dict
            Dictionary containing the ICU history data for the subjects.

        Examples
        --------
        >>> icu_history_data = reader.read_icu_history(subject_ids=[10006, 10011])
        >>> icu_history_data[10006].head()
        """
        return self._read_filetype("subject_icu_history", num_subjects, subject_ids, read_ids, seed)

    def _read_filetype(
        self,
        file_type: str,
        num_subjects: int,
        subject_ids: Union[List[int], List[str], np.ndarray],
        read_ids: bool,
        seed: int,
    ):

        subject_ids = self._cast_subject_ids(subject_ids)

        if subject_ids is not None and num_subjects is not None:
            raise ValueError("Only one of subject_ids or num_subjects can be specified!")

        subject_ids = self._sample_ids(subject_ids, num_subjects, seed)
        subject_folders = [Path(self.root_path, str(subject_id)) for subject_id in subject_ids]
        if read_ids:
            return self._read_data_with_ids(subject_folders, file_type)
        return self._read_data_without_ids(subject_folders, file_type)

    def _read_data_with_ids(self, subject_folders: Path, file_type: str):
        return_data = dict()
        for subject_path in subject_folders:
            subject_id = int(subject_path.name)
            return_data.update({
                subject_id:
                    self.read_subject(dir_path=Path(subject_path),
                                      file_types=[file_type],
                                      file_type_keys=False,
                                      read_ids=True).pop()
            })
        return return_data

    def _read_data_without_ids(self, subject_folders: Path, file_type: str):
        return_data = list()
        for subject_id in subject_folders:
            if file_type == 'timeseries':
                return_data.extend(
                    self.read_subject(dir_path=Path(subject_id),
                                      file_types=[file_type],
                                      file_type_keys=False,
                                      read_ids=False).pop())
            else:
                return_data.append(
                    self.read_subject(dir_path=Path(subject_id),
                                      file_types=[file_type],
                                      file_type_keys=False,
                                      read_ids=False).pop())
        return return_data

    def _check_subject_dir(self, subject_folder: Path, file_types: tuple):
        """_summary_

        Args:
            dir_path (Path): _description_
            file_types (tuple): _description_

        Returns:
            _type_: _description_
        """
        if os.getenv("DEBUG"):
            for filename in file_types:
                if not Path(subject_folder, f"{filename}.csv").is_file():
                    debug_io(f"Directory {subject_folder} does not have file {filename}.csv")
        return all([
            True if Path(subject_folder, f"{filename}.csv").is_file() else False
            for filename in file_types
        ])

    def _read_file(self, filename: str, dir_path: Path):  # , return_data: dict):
        """_summary_

        Args:
            filename (str): _description_
            dir_path (Path): _description_
            subject_id (int): _description_
        """
        file_df = pd.read_csv(Path(dir_path, f"{filename}.csv"),
                              dtype=self._dtypes[filename],
                              index_col=self._index_name_mapping[filename],
                              low_memory=False)

        if filename in self._convert_datetime:
            for column in self._convert_datetime[filename]:
                file_df[column] = pd.to_datetime(file_df[column])
        return file_df

    def _get_timeseries(self, dir_path: Path, read_ids: bool):
        """_summary_

        Args:
            dir_path (Path): _description_
            subject_id (int): _description_
        """
        subject_files = os.listdir(dir_path)
        if read_ids:
            timeseries = dict()
        else:
            timeseries = list()

        for file in subject_files:
            stay_id = re.findall('[0-9]+', file)
            if not stay_id:
                continue

            stay_id = stay_id.pop()
            if file.replace(stay_id, "") == "timeseries_.csv":
                if read_ids:
                    timeseries[int(stay_id)] = pd.read_csv(
                        Path(dir_path, file), dtype=self._dtypes["timeseries"]).set_index('hours')
                else:
                    timeseries.append(
                        pd.read_csv(Path(dir_path, file),
                                    dtype=self._dtypes["timeseries"]).set_index('hours'))

        return timeseries


class ProcessedSetReader(AbstractReader):
    """
    A reader for processed datasets, providing methods to read samples and individual subject data.

    Parameters
    ----------
    root_path : Path
        The root directory path containing subject folders.
    subject_ids : list of int, optional
        List of subject IDs to read. If None, reads all subjects in the root_path.
    set_index : bool, optional
        Whether to set the index for the dataframes. Defaults to True.

    Examples
    --------
    >>> root_path = Path("/path/to/data")
    >>> reader = ProcessedSetReader(root_path, subject_ids=[10006, 10011, 10019])
    >>> X, y = reader.random_samples(n_samples=2).values()
    >>> X[10006][244351].head()
    """

    def __init__(self, root_path: Path, subject_ids: list = None, set_index: bool = True) -> None:
        """_summary_

        Args:
            root_path (Path): _description_
            subject_folders (list, optional): _description_. Defaults to None.
        """
        self._reader_switch_Xy = {
            "csv": {
                "X": (lambda x: self._read_csv(x, dtypes=DATASET_SETTINGS["timeseries"]["dtype"])),
                "y": lambda x: self._read_csv(x)
            },
            "npy": {
                "X": np.load,
                "y": np.load,
                "t": np.load
            },
            "h5": {
                "X": pd.read_hdf,
                "y": pd.read_hdf
            }
        }
        super().__init__(root_path, subject_ids)
        self._random_ids = deepcopy(self.subject_ids)
        self._convert_datetime = ["INTIME", "CHARTTIME", "OUTTIME"]
        self._possibgle_datatypes = [pd.DataFrame, np.ndarray, np.array, None]

    @staticmethod
    def _read_csv(path: Path, dtypes: tuple = None) -> pd.DataFrame:
        df = pd.read_csv(path, dtype=dtypes)
        if 'hours' in df.columns:
            df = df.set_index('hours')
        if 'Timestamp' in df.columns:
            df = df.set_index('Timestamp')
        return df

    def read_samples(self,
                     subject_ids: Union[List[str], List[int]] = None,
                     read_ids: bool = False,
                     read_timestamps: bool = False,
                     data_type=None):
        """
        Read samples for the specified subject IDs, either as dictionary with ID keys or as list.

        Parameters
        ----------
        subject_ids : Union[List[str], List[int]], optional
            List of subject IDs to read. If None, reads all subjects.
        read_ids : bool, optional
            Whether to read IDs. Defaults to False.
        read_timestamps : bool, optional
            Whether to read timestamps. Defaults to False.
        data_type : type, optional
            Data type to cast the read data to. Can be one of [pd.DataFrame, np.ndarray, None]. Defaults to None.

        Returns
        -------
        dict
            Dictionary containing the samples read.

        Examples
        --------
        >>> # Reading samples for subject ID 10006 and stay 244351
        >>> samples = reader.read_samples(subject_ids=[10006], read_ids=True)
        >>> samples["X"][10006][244351].head()
        """

        dataset = {"X": {}, "y": {}} if read_ids else {"X": [], "y": []}

        if read_timestamps:
            dataset.update({"t": {} if read_ids else []})

        if subject_ids is None:
            subject_ids = self.subject_ids

        subject_ids = self._cast_subject_ids(subject_ids)

        for subject_id in subject_ids:
            sample = self.read_sample(subject_id,
                                      read_ids=read_ids,
                                      read_timestamps=read_timestamps,
                                      data_type=data_type)
            for prefix in sample:
                if not len(sample[prefix]):
                    warn_io(f"Subject {subject_id} does not exist!")
                if read_ids:
                    dataset[prefix].update({subject_id: sample[prefix]})
                else:
                    dataset[prefix].extend(sample[prefix])

        return dataset

    def read_sample(self,
                    subject_id: Union[int, str],
                    read_ids: bool = False,
                    read_timestamps: bool = False,
                    data_type=None) -> dict:
        """
        Read data for a single subject.

        Parameters
        ----------
        subject_id : Union[int, str]
            The subject ID to read.
        read_ids : bool, optional
            Whether to read IDs. Defaults to False.
        read_timestamps : bool, optional
            Whether to read timestamps. Defaults to False.
        data_type : type, optional
            Data type to cast the read data to. Can be one of [pd.DataFrame, np.ndarray, None]. Defaults to None.

        Returns
        -------
        dict
            Dictionary containing the data read for the subject.

        Raises
        ------
        ValueError
            If the data_type is not one of the possible data types.

        Examples
        --------
        >>> sample = reader.read_sample(subject_id=10006, read_ids=False)
        >>> sample["X"][0].head()
        """
        subject_id = int(subject_id)
        if not data_type in self._possibgle_datatypes:
            raise ValueError(
                f"Parameter data_type must be one of {self._possibgle_datatypes} but is {data_type}."
            )
        if data_type == np.array:
            data_type = np.ndarray

        dir_path = Path(self._root_path, str(subject_id))

        def _extract_number(string: str) -> int:
            stripper = f"abcdefghijklmnopqrstuvwxyzABZDEFGHIJKLMNOPQRSTUVWXYZ."
            return int(string.replace(".h5", "").replace(".csv", "").strip(stripper).strip("_"))

        def _convert_file_data(X):
            if data_type is None:
                return X
            if not isinstance(X, data_type) and data_type == np.ndarray:
                return X.to_numpy()
            elif not isinstance(X, data_type) and data_type == pd.DataFrame:
                return pd.DataFrame(X)
            return X

        dataset = {"X": {}, "y": {}} if read_ids else {"X": [], "y": []}

        if read_timestamps:
            dataset.update({"t": {} if read_ids else []})

        stay_id_stack = list()
        for file in dir_path.iterdir():
            stay_id = _extract_number(file.name)
            file_extension = file.suffix.strip(".")
            reader = self._reader_switch_Xy[file_extension]
            reader_kwargs = ({"allow_pickle": True} if file_extension == "npy" else {})

            if stay_id in stay_id_stack:
                continue

            stay_id_stack.append(stay_id)
            for prefix in dataset.keys():
                file_path = Path(file.parent, f"{prefix}_{stay_id}{file.suffix}")
                if not file_path.is_file():
                    continue
                file_data = reader[prefix](file_path, **reader_kwargs)
                file_data = _convert_file_data(file_data)

                if prefix == "t" and read_timestamps:
                    continue

                if read_ids:
                    if subject_id not in dataset[prefix]:
                        dataset[prefix].update({_extract_number(file.name): file_data})
                    else:
                        dataset[prefix][_extract_number(file.name)] = file_data
                else:
                    dataset[prefix].append(file_data)

        return dataset

    def random_samples(
            self,
            n_samples: int = 1,
            read_ids: bool = False,  # This is for debugging
            read_timestamps: bool = False,
            data_type=None,
            return_ids: bool = False,
            seed: int = 42):
        """
        Sample subjects randomly without replacement until subject list is exhauasted.

        Parameters
        ----------
        n_samples : int, optional
            Number of samples to read. Default is 1.
        read_ids : bool, optional
            Whether to read IDs. Default is False.
        read_timestamps : bool, optional
            Whether to read timestamps. Default is False.
        data_type : type, optional
            Data type to cast the read data to. Can be one of [pd.DataFrame, np.ndarray, None]. Default is None.
        return_ids : bool, optional
            Whether to return the sampled IDs along with the data. Default is False.
        seed : int, optional
            Random seed for reproducibility. Default is 42.

        Returns
        -------
        dict
            Dictionary containing the sampled data.
        list, optional
            List of sampled subject IDs if return_ids is True.

        Examples
        --------
        >>> samples = reader.random_samples(n_samples=3, read_ids=True)
        >>> samples["X"].keys()
        >>> data, ids = reader.random_samples(n_samples=3, return_ids=True)
        >>> ids
        >>> [10006, 10011, 10019]
        """
        random.seed(seed)
        sample_ids = list()
        n_samples_needed = n_samples

        while n_samples_needed > 0:
            if not self._random_ids:
                self._random_ids = list(set(self.subject_ids) - set(sample_ids))
                random.shuffle(self._random_ids)

            n_samples_curr = min(len(self._random_ids), n_samples_needed)
            sample_ids.extend(self._random_ids[:n_samples_curr])
            self._random_ids = self._random_ids[n_samples_curr:]
            n_samples_needed -= n_samples_curr

            if len(sample_ids) >= len(self.subject_ids):
                if len(sample_ids) > len(self.subject_ids):
                    warn_io(
                        f"Maximum number of samples in dataset reached! Requested {n_samples}, but dataset size is {len(self.subject_ids)}."
                    )
                break
        if return_ids:
            return self.read_samples(sample_ids,
                                     read_ids=read_ids,
                                     read_timestamps=read_timestamps,
                                     data_type=data_type), sample_ids
        return self.read_samples(sample_ids,
                                 read_ids=read_ids,
                                 read_timestamps=read_timestamps,
                                 data_type=data_type)


class EventReader():
    """
    A reader for event data from CHARTEVENTS, OUTPUTEVENTS, LABEVENTS, providing methods to read data either in chunks
    or in a single shot.

    Parameters
    ----------
    dataset_folder : Path
        The path to the dataset folder containing event data files.
    subject_ids : list of int, optional
        List of subject IDs to read. If None, reads all subjects.
    chunksize : int, optional
        The size of chunks to read at a time. Defaults to None.
    tracker : ExtractionTracker, optional
        An object to track the extraction progress. Defaults to None.

    Examples
    --------
    >>> dataset_folder = Path("/path/to/dataset")
    >>> event_reader = EventReader(dataset_folder, subject_ids=[10006, 10011, 10019], chunksize=1000)
    >>> event_reader.get_chunk()["CHARTEVENTS.csv"].head()
    """

    def __init__(self,
                 dataset_folder: Path,
                 subject_ids: list = None,
                 chunksize: int = None,
                 tracker: ExtractionTracker = None) -> None:
        """_summary_

        Args:
            dataset_folder (Path): _description_
            chunksize (int, optional): _description_. Defaults to None.
            tracker (object, optional): _description_. Defaults to None.
        """
        self.dataset_folder = dataset_folder
        self._done = False

        # Logic to early terminate if none of the subjects are in the remaining dataset
        if subject_ids is not None and len(subject_ids):
            self._last_occurrence = {
                "CHARTEVENTS.csv": {},
                "LABEVENTS.csv": {},
                "OUTPUTEVENTS.csv": {}
            }
            subject_target = dict(
                zip([str(subject_id) for subject_id in subject_ids],
                    ["SUBJECT_ID"] * len(subject_ids)))
            self._subject_ids = [int(subject_id) for subject_id in subject_ids]
            self._lo_thread_response = {}  # Last occurence thread response
            self._lo_thread_done = {}
            for csv in self._last_occurrence:
                self._lo_thread_response[csv] = threading.Event()
                thread = threading.Thread(target=self._csv_find_last,
                                          args=(Path(self.dataset_folder, csv), subject_target,
                                                self._last_occurrence[csv],
                                                self._lo_thread_response[csv]))
                thread.start()
                self._lo_thread_done[csv] = False

        else:
            self._lo_thread_response = None
            self._lo_thread_done = {
                "CHARTEVENTS.csv": False,
                "LABEVENTS.csv": False,
                "OUTPUTEVENTS.csv": False
            }
            self._subject_ids = None

        self._chunksize = chunksize
        self._tracker = tracker
        self._csv_settings = DATASET_SETTINGS["CHARTEVENTS"]
        self._event_csv_kwargs = {
            "CHARTEVENTS.csv": {
                "dtype": convert_dtype_dict(DATASET_SETTINGS["CHARTEVENTS"]["dtype"])
            },
            "LABEVENTS.csv": {
                "dtype": convert_dtype_dict(DATASET_SETTINGS["LABEVENTS"]["dtype"])
            },
            "OUTPUTEVENTS.csv": {
                "dtype": convert_dtype_dict(DATASET_SETTINGS["OUTPUTEVENTS"]["dtype"])
            }
        }
        self.event_csv_skip_rows = dict(zip(self._event_csv_kwargs.keys(), [0, 0, 0]))

        if chunksize:
            # Readers from which to get chunk
            # Pandas is leaky when get chunk is not carried out to the EOF so we
            # have to manage the file handle ourselves
            self._csv_reader = dict()
            self._csv_handle = dict()
            for csv_name, kwargs in self._event_csv_kwargs.items():
                file_handle = Path(dataset_folder, csv_name).open("rb")
                self._csv_reader[csv_name] = pd.read_csv(file_handle,
                                                         iterator=True,
                                                         chunksize=chunksize,
                                                         low_memory=False,
                                                         **kwargs)
                self._csv_handle[csv_name] = file_handle
            if subject_ids is None:
                # If subject ids is specified we need to start from the begining again
                # Since the new subjects might be in the first chunks
                self._init_reader()

        self._convert_datetime = ["INTIME", "CHARTTIME", "OUTTIME"]
        resource_folder = Path(dataset_folder, "resources")
        assert resource_folder.is_dir(), FileNotFoundError(
            f"Folder {str(resource_folder)} does not contain resources folder, which in turn should contain"
            f" hcup_ccs_2015_definitions.yaml and itemid_to_variable_map.csv.")
        self._varmap_df = read_varmap_csv(resource_folder)

    @property
    def done_reading(self):
        """
        Check if all chunks have been read.

        Returns
        -------
        bool
            True if all chunks have been read, False otherwise.

        Examples
        --------
        >>> event_reader.done_reading
        False
        """
        return self._done

    def _init_reader(self):
        """
        Initialize the reader by skipping rows according to the tracker if it exists.
        """
        info_io(f"Starting reader initialization.")
        header = "Initializing reader and starting at row:\n"
        msg = list()
        for csv in self._event_csv_kwargs:
            try:
                n_chunks = self._tracker.count_subject_events[csv] // self._chunksize
                skip_rows = self._tracker.count_subject_events[csv] % self._chunksize
                [len(self._csv_reader[csv].get_chunk()) for _ in range(n_chunks)]  # skipping chunks
                self.event_csv_skip_rows[csv] = skip_rows  # rows to skip in first chunk
                msg.append(f"{csv}: {self._tracker.count_subject_events[csv]}")
            except:
                self._csv_handle[csv].close()
        info_io(header + " - ".join(msg))

    def get_chunk(self) -> tuple:
        """
        Get the next chunk of event data.

        Returns
        -------
        tuple
            A tuple containing:
            - event_frames: dict of pd.DataFrame
                A dictionary where keys are CSV file names and values are dataframes of the read chunk.
            - frame_lengths: dict of int
                A dictionary where keys are CSV file names and values are the number of events read in the chunk.

        Examples
        --------
        >>> event_frames, frame_lengths = event_reader.get_chunk()
        >>> event_frames["CHARTEVENTS.csv"].head()
        
        """
        def read_frame(csv_name):
            # Read a frame
            if self._lo_thread_response is not None:
                if not self._lo_thread_done[csv_name] and self._lo_thread_response[csv_name].is_set(
                ):
                    last_occurences = max(self._last_occurrence[csv_name].values())
                    self._last_occurrence[csv_name] = last_occurences
                    debug_io(f"Last occurence for {csv_name}: {last_occurences}")
                    self._lo_thread_done[csv_name] = True

            events_df = self._csv_reader[csv_name].get_chunk()

            # If start index exceeds last occurence of any subject, stop reader
            if self._lo_thread_done[csv_name] and events_df.index[0] >= self._last_occurrence[
                    csv_name]:
                debug_io(
                    f"Reader for {csv_name} is done on last occurence at line {events_df.index[0]}."
                )
                self._csv_handle[csv_name].close()
                if all([handle.closed for handle in self._csv_handle.values()]):
                    self._done = True

            # Uppercase column names for consistency
            events_df = upper_case_column_names(events_df)

            if self._subject_ids is not None:
                events_df = events_df[events_df["SUBJECT_ID"].isin(self._subject_ids)]

            if not 'ICUSTAY_ID' in events_df:
                events_df['ICUSTAY_ID'] = pd.NA
                events_df['ICUSTAY_ID'] = events_df['ICUSTAY_ID'].astype(
                    self._event_csv_kwargs[csv_name]["dtype"]["ICUSTAY_ID"])

            # Drop specified columns and NAN rows, merge onto varmap for variable definitions
            drop_cols = set(events_df.columns) - set(self._csv_settings["columns"])
            events_df = events_df.drop(drop_cols, axis=1)

            # Skip start rows if directory already existed
            if self.event_csv_skip_rows[csv_name]:
                events_df = events_df.iloc[self.event_csv_skip_rows[csv_name]:]
                self.event_csv_skip_rows[csv_name] = 0

            # Convert to datetime
            for column in self._csv_settings["convert_datetime"]:
                events_df[column] = pd.to_datetime(events_df[column])
            if not events_df.empty and self._lo_thread_response is not None:
                debug_io(
                    f"Csv: {csv_name}\nRead chunk of size: {len(events_df)}\nLast idx: {events_df.index[-1]}"
                )

            return events_df

        event_frames = dict()

        for csv_name in self._event_csv_kwargs:
            # Read frame, if reader is done, it will raise
            try:
                if not self._csv_handle[csv_name].closed:
                    event_frames[csv_name] = read_frame(csv_name)
            except StopIteration as error:
                debug_io(f"Reader finished on {error}")
                self._csv_handle[csv_name].close()
                if all([handle.closed for handle in self._csv_handle.values()]):
                    self._done = True

            # Readers are done, return empty frame if queried
            if all([handle.closed for handle in self._csv_handle.values()]):
                return {csv_name: pd.DataFrame() for csv_name in self._event_csv_kwargs}, dict()

        # Number of subject events per CVS type
        frame_lengths = {csv_name: 0 for csv_name in self._event_csv_kwargs}
        frame_lengths.update({csv_name: len(frame) for csv_name, frame in event_frames.items()})

        return event_frames, frame_lengths

    def get_all(self):
        """
        Get all event data from the dataset.

        Returns
        -------
        pd.DataFrame
            Dataframe containing all event data.

        Examples
        --------
        >>> all_events = event_reader.get_all()
        >>> all_events.head()
        """
        event_csv = ["CHARTEVENTS.csv", "LABEVENTS.csv", "OUTPUTEVENTS.csv"]
        event_frames = list()

        for csv in event_csv:
            events_df = pd.read_csv(Path(self.dataset_folder, csv),
                                    low_memory=False,
                                    **self._event_csv_kwargs[csv])
            events_df = upper_case_column_names(events_df)

            if not 'ICUSTAY_ID' in events_df:
                events_df['ICUSTAY_ID'] = pd.NA

            # Drop specified columns and NAN rows, merge onto varmap for variable definitions
            drop_cols = set(events_df.columns) - set(self._csv_settings["columns"])
            events_df = events_df.drop(drop_cols, axis=1)

            for column in self._csv_settings["convert_datetime"]:
                events_df[column] = pd.to_datetime(events_df[column])

            event_frames.append(events_df)

        full_df = pd.concat(event_frames, ignore_index=True)

        return full_df

    @staticmethod
    def _csv_find_last(csv_path: Path,
                       target_dict: dict,
                       last_occurences: dict,
                       done_event: threading.Event = None):
        """Finds the last line where the values in the target_dict are found in the csv file.
        Target_dict should be a dictionary with the target value as key and the target column name as value.
        """
        with open(csv_path, 'r', encoding='utf-8') as file:
            # Check if the given column name is valid
            headers = file.readline().strip().split(',')
            column_indices = {name.strip("\'\""): index for index, name in enumerate(headers)}

            missing_columns = [
                column_name for column_name in target_dict.values()
                if column_name not in column_indices
            ]
            if missing_columns:
                raise ValueError(f"Column name '{*missing_columns,}' does not exist in the file")

            # Column->idx and idx->column mapping
            column_indices = {
                column_name: column_indices[column_name] for column_name in target_dict.values()
            }
            value_to_idx = {value: column_indices[target_dict[value]] for value in target_dict}
            # Init counts
            line_number = 0
            last_occurences.update(dict(zip(target_dict.keys(), [0] * len(target_dict))))

            columns_of_interest = list(set(value_to_idx.values()))
            max_column_of_interest = max(columns_of_interest) + 1

            # Run through the file
            for line in file:
                columns = line.strip().split(',', maxsplit=max_column_of_interest)
                # Check if the target column has the target value
                line_number += 1
                for target_value in last_occurences.keys():
                    if target_value == columns[value_to_idx[target_value]]:
                        last_occurences[target_value] = line_number

        done_event.set()
        debug_io(
            f"Thread for {csv_path.name} has and found last occurence {max(last_occurences.values())}."
        )
        return last_occurences


class SplitSetReader(object):

    def __init__(self, root_path: Path, split_sets: Dict[str, List[int]]) -> None:
        """
        A reader for datasets split into training, validation, and test sets, providing access to each split.

        Parameters
        ----------
        root_path : Path
            The root directory path containing the dataset.
        split_sets : dict of {str: list of int}
            A dictionary where keys are split names (e.g., "train", "val", "test") and values are lists of subject IDs.

        Examples
        --------
        >>> root_path = Path("/path/to/data")
        >>> split_sets = {
        ...     "train": [1, 2, 3],
        ...     "val": [4, 5],
        ...     "test": [6, 7]
        ... }
        >>> split_reader = SplitSetReader(root_path, split_sets)
        >>> train_reader = split_reader.train
        >>> val_reader = split_reader.val
        >>> test_reader = split_reader.test
        """
        self._root_path = Path(root_path)
        self._subject_ids = split_sets
        self._readers = {
            split: ProcessedSetReader(self._root_path, subject_ids=split_sets[split])
            for split in split_sets
            if split_sets[split]
        }

        self._splits = list(self._readers.keys())
        cum_length = sum([len(split) for split in split_sets.values()])
        self._ratios = {split: len(split_sets[split]) / cum_length for split in split_sets}

    @property
    def split_names(self) -> list:
        """
        Get the names of the dataset splits.

        Returns
        -------
        list
            List of split names.

        Examples
        --------
        >>> split_reader.split_names
        ['train', 'val', 'test']
        """
        return self._splits

    @property
    def root_path(self):
        """
        Get the root path.

        Returns
        -------
        Path
            The root directory path.

        Examples
        --------
        >>> split_reader.root_path
        PosixPath('/path/to/data')
        """
        return self._root_path

    @property
    def train(self) -> ProcessedSetReader:
        """
        Get the reader for the training set.

        Returns
        -------
        ProcessedSetReader
            The reader for the training set, or None if not available.

        Examples
        --------
        >>> train_reader = split_reader.train
        >>> train_reader.read_samples()
        """
        if "train" in self._readers:
            return self._readers["train"]
        return

    @property
    def val(self) -> ProcessedSetReader:
        """
        Get the reader for the validation set.

        Returns
        -------
        ProcessedSetReader
            The reader for the validation set, or None if not available.

        Examples
        --------
        >>> val_reader = split_reader.val
        >>> val_reader.read_samples()
        """
        if "val" in self._readers:
            return self._readers["val"]
        return

    @property
    def test(self) -> ProcessedSetReader:
        """
        Get the reader for the test set.

        Returns
        -------
        ProcessedSetReader
            The reader for the test set, or None if not available.

        Examples
        --------
        >>> test_reader = split_reader.test
        >>> test_reader.read_samples()
        """
        if "test" in self._readers:
            return self._readers["test"]
        return


### FILE: .\src\datasets\trackers.py ###
from typing import List
from utils.IO import *
from storable import storable


@storable
class ExtractionTracker():
    """
    Tracks the extraction process of the dataset.

    This class keeps track of various aspects of the data extraction process, including the number
    of events extracted from different files, the total number of samples, and the progress of the
    extraction for its different steps.
    """

    #: dict: A dictionary tracking the number of events extracted from specific files.
    count_subject_events: dict = {"OUTPUTEVENTS.csv": 0, "LABEVENTS.csv": 0, "CHARTEVENTS.csv": 0}
    #: int: The total number of samples extracted as timeseries.
    count_total_samples: int = 0
    #: bool: Flag indicating if by-subject information has been extracted.
    has_bysubject_info: bool = False
    #: bool: Flag indicating if episodic data has been extracted.
    has_episodic_data: bool = False
    #: bool: Flag indicating if timeseries data has been extracted.
    has_timeseries: bool = False
    #: bool: Flag indicating if subject events have been extracted.
    has_subject_events: bool = False
    #: bool: Flag indicating if ICU history has been extracted.
    has_icu_history: bool = False
    #: bool: Flag indicating if diagnoses data has been extracted.
    has_diagnoses: bool = False
    #: bool: Flag indicating if the extraction process is finished.
    is_finished: bool = False
    #: list: A list of subject IDs for tracking progress.
    subject_ids: list = list()  # Not extraction target but tracking
    #: int: The target number of samples for extraction.
    num_samples: int = None  # Extraction target
    #: int: The target number of subjects for extraction.
    num_subjects: int = None  # Extraction target

    def __init__(self,
                 num_samples: int = None,
                 num_subjects: int = None,
                 subject_ids: list = None,
                 *args,
                 **kwargs) -> None:

        # If num samples has increase more samples need to be raised, if decreased value error
        if self.num_samples is not None and num_samples is None:
            # If changed to None extraction is carried out for all samples
            self.reset(flags_only=True)

        if num_samples is not None and self.count_total_samples < num_samples:
            self.reset(flags_only=True)
            self.num_samples = num_samples
        elif self.num_samples is not None and num_samples is None:
            self.reset(flags_only=True)
            self.num_samples = num_samples

        if num_subjects is not None and len(self.subject_ids) < num_subjects:
            self.reset(flags_only=True)
            self.num_subjects = num_subjects
        # Continue processing if num subjects switche to None
        elif self.num_subjects is not None and num_subjects is None:
            self.reset(flags_only=True)
            self.num_subjects = num_subjects

        if subject_ids is not None:
            unprocessed_subjects = set(subject_ids) - set(self.subject_ids)
            if unprocessed_subjects:
                self.reset(flags_only=True)

    def reset(self, flags_only: bool = False):
        """
        Resets the tracker state.

        Parameters
        ----------
        flags_only : bool, optional
            If True, only reset flags; otherwise, reset all counts and lists.
        """
        if not flags_only:
            self.count_subject_events = {
                "OUTPUTEVENTS.csv": 0,
                "LABEVENTS.csv": 0,
                "CHARTEVENTS.csv": 0
            }
            self.count_total_samples = 0
            self.num_samples = None
            self.num_subjects = None
            self.subject_ids = list()
        # The other dfs are light weight and computed for all subjects
        self.has_episodic_data = False
        self.has_timeseries = False
        self.has_bysubject_info = False
        self.has_subject_events = False
        self.is_finished = False


@storable
class PreprocessingTracker():
    """
    Tracks the preprocessing of the dataset.

    This class keeps track of the preprocessing steps for the dataset, including the number of
    subjects processed and various preprocessing settings. Can be used for preprocessing,
    discretization, and feature engineering.
    """

    #: dict: A dictionary tracking the subjects and their preprocessing status.
    subjects: dict = {}
    #: int: The target number of subjects for preprocessing.
    num_subjects: int = None
    #: bool: Flag indicating if the preprocessing is finished.
    is_finished: bool = False
    #: bool: Flag indicating if the total count should be stored.
    _store_total: bool = True
    #: int: Discretization only. The time step size used in preprocessing.
    time_step_size: int = None
    #: bool: Discretization only. Flag indicating if the time series should start at zero.
    start_at_zero: bool = None
    #: str: The strategy used for imputing missing data.
    impute_strategy: str = None
    #: str: Discretization only. Legacy or experimental mode.
    mode: str = None

    def __init__(self, num_subjects: int = None, subject_ids: list = None, **kwargs):
        self._lock = None
        # Continue processing if num subjects is not reached
        if num_subjects is not None and len(self.subjects) - 1 < num_subjects:
            self.is_finished = False
            self.num_subjects = num_subjects
        # Continue processing if num subjects switche to None
        elif self.num_subjects is not None and num_subjects is None:
            self.is_finished = False
            self.num_subjects = num_subjects

        if subject_ids is not None:
            unprocessed_subjects = set(subject_ids) - set(self.subjects.keys())
            if unprocessed_subjects:
                self.is_finished = False

        # The impute startegies of the discretizer might change
        # In this case we rediscretize the data
        if kwargs:
            for attribute in ["time_step_size", "start_at_zero", "impute_strategy", "mode"]:
                if attribute in kwargs:
                    if getattr(self, attribute) is not None and getattr(
                            self, attribute) != kwargs[attribute]:
                        self.reset()
                    setattr(self, attribute, kwargs[attribute])

    @property
    def subject_ids(self) -> List[int]:
        """
        Get the list of subject IDs.

        Returns
        -------
        list
            A list of subject IDs.
        """
        if hasattr(self, "_progress"):
            return [
                subject_id for subject_id in self._read("subjects").keys() if subject_id != "total"
            ]
        return list()

    @property
    def stay_ids(self) -> List[int]:
        """
        Get the list of stay IDs.

        Returns
        -------
        list
            A list of stay IDs.
        """
        if hasattr(self, "_progress"):
            return [
                stay_id for subject_id, subject_data in self._read("subjects").items()
                if subject_id != "total" for stay_id in subject_data.keys() if stay_id != "total"
            ]
        return list()

    @property
    def samples(self) -> int:
        """
        Get the total number of samples processed.

        Returns
        -------
        int
            The total number of samples processed.
        """
        if hasattr(self, "_progress"):
            return sum([
                subject_data["total"]
                for subject_id, subject_data in self._read("subjects").items()
                if subject_id != "total"
            ])
        return 0

    def reset(self):
        """
        Resets the tracker state.
        """
        self.subjects = {}
        self.is_finished = False
        self.num_subjects = None


@storable
class DataSplitTracker():
    """
    Tracks the data splitting process.

    This class keeps track of the data splitting process, including the sizes of the train, 
    validation, and test sets, as well as demographic settings.
    """

    #: float: The proportion of the data to be used as the test set.
    test_size: float = None
    #: float: The proportion of the data to be used as the validation set.
    val_size: float = None
    #: float: The proportion of the data to be used as the training set.
    train_size: float = None
    #: dict: A dictionary tracking the subjects and their split status.
    subjects: dict = {}
    #: dict: A dictionary specifying demographic filters to be applied.
    demographic_filter: dict = None
    #: dict: A dictionary specifying demographic splits to be applied.
    demographic_split: dict = None
    #: dict: A dictionary tracking the split status of the data.
    split: dict = {}
    #: dict: A dictionary tracking the ratios of the splits.
    ratios: dict = {}
    #: bool: Flag indicating if the data splitting is finished.
    is_finished: bool = False

    def __init__(self,
                 tracker: PreprocessingTracker,
                 test_size: float = 0.0,
                 val_size: float = 0.0,
                 demographic_filter: dict = None,
                 demographic_split: dict = None):

        if self.is_finished:
            # Reset if changed
            if self.test_size != test_size:
                self.reset()
            elif self.val_size != val_size:
                self.reset()
            elif self.demographic_filter != demographic_filter:
                self.reset()
            elif self.demographic_split != demographic_split:
                self.reset()
        # Apply settings
        self.test_size = test_size
        self.val_size = val_size
        self.demographic_filter = demographic_filter
        self.demographic_split = demographic_split
        self.subjects = tracker.subjects

    def reset(self) -> None:
        """
        Resets the tracker state.
        """
        # Reset the results
        self.is_finished = False
        self.ratios = {}
        self.split = {}

    @property
    def subject_ids(self) -> list:
        """
        Get the list of subject IDs.

        Returns
        -------
        list
            A list of subject IDs.
        """
        if hasattr(self, "_progress"):
            return [
                subject_id for subject_id in self._read("subjects").keys() if subject_id != "total"
            ]
        return list()

    @property
    def split_sets(self):
        """
        Get the split sets.

        Returns
        -------
        list
            A list of split sets.
        """
        if hasattr(self, "_progress"):
            return list(self.ratios.keys())
        return list()


### FILE: .\src\datasets\writers.py ###
"""
Dataset Writers
===============

This module provides classes and methods for writing dataset files, and creating the subject directories named with the respecitve subject ID. The main class `DataSetWriter`
is used to write the subject data either as .npy, .csv, or .hdf5 files. 
and ICU history.

Classes
-------
- DataSetWriter: A writer class for datasets, providing methods to write data to files.

References
----------
- YerevaNN/mimic3-benchmarks: https://github.com/YerevaNN/mimic3-benchmarks
"""
import pandas as pd
import shutil
import numpy as np
from pathos.helpers import mp
import operator
from pathlib import Path
from utils.IO import *
from functools import reduce

__all__ = ["DataSetWriter"]


class DataSetWriter():
    """
    A writer class for datasets, providing methods to write data to files and create subject ID labeld directories.

    Parameters
    ----------
    root_path : Path
        The root directory path where the dataset files will be written.

    Examples
    --------
    >>> root_path = Path("/path/to/data")
    >>> writer = DataSetWriter(root_path)
    >>> data = {
    ...     "subject_events": {10006: pd.DataFrame(...), 10011: pd.DataFrame(...)},
    ...     "episodic_data": {10006: pd.DataFrame(...), 10011: pd.DataFrame(...)}
    ... }
    >>> writer.write_bysubject(data, file_type="csv")
    """

    def __init__(self, root_path: Path) -> None:
        self.root_path = root_path

    def _check_filename(self, filename: str):
        """
        Check if the filename is valid.
        """
        possible_filenames = [
            "episodic_data", "timeseries", "subject_events", "subject_diagnoses",
            "subject_icu_history", "X", "y", "t", "header"
        ]

        if filename not in possible_filenames:
            raise (f"choose a filename from {possible_filenames}")

    def _get_subject_ids(self, data: dict):
        """
        Get the subject IDs from the data dictionary.
        """

        id_sets = [set(dictionary.keys()) for dictionary in data.values()]
        subject_ids = list(reduce(operator.and_, id_sets))
        return subject_ids

    def write_bysubject(self,
                        data: dict,
                        index: bool = True,
                        exists_ok: bool = False,
                        file_type: str = "csv"):
        """
        Write data of file type by subject and create subject ID labeled directories.

        Parameters
        ----------
        data : dict
            The data to write.
        index : bool, optional
            Whether to write the index. Default is True.
        exists_ok : bool, optional
            Whether to overwrite existing files. Default is False.
        file_type : str, optional
            The file type to write. Must be one of ['csv', 'npy', 'hdf5']. Default is 'csv'.

        Raises
        ------
        ValueError
            If the file_type is not supported.
        """
        if self.root_path is None:
            return

        if not file_type in ["csv", "npy", "hdf5"]:
            raise ValueError(
                f"file_type {file_type} not supported. Must be one of ['csv', 'npy', 'hdf5']")

        for subject_id in self._get_subject_ids(data):

            self._write_subject(subject_id=subject_id,
                            data={filename: data[filename][subject_id] for filename in data.keys()},
                            index=index,
                            exists_ok=exists_ok,
                            file_type=file_type)

        return

    def _write_subject(self,
                   subject_id: int,
                   data: dict,
                   index: bool = True,
                   exists_ok: bool = False,
                   file_type: str = "csv"):
        """
        Write all files for a single subject
        """

        def save_df(df: pd.DataFrame,
                    path: Path,
                    index: str = True,
                    file_type: str = "csv",
                    exists_ok: bool = False) -> None:
            if exists_ok and path.is_file() and not file_type == "hd5f":
                mode = "a"
                header = False
            else:
                mode = "w"
                header = True
            if file_type == "hdf5":
                pd.DataFrame(df).to_hdf(Path(path.parent, f"{path.stem}.h5"),
                                        key="data",
                                        mode=mode,
                                        index=index)
            elif file_type == "csv":
                pd.DataFrame(df).to_csv(Path(path.parent, f"{path.stem}.csv"),
                                        mode=mode,
                                        index=index,
                                        header=header)
            elif file_type == "npy":
                if isinstance(df, (pd.DataFrame, pd.Series)):
                    df = df.to_numpy()
                np.save(Path(path.parent, f"{path.stem}.npy"), df)

        if file_type in ["npy", "hdf5"] and exists_ok:
            raise ValueError("Append mode not supported for numpy files!")

        if not file_type in ["csv", "npy", "hdf5"]:
            raise ValueError(
                f"file_type {file_type} not supported. Must be one of ['csv', 'npy', 'hdf5']")
        for filename, item in data.items():
            delet_flag = False
            self._check_filename(filename)

            subject_path = Path(self.root_path, str(subject_id))

            if not subject_path.is_dir():
                subject_path.mkdir(parents=True, exist_ok=True)
            if isinstance(item, (pd.DataFrame, pd.Series, np.ndarray)):
                if not len(item):
                    continue
                csv_path = Path(subject_path, f"{filename}")
                save_df(df=item,
                        path=csv_path,
                        index=index,
                        file_type=file_type,
                        exists_ok=exists_ok)
            elif isinstance(item, dict):
                for icustay_id, data in item.items():
                    if not len(data):
                        continue
                    csv_path = Path(subject_path, f"{filename}_{icustay_id}")
                    save_df(df=data,
                            path=csv_path,
                            index=index,
                            file_type=file_type,
                            exists_ok=exists_ok)

            # do not create empty or incomplete folders
            if not [folder for folder in subject_path.iterdir()] or delet_flag:
                debug_io(
                    f"Removing folder {subject_path}, because a file is missing or the folder is empty!"
                )
                shutil.rmtree(str(subject_path))

    def write_subject_events(self, data: dict, lock: mp.Lock = None, dtypes: dict = None):
        """
        Write subject events data to files by creating a new file or appending to existing file and create subject ID labeled directories.

        Parameters
        ----------
        data : dict
            The subject events data to write.
        lock : mp.Lock, optional
            A lock object to synchronize writing. Default is None.
        dtypes : dict, optional
            Data types to cast the dataframe to. Default is None.
        """
        if self.root_path is None:
            return

        def write_csv(dataframe: pd.DataFrame, path: Path, lock: mp.Lock):
            if dataframe.empty:
                return
            if lock is not None:
                lock.acquire()
            if dtypes is not None:
                dataframe = dataframe.astype(dtypes)
            if not path.is_file():
                dataframe.to_csv(path, index=False)
            else:
                dataframe.to_csv(path, mode='a', index=False, header=False)

            if lock is not None:
                lock.release()
            return

        for subject_id, subject_data in data.items():
            subject_path = Path(self.root_path, str(subject_id))
            subject_path.mkdir(parents=True, exist_ok=True)
            subject_event_path = Path(subject_path, "subject_events.csv")

            write_csv(subject_data, subject_event_path, lock)

        return


### FILE: .\src\datasets\__init__.py ###
"""Dataset file

This file allows access to the dataset as specified.
All function in this file are used by the main interface function load_data.
Subfunctions used within private functions are located in the datasets.utils module.

TODOS
- Use a settings.json
- This is a construction site, see what you can bring in here
- Provid link to kaggle in load_data doc string
- Expand function to utils

YerevaNN/mimic3-benchmarks
"""
import yaml
import os
from typing import Union
from pathlib import Path
from utils.IO import *
from settings import *
from datasets.processors.preprocessors import MIMICPreprocessor
from datasets.processors.feature_engines import MIMICFeatureEngine
from datasets.processors.discretizers import MIMICDiscretizer
from . import extraction
from .readers import ProcessedSetReader, ExtractedSetReader
from .split import train_test_split

# global settings

__all__ = ["load_data", "train_test_split"]


def load_data(source_path: str,
              storage_path: str = None,
              chunksize: int = None,
              subject_ids: list = None,
              num_subjects: int = None,
              time_step_size: float = 1.0,
              impute_strategy: str = "previous",
              mode: str = "legacy",
              start_at_zero=True,
              extract: bool = True,
              preprocess: bool = False,
              engineer: bool = False,
              discretize: bool = False,
              task: str = None) -> Union[ProcessedSetReader, ExtractedSetReader, dict]:
    """_summary_

    Args:
        stoarge_path (str, optional): Location where the processed dataset is to be stored. Defaults to None.
        source_path (str, optional): Location form which the unprocessed dataset is to be loaded. Defaults to None.
        ehr (str, optional): _description_. Defaults to None.
        from_storage (bool, optional): _description_. Defaults to True.
        chunksize (int, optional): _description_. Defaults to None.
        num_subjects (int, optional): _description_. Defaults to None.

    Raises:
        ValueError: _description_

    Returns:
        _type_: _description_
    """
    storage_path = Path(storage_path)
    source_path = Path(source_path)

    subject_ids = _check_inputs(storage_path=storage_path,
                                source_path=source_path,
                                chunksize=chunksize,
                                subject_ids=subject_ids,
                                num_subjects=num_subjects,
                                extract=extract,
                                preprocess=preprocess,
                                engineer=engineer,
                                discretize=discretize,
                                task=task)

    # Iterative generation if a chunk size is specified
    if storage_path is not None and chunksize is not None:
        if extract or preprocess or engineer or discretize:
            extracted_storage_path = Path(storage_path, "extracted")
            # Account for missing subjects
            reader = extraction.iterative_extraction(storage_path=extracted_storage_path,
                                                     source_path=source_path,
                                                     chunksize=chunksize,
                                                     subject_ids=subject_ids,
                                                     num_subjects=num_subjects,
                                                     task=task)

        if preprocess or engineer or discretize:
            # Contains phenotypes and a list of codes referring to the phenotype
            with Path(source_path, "resources", "hcup_ccs_2015_definitions.yaml").open("r") as file:
                phenotypes_yaml = yaml.full_load(file)

            processed_storage_path = Path(storage_path, "processed", task)
            preprocessor = MIMICPreprocessor(task=task,
                                            storage_path=processed_storage_path,
                                            phenotypes_yaml=phenotypes_yaml,
                                            label_type="one-hot",
                                            verbose=True)
            
            reader = preprocessor.transform_reader(reader=reader, 
                                                   subject_ids=subject_ids, 
                                                   num_subjects=num_subjects)

        if engineer:
            engineered_storage_path = Path(storage_path, "engineered", task)
            engine = MIMICFeatureEngine(config_dict=Path(os.getenv("CONFIG"), "engineering_config.json"),
                                        storage_path=storage_path,
                                        task=task,
                                        verbose=True)
            reader = engine.transform_reader(reader=reader,
                                            subject_ids=subject_ids,
                                            num_subjects=num_subjects)
        if discretize:
            discretized_storage_path = Path(storage_path, "discretized", task)
            discretizer = MIMICDiscretizer(reader=reader,
                                        task=task,
                                        storage_path=discretized_storage_path,
                                        time_step_size=time_step_size,
                                        impute_strategy=impute_strategy,
                                        start_at_zero=start_at_zero,
                                        mode=mode,
                                        verbose=False)
            reader = discretizer.transform_reader(reader=reader,
                                                subject_ids=subject_ids,
                                                num_subjects=num_subjects)
            '''
            reader = discretizing.iterative_discretization(reader=reader,
                                                           task=task,
                                                           storage_path=discretized_storage_path,
                                                           time_step_size=time_step_size,
                                                           impute_strategy=impute_strategy,
                                                           start_at_zero=start_at_zero,
                                                           mode=mode)
            '''

        return reader

    elif chunksize is not None:
        raise ValueError("To run iterative iteration, specify storage path!")

    # Compact generation otherwise
    if extract or preprocess or engineer or discretize:
        extracted_storage_path = Path(storage_path, "extracted")
        dataset = extraction.compact_extraction(storage_path=extracted_storage_path,
                                                source_path=source_path,
                                                num_subjects=num_subjects,
                                                subject_ids=subject_ids,
                                                task=task)
    if preprocess or engineer or discretize:
        processed_storage_path = Path(storage_path, "processed", task)
        # Contains phenotypes and a list of codes referring to the phenotype
        with Path(source_path, "resources", "hcup_ccs_2015_definitions.yaml").open("r") as file:
            phenotypes_yaml = yaml.full_load(file)
        preprocessor = MIMICPreprocessor(task=task,
                                            storage_path=processed_storage_path,
                                            phenotypes_yaml=phenotypes_yaml,
                                            label_type="one-hot",
                                            verbose=True)
        dataset = preprocessor.transform_dataset(dataset=dataset,
                                       subject_ids=subject_ids,
                                       num_subjects=num_subjects,
                                       source_path=extracted_storage_path)
        '''
        dataset = preprocessing.compact_processing(dataset=dataset,
                                                   task=task,
                                                   subject_ids=subject_ids,
                                                   num_subjects=num_subjects,
                                                   storage_path=processed_storage_path,
                                                   source_path=extracted_storage_path,
                                                   phenotypes_yaml=phenotypes_yaml)
        '''

    if engineer:
        engineered_storage_path = Path(storage_path, "engineered", task)
        dataset = feature_engineering.compact_fengineering(dataset["X"],
                                                           dataset["y"],
                                                           task=task,
                                                           storage_path=engineered_storage_path,
                                                           source_path=processed_storage_path,
                                                           subject_ids=subject_ids,
                                                           num_subjects=num_subjects)

    if discretize:
        discretized_storage_path = Path(storage_path, "discretized", task)
        dataset = discretizing.compact_discretization(dataset["X"],
                                                      dataset["y"],
                                                      task=task,
                                                      storage_path=discretized_storage_path,
                                                      source_path=processed_storage_path,
                                                      time_step_size=time_step_size,
                                                      impute_strategy=impute_strategy,
                                                      start_at_zero=start_at_zero,
                                                      mode=mode)

    # TODO: make dependent from return reader (can also return reader)
    # TODO: write some tests for comparct generation
    return dataset


def _check_inputs(storage_path: str, source_path: str, chunksize: int, subject_ids: list,
                  num_subjects: int, extract: bool, preprocess: bool, engineer: bool,
                  discretize: bool, task: str):
    if chunksize and not storage_path:
        raise ValueError(f"Specify storage path if using iterative processing!"
                         f"Storage path is '{storage_path}' and chunksize is '{chunksize}'")
    if (preprocess or engineer) and not task:
        raise ValueError(
            "Specify the 'task' parameter for which to preprocess or engineer the data!"
            " Possible values for task are: DECOMP, LOS, IHM, PHENO")
    if task and not (engineer or preprocess or discretize):
        warn_io(f"Specified  task '{task}' for data extraction only, despite "
                "data extraction being task agnostic. Parameter is ignored.")
    if subject_ids and num_subjects:
        raise ValueError("Specify either subject_ids or num_subjects, not both!")
    if not any([extract, preprocess, engineer]):
        raise ValueError("One of extract, preprocess or engineer must be set to load the dataset.")
    if subject_ids is not None:
        return [int(subject_id) for subject_id in subject_ids]
    return None


if __name__ == "__main__":
    ...


### FILE: .\src\datasets\extraction\event_consumer.py ###
"""
This module provides classes for processing subject events from the raw CHARTEVENT, LABEVENTS and OUTPUTEVENTS csv and store it by subject.
This class is used for multiprocessing and employed by the iterative extraction. The preprocessing is done by
the datasets.extraction.extraction_functions.extract_subject_events, this class handle the multiprocessing chain.

Classes
-------
- EventConsumer(storage_path, in_q, out_q, icu_history_df, lock)
    Consumes events from a queue, processes them, and writes to storage.

Examples
--------
.. code-block:: python

    from pathlib import Path
    from multiprocess import JoinableQueue, Lock
    import pandas as pd
    from some_module import EventConsumer, EventReader, ExtractionTracker

    # Initialize parameters
    storage_path = Path('/path/to/storage')
    in_q = JoinableQueue()
    out_q = JoinableQueue()
    icu_history_df = pd.read_csv('/path/to/icu_history.csv')
    tracker = ExtractionTracker(storage_path)

    # Create and start the consumer
    consumer = EventConsumer(storage_path=storage_path,
                             in_q=in_q,
                             out_q=out_q,
                             icu_history_df=icu_history_df,
                             lock=tracker._lock)
    consumer.start()

    # Create the event reader and get a chunk of data
    event_reader = EventReader(dataset_folder='/path/to/data',
                               chunksize=100000)
    event_frames, frame_lengths = event_reader.get_chunk()
    events_df = pd.concat(event_frames.values(), ignore_index=True)

    # Put the data into the queue and join the consumer
    in_q.put((events_df, frame_lengths))
    consumer.join()
"""

import pandas as pd
from pathlib import Path
from utils.IO import *
from multiprocess import Process, JoinableQueue, Lock
from .extraction_functions import extract_subject_events
from ..writers import DataSetWriter


class EventConsumer(Process):
    """
    A process that consumes events from a queue, processes them, and writes the results to storage.

    Parameters
    ----------
    storage_path : Path
        Path to the storage directory where the processed data will be saved.
    in_q : JoinableQueue
        Queue from which to read the events to process.
    out_q : JoinableQueue
        Queue to which the processed events are sent.
    icu_history_df : pd.DataFrame
        DataFrame containing ICU history information.
    lock : Lock
        Lock to manage access to shared resources.

    Methods
    -------
    run()
        Runs the consumer process, processing events from the queue.
    _make_subject_events(chartevents_df, icu_history_df)
        Creates subject events from chartevents and ICU history data.
    start()
        Starts the consumer process.
    join()
        Joins the consumer process.
    """

    def __init__(self, storage_path: Path, in_q: JoinableQueue, out_q: JoinableQueue,
                 icu_history_df: pd.DataFrame, lock: Lock):
        super().__init__()
        self._in_q = in_q
        self._out_q = out_q
        self._icu_history_df = icu_history_df
        self._dataset_writer = DataSetWriter(storage_path)
        self._lock = lock

    def run(self):
        count = 0  # count read data chunks
        while True:
            # Draw from queue
            chartevents_df, frame_lengths = self._in_q.get()

            if chartevents_df is None:
                # Join the consumer and update the queue
                self._in_q.task_done()
                self._out_q.put((frame_lengths, True))
                debug_io(f"Consumer finished on empty df and consumed {count} event chunks.")
                break
            # Process events
            subject_events = self._make_subject_events(chartevents_df, self._icu_history_df)
            self._dataset_writer.write_subject_events(subject_events, self._lock)

            # Update queue and send tracking data to publisher
            self._in_q.task_done()
            self._out_q.put((frame_lengths, False))
            # Update tracking
            count += 1
        return

    @staticmethod
    def _make_subject_events(chartevents_df: pd.DataFrame, icu_history_df: pd.DataFrame):
        return extract_subject_events(chartevents_df, icu_history_df)

    def start(self):
        super().start()
        debug_io("Started consumers")

    def join(self):
        super().join()
        debug_io("Joined in consumers")


### FILE: .\src\datasets\extraction\event_producer.py ###
"""
Dataset Extraction Module
=========================

This module provides the EventProducer class for reading and processing subject events from the raw CHARTEVENT, LABEVENTS and OUTPUTEVENTS CSVs and 
handling their processing into subject events. It supports both compact and iterative extraction methods to handle different use cases. 

**Subject Events**: A dictionary where each key is a subject ID, and the value is a DataFrame of 
chart events (e.g., lab results, vital signs) associated with that subject.
        - **From**: CHARTEVENTS, LABEVENTS, OUTPUTEVENTS
        - **In**: evenet_consumer.py
        - **Cols**: 
            - SUBJECT_ID
            - HADM_ID
            - ICUSTAY_ID
            - CHARTTIME
            - ITEMID
            - VALUE
            - VALUEUOM

Classes
-------
- EventProducer(source_path, storage_path, num_samples, chunksize, tracker, icu_history_df, subject_ids)
    Produces events by reading data from source, processing it, and passing it to consumers.

Examples
--------
.. code-block:: python

    from pathlib import Path
    import pandas as pd
    from some_module import EventProducer, ExtractionTracker

    # Initialize parameters
    source_path = Path('/path/to/source')
    storage_path = Path('/path/to/storage')
    chunksize = 100000
    icu_history_df = pd.read_csv('/path/to/icu_history.csv')
    tracker = ExtractionTracker(storage_path)
    subject_ids = [1234, 1235, 1236, 1237, 1238]

    # Create and run the producer
    producer = EventProducer(source_path, storage_path, chunksize, tracker, icu_history_df, subject_ids)
    producer.run()
"""

import pandas as pd
import pathos, multiprocess
from pathlib import Path
from copy import deepcopy
from multiprocess import Manager
from pathos.multiprocessing import cpu_count
from utils.IO import *
from .event_consumer import EventConsumer
from .progress_publisher import ProgressPublisher
from ..mimic_utils import *
from ..trackers import ExtractionTracker
from ..readers import EventReader


class EventProducer(object):
    """
    Produces events by reading data from the source, processing it, and passing it to consumers.
    The class handles the processing chain, that is the producer spawn event consumers and the progress publisher.
    It reads the events from the LABOUTPUTS, CHARTEVENTS and OUTPUTEVENTS CSV's and passes them via
    queues to the event consumer, which passes its progress to the progress publisher. Once dones,
    the producer joins all other processes.     

    Parameters
    ----------
    source_path : Path
        Path to the source directory containing the raw data files.
    storage_path : Path
        Path to the storage directory where the processed data will be saved.
    num_samples : int
        Number of samples to process.
    chunksize : int
        Size of chunks to read at a time.
    tracker : ExtractionTracker
        Tracker to keep track of extraction progress.
    icu_history_df : pd.DataFrame
        DataFrame containing ICU history information.
    subject_ids : list of int, optional
        List of subject IDs to process. If None, all subjects are processed. Default is None.

    Methods
    -------
    run()
        Starts the event production process, reading data, processing it, and passing it to consumers.
    _count_timeseries_sample(varmap_df, frame)
        Counts the number of unique time series samples in a DataFrame.
    _update_total_lengths(ts_lengths)
        Updates the total lengths of time series samples.
    """

    def __init__(self,
                 source_path: Path,
                 storage_path: Path,
                 num_samples: int,
                 chunksize: int,
                 tracker: ExtractionTracker,
                 icu_history_df: pd.DataFrame,
                 subject_ids: list = None):
        super().__init__()
        self._source_path = source_path
        self._storage_path = storage_path
        self._tracker = tracker
        self._icu_history_df = icu_history_df
        self._num_samples = num_samples
        self._chunksize = chunksize
        self._subject_ids = subject_ids

        # Counting variables
        self._count = 0  # count read data chunks
        self._total_length = self._tracker.count_total_samples
        event_csv = ["CHARTEVENTS.csv", "LABEVENTS.csv", "OUTPUTEVENTS.csv"]
        self._ts_total_lengths = dict(zip(event_csv, [0] * 3))

        # Queues to connect process stages
        manager = Manager()
        self._in_q = manager.Queue()
        self._out_q = manager.Queue()

        # Number of cpus is adjusted depending on sample size
        if num_samples is not None:
            self._cpus = min(
                cpu_count() - 2,
                int(np.ceil((num_samples - tracker.count_total_samples) / (chunksize))))
        else:
            self._cpus = cpu_count() - 2
        debug_io(f"Using {self._cpus+2} CPUs!")

    def run(self):
        # Start event consumers (processing and storing)
        consumers = [
            EventConsumer(storage_path=self._storage_path,
                          in_q=self._in_q,
                          out_q=self._out_q,
                          icu_history_df=self._icu_history_df,
                          lock=self._tracker._lock)
            # lock=multiprocess.Lock())
        ]

        consumers[0].start()
        # Starting publisher (publish processing progress)
        progress_publisher = ProgressPublisher(n_consumers=self._cpus,
                                               source_path=self._source_path,
                                               in_q=self._out_q,
                                               tracker=self._tracker)
        progress_publisher.start()
        event_reader = EventReader(dataset_folder=self._source_path,
                                   chunksize=self._chunksize,
                                   subject_ids=self._subject_ids,
                                   tracker=self._tracker)
        while True:
            # Create more consumers if needed
            if len(consumers) < self._cpus and not self._in_q.empty():
                consumers.append(
                    EventConsumer(storage_path=self._storage_path,
                                  in_q=self._in_q,
                                  out_q=self._out_q,
                                  icu_history_df=self._icu_history_df,
                                  lock=self._tracker._lock))
                consumers[-1].start()
            # Read data chunk from CSVs through readers
            event_frames, frame_lengths = event_reader.get_chunk()

            # Number of timeseries samples after preprocessing
            # TODO! This shouldn't be computed if num_samples is unspecified
            ts_lengths = {
                csv_name: self._count_timeseries_sample(event_reader._varmap_df, frame)
                for csv_name, frame in event_frames.items()
            }

            # If sample limit defined
            if self._num_samples is not None:
                # If remaining samples until sample limit is reached is smaller than sum of remaining samples
                if self._num_samples - self._total_length < sum(ts_lengths.values()):
                    # Get the correct number of samples
                    event_frames, frame_lengths, ts_lengths = get_samples_per_df(
                        event_frames, self._num_samples - self._total_length)
                    # Let consumers know about sample limit finish, so that read chunks is not incremented
                    frame_lengths["sample_limit"] = True
                    # Queue up data frame
                    events_df = pd.concat(event_frames.values(), ignore_index=True)
                    self._in_q.put((events_df, frame_lengths))

                    # Close consumers on empty dfs
                    for _ in range(self._cpus):
                        self._in_q.put((None, {}))
                    self._update_total_lengths(ts_lengths)

                    # Record total length
                    self._total_length += sum(ts_lengths.values())
                    self._tracker.count_total_samples = self._total_length
                    self._count += 1
                    debug_io(
                        f"Event producer finished on sample size restriction and produced {self._count} event chunks."
                    )
                    break

            # Queue up data frame
            events_df = pd.concat(event_frames.values(), ignore_index=True)

            # Close consumers on empty df
            if event_reader.done_reading:
                for _ in range(len(consumers)):
                    self._in_q.put((None, {}))
                debug_io(
                    f"Event producer finished on empty data frame and produced {self._count} event chunks."
                )
                break
            else:
                self._in_q.put((events_df, frame_lengths))

            # Update tracking
            self._count += 1
            self._total_length += sum(ts_lengths.values())
            self._update_total_lengths(ts_lengths)
            self._tracker.count_total_samples = self._total_length

        # Join processes and queues when done reading
        debug_io(f"Joining in queue")
        self._in_q.join()
        debug_io(f"In queue joined")
        [consumer.join() for consumer in consumers]
        # signal the publisher that we're done
        debug_io(f"Sending consumer finishes for uninitialized consumers: {frame_lengths}")
        [self._out_q.put(({}, True)) for _ in range(self._cpus - len(consumers))]
        debug_io(f"Joining out queue")
        self._out_q.join()
        debug_io(f"Out queue joined")
        progress_publisher.join()

        return

    def _count_timeseries_sample(self, varmap_df: pd.DataFrame, frame: pd.DataFrame):
        frame = deepcopy(frame)
        if frame.empty:
            return 0
        frame = frame.merge(varmap_df, left_on='ITEMID', right_index=True)
        frame = frame.dropna(subset=['HADM_ID'])
        frame = frame[['HADM_ID', 'ICUSTAY_ID',
                       'CHARTTIME']].merge(self._icu_history_df[['HADM_ID', 'ICUSTAY_ID']],
                                           left_on=['HADM_ID'],
                                           right_on=['HADM_ID'],
                                           suffixes=['', '_r'],
                                           how='inner')
        frame['ICUSTAY_ID'] = frame['ICUSTAY_ID'].fillna(frame['ICUSTAY_ID_r'])
        frame = frame.dropna(subset=['ICUSTAY_ID'])
        frame = frame[frame['ICUSTAY_ID'] == frame['ICUSTAY_ID_r']]

        return int(frame[["CHARTTIME"]].nunique())

    def _update_total_lengths(self, ts_lengths: dict):
        for csv_name, value in ts_lengths.items():
            self._ts_total_lengths[csv_name] += value
        return


### FILE: .\src\datasets\extraction\extraction_functions.py ###
"""
Extraction Functions Module
===========================

This module provides functions for extracting and processing ICU event data into a structured format.
The functions are used for both compact and iterative extraction processes. Compact extraction calls 
them from the __init__.py, while iterative calls them from the event_producer.py and timeseries_processor.py.

The primary output of these functions includes three main components:

1. **Subject Events**: A dictionary where each key is a subject ID, and the value is a DataFrame of 
   chart events (e.g., lab results, vital signs) associated with that subject.
        - **From**: CHARTEVENTS, LABEVENTS, OUTPUTEVENTS
        - **In**: evenet_consumer.py
        - **Cols**: 
            - SUBJECT_ID
            - HADM_ID
            - ICUSTAY_ID
            - CHARTTIME
            - ITEMID
            - VALUE
            - VALUEUOM

2. **Timeseries Data**: A dictionary where each key is a subject ID, and the value is another dictionary. 
   The inner dictionary maps each ICU stay ID to a DataFrame of time series data, containing recorded 
   events for specified variables (e.g., heart rate, blood pressure) indexed by time.
    - **From**: icu history, subject events, varmap
    - **In**: timeseries_processor.py
    - **Cols**: 
            - Capillary refill rate
            - Diastolic blood pressure
            - Fraction inspired oxygen
            - Glascow coma scale eye opening
            - Glascow coma scale motor response
            - Glascow coma scale total
            - Glascow coma scale verbal response
            - Glucose
            - Heart Rate
            - Height
            - Mean blood pressure
            - Oxygen saturation
            - pH
            - Respiratory rate
            - Systolic blood pressure
            - Temperature
            - Weight

3. **Episodic Data**: A dictionary where each key is a subject ID, and the value is a DataFrame of 
   episodic data, summarizing each ICU stay with patient-specific and stay-specific information 
   (e.g., age, length of stay, mortality, gender, ethnicity, height, weight, and diagnoses).
        - **From**: icu history, subject events, diagnoses
        - **In**: timeseries_processor.py
        - **Cols**:
            - ICU Stay ID
            - Age
            - Length of Stay (LOS)
            - Mortality
            - Gender
            - Ethnicity
            - Height
            - Weight
            - Diagnoses binaries as columns
"""


import pandas as pd
from utils.IO import *
from ..mimic_utils import *

__all__ = ["extract_subject_events", "extract_timeseries", "extract_episodic_data"]


def extract_subject_events(chartevents_df: pd.DataFrame, icu_history_df: pd.DataFrame):
    """
    Extracts and processes chartevent data into a dictionary of events by subject.

    This method processes the input DataFrame containing chartevent from the OUTPUTEVENTS, LABEVENTS or INPUTEVENTS CSV's 
    and merges it with the ICU stay history data to create a structured dictionary of events per subject and stay ID. 
    It filters and aligns the chartevents with the corresponding ICU stays, ensuring that each event is correctly associated with 
    the appropriate ICU stay and subject, as some events might not include these IDs. If not ICUSTAY_ID could be found,
    the data is dropped.

    Parameters
    ----------
    chartevents_df : pd.DataFrame
        DataFrame containing chartevent data from ICU.
    icu_history_df : pd.DataFrame
        DataFrame containing ICU stay data.

    Returns
    -------
    dict
        Dictionary containing chartevents per subject ID.
    """
    chartevents_df = chartevents_df.dropna(subset=['HADM_ID'])
    recovered_df = chartevents_df.merge(icu_history_df,
                                        left_on=['HADM_ID'],
                                        right_on=['HADM_ID'],
                                        how='left',
                                        suffixes=['', '_r'],
                                        indicator=True)
    recovered_df = recovered_df[recovered_df['_merge'] == 'both']
    recovered_df['ICUSTAY_ID'] = recovered_df['ICUSTAY_ID'].fillna(recovered_df['ICUSTAY_ID_r'])
    recovered_df = recovered_df.dropna(subset=['ICUSTAY_ID'])
    recovered_df = recovered_df[(recovered_df['ICUSTAY_ID'] == recovered_df['ICUSTAY_ID_r'])]
    recovered_df = recovered_df[[
        'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'CHARTTIME', 'ITEMID', 'VALUE', 'VALUEUOM'
    ]]

    return {id: x for id, x in recovered_df.groupby('SUBJECT_ID') if not x.empty}


def extract_timeseries(subject_events, subject_diagnoses, subject_icu_history, varmap_df):
    """
    Processes subject events, diagnoses, and ICU history into time series and episodic data.

    This method retrieves the ICU history, diagnoses, and events data for each subject. It merges the events data 
    with variable mappings, sorts the ICU stays by admission and discharge times, and extracts diagnosis labels and 
    general patient data for each ICU stay. These are then stored in the episodic_data_df. The method then cleans and processes the events data
     (e.g. convert to same metric units, remove null values), 
    extracts time series data for each variable of interest, per episode. It extracts hourly indexed time 
    series data and retrieves static values such as weight and height for each ICU stay. Finally, the episodic and timeseries
    data are stored in the subject directory. 

    **Episodic data** includes the following information for each ICU stay:
        - ICU Stay ID
        - Age
        - Length of Stay (LOS)
        - Mortality
        - Gender
        - Ethnicity
        - Height
        - Weight
        - Diagnoses binaries as columns
    
    **Time series** variables of interest are: 
        - Capillary refill rate
        - Diastolic blood pressure
        - Fraction inspired oxygen
        - Glascow coma scale eye opening
        - Glascow coma scale motor response
        - Glascow coma scale total
        - Glascow coma scale verbal response
        - Glucose
        - Heart Rate
        - Height
        - Mean blood pressure
        - Oxygen saturation
        - pH
        - Respiratory rate
        - Systolic blood pressure
        - Temperature
        - Weight

    Parameters
    ----------
    subject_events : dict
        Dictionary containing chart and laboratory events per subject.
    subject_diagnoses : dict
        Dictionary containing diagnoses per ICU stay per patient.
    subject_icu_history : dict
        Dictionary containing ICU history per subject.
    varmap_df : pd.DataFrame
        DataFrame containing variable mappings.

    Returns
    -------
    tuple
        - episodic_data : dict
            Dictionary describing each ICU stay.
        - timeseries : dict
            Dictionary containing time series of events over time for each ICU stay.
    """
    variables = varmap_df.VARIABLE.unique()
    timeseries = dict()
    episodic_data = dict()

    for subject_id in subject_icu_history.keys():
        try:
            current_icu_history_df: pd.DataFrame = subject_icu_history[subject_id]
            current_diagnoses_df: pd.DataFrame = subject_diagnoses[subject_id]
            current_events_df: pd.DataFrame = subject_events[subject_id]
        except:
            continue

        # Adjust current events
        current_events_df = current_events_df.merge(varmap_df, left_on='ITEMID', right_index=True)
        current_events_df = current_events_df.loc[current_events_df.VALUE.notnull()]
        current_events_df.loc[:, 'VALUEUOM'] = current_events_df['VALUEUOM'].fillna('').astype(str)

        # Sort stays
        current_icu_history_df = current_icu_history_df.sort_values(by=['INTIME', 'OUTTIME'])

        # General patient data belonging to each ICU stay
        diagnosis_labels = extract_diagnoses_util(current_diagnoses_df.reset_index(drop=True))
        episodic_data_df = extract_episodic_data(current_icu_history_df)

        # Reset index before merge, so that we can keep it
        episodic_data_df = episodic_data_df.merge(diagnosis_labels,
                                                  left_index=True,
                                                  right_index=True)
        episodic_data_df.index.names = ["Icustay"]
        current_events_df = clean_chartevents_util(current_events_df)

        timeseries_df = extract_timeseries_util(current_events_df, variables)

        timeseries[subject_id] = dict()

        for index in range(current_icu_history_df.shape[0]):
            stay_id = current_icu_history_df.ICUSTAY_ID.iloc[index]
            intime = current_icu_history_df.INTIME.iloc[index]
            outtime = current_icu_history_df.OUTTIME.iloc[index]
            icu_episode_df = extract_episode(timeseries_df, stay_id, intime, outtime)

            if icu_episode_df.shape[0] == 0:
                continue

            icu_episode_df = extract_hour_index(icu_episode_df, intime)

            episodic_data_df.loc[stay_id, 'Weight'] = get_static_value(icu_episode_df, 'Weight')
            episodic_data_df.loc[stay_id, 'Height'] = get_static_value(icu_episode_df, 'Height')

            timeseries[subject_id][stay_id] = icu_episode_df

        episodic_data[subject_id] = episodic_data_df

    return episodic_data, timeseries


def extract_episodic_data(subject_icu_history: pd.DataFrame) -> pd.DataFrame:
    """
    Create episodic data from subject ICU history.

    This method processes the ICU history of subjects to create a DataFrame containing episodic data for each ICU stay.
    The episodic data includes per ICU stay:
    - ID
    - age
    - length of stay
    - mortality
    - gender
    - ethnicity
    - height
    - weight

    Gender and ethnicity are imputed using predefined mappings if they are missing.

    Parameters
    ----------
    subject_icu_history : pd.DataFrame
        DataFrame containing the subject ICU history.

    Returns
    -------
    pd.DataFrame
        DataFrame containing episodic data.
    """
    # Episodic data contains: Icustay, Age, Length of Stay, Mortality, Gender, Ethnicity, Height and Weight
    episodic_data = subject_icu_history[["ICUSTAY_ID", "AGE", "LOS",
                                         "MORTALITY"]].rename(columns={"ICUSTAY_ID": "Icustay"})

    def imputeby_map(string, map):
        """
        """
        if string in map:
            return map[string]
        return map['OTHER']

    # Impute gender
    episodic_data['GENDER'] = subject_icu_history.GENDER.fillna('').apply(
        imputeby_map, args=([DATASET_SETTINGS["gender_map"]]))

    # Impute ethnicity
    ethnicity_series = subject_icu_history.ETHNICITY.apply(
        lambda x: x.replace(' OR ', '/').split(' - ')[0].split('/')[0])
    episodic_data['ETHNICITY'] = ethnicity_series.fillna('').apply(
        imputeby_map, args=([DATASET_SETTINGS["ethnicity_map"]]))

    # Empty values
    episodic_data['Height'] = np.nan
    episodic_data['Weight'] = np.nan

    episodic_data = episodic_data.set_index('Icustay')

    return episodic_data


def extract_diagnoses_util(diagnoses: pd.DataFrame) -> pd.DataFrame:
    """
    Create a DataFrame of diagnoses from each ICU stay with diagnose code in the column and stay ID as index.

    This method processes the diagnoses DataFrame to create a binary matrix indicating the presence of each diagnosis 
    present in the diagnoses.csv, as code over ICU stay. The resulting DataFrame has ICU stay IDs as rows and diagnosis codes as columns, with 
    binary values indicating whether each diagnosis code is present.

    Parameters
    ----------
    diagnoses : pd.DataFrame
        DataFrame containing diagnoses.

    Returns
    -------
    pd.DataFrame
        DataFrame with diagnoses as columns and stay IDs as rows.
    """
    # Diagnoese from each ICU stay with diagnose code in the column and stay ID as index
    diagnoses['VALUE'] = 1
    diagnoses = diagnoses[['ICUSTAY_ID', 'ICD9_CODE', 'VALUE']].drop_duplicates()
    labels = diagnoses.pivot(index='ICUSTAY_ID', columns='ICD9_CODE', values='VALUE')
    labels = labels.fillna(0).astype(int)
    labels = labels.reindex(columns=DATASET_SETTINGS["diagnosis_labels"])
    labels = labels.fillna(0).astype(int)

    return labels


def extract_timeseries_util(subject_events: pd.DataFrame, variables) -> pd.DataFrame:
    """
    This method processes the input DataFrame containing chart events to generate a time series DataFrame for the 
    specified variables. The data is pivoted against the VARIABLE (17 main features), which are now the columns containing 
    the entry from the previous VALUE row, so that each row represents a specific chart time. The resulting DataFrame 
    contains the time series data of each variable indexed by chart time and includes the ICU stay ID.

    Parameters
    ----------
    chartevents : pd.DataFrame
        DataFrame containing chart events.
    variables : list
        List of variables to include in the time series.

    Returns
    -------
    pd.DataFrame
        DataFrame containing the time series data.
    """
    # Lets create the time series
    metadata = subject_events[['CHARTTIME', 'ICUSTAY_ID']]
    metadata = metadata.sort_values(by=['CHARTTIME', 'ICUSTAY_ID'])
    metadata = metadata.drop_duplicates(keep='first').set_index('CHARTTIME')

    # Timeseries contains only the following. Subject_id and personal information in episodic data
    timeseries_df = subject_events[['CHARTTIME', 'VARIABLE', 'VALUE']]
    timeseries_df = timeseries_df.sort_values(by=['CHARTTIME', 'VARIABLE', 'VALUE'], axis=0)
    timeseries_df = timeseries_df.drop_duplicates(subset=['CHARTTIME', 'VARIABLE'], keep='last')
    timeseries_df = timeseries_df.pivot(index='CHARTTIME', columns='VARIABLE', values='VALUE')
    timeseries_df = timeseries_df.merge(metadata, left_index=True, right_index=True)
    timeseries_df = timeseries_df.sort_index(axis=0).reset_index()

    timeseries_df = timeseries_df.reindex(columns=np.append(variables, ['ICUSTAY_ID', 'CHARTTIME']))

    return timeseries_df


def extract_episode(timeseries_df: pd.DataFrame,
                 stay_id: int,
                 intime=None,
                 outtime=None) -> pd.DataFrame:
    """
    Create an episode DataFrame from the time series data.

    Parameters
    ----------
    timeseries_df : pd.DataFrame
        DataFrame containing the time series data.
    stay_id : int
        The stay ID to filter the time series data.
    intime : datetime, optional
        The admission time to filter the time series data.
    outtime : datetime, optional
        The discharge time to filter the time series data.

    Returns
    -------
    pd.DataFrame
        DataFrame containing the episode data.
    """
    # All events with ID
    indices = (timeseries_df.ICUSTAY_ID == stay_id)

    # Plus all events int time frame
    if intime is not None and outtime is not None:
        indices = indices | ((timeseries_df.CHARTTIME >= intime) &
                             (timeseries_df.CHARTTIME <= outtime))

    # Filter out and remove ID (ID already in episodic data)
    timeseries_df = timeseries_df.loc[indices]
    del timeseries_df['ICUSTAY_ID']

    return timeseries_df


def extract_hour_index(episode_df: pd.DataFrame,
                    intime,
                    remove_charttime: bool = True) -> pd.DataFrame:
    """
    Create an hour-based index for the episode DataFrame.

    Parameters
    ----------
    episode_df : pd.DataFrame
        DataFrame containing the episode data.
    intime : datetime
        The admission time to calculate the hours.
    remove_charttime : bool, optional
        Whether to remove the CHARTTIME column, by default True.

    Returns
    -------
    pd.DataFrame
        DataFrame with an hour-based index.
    """
    # Get difference and convert to hours
    episode_df = episode_df.copy()
    episode_df['hours'] = (episode_df.CHARTTIME -
                           intime).apply(lambda s: s / np.timedelta64(1, 's'))
    episode_df['hours'] = episode_df.hours / 60. / 60

    # Set index
    episode_df = episode_df.set_index('hours').sort_index(axis=0)

    if remove_charttime:
        del episode_df['CHARTTIME']

    return episode_df

### FILE: .\src\datasets\extraction\progress_publisher.py ###
"""
Dataset Extraction Module
=========================

This module provides a class for publishing the progress of the event processing.
This class is used for multiprocessing and employed by the iterative extraction.

Classes
-------
- ProgressPublisher(n_consumers, source_path, in_q, tracker)
    Publishes the progress of the event processing.
"""


import os
from utils import count_csv_size
from utils.IO import *
from pathlib import Path
from multiprocess import JoinableQueue, Process
from ..trackers import ExtractionTracker


class ProgressPublisher(Process):
    """
    Publishes the progress of the event processing to the command line.

    Parameters
    ----------
    n_consumers : int
        Number of consumer processes.
    source_path : Path
        Path to the source directory containing the raw data files.
    in_q : JoinableQueue
        Queue from which to read the progress updates.
    tracker : ExtractionTracker
        Tracker to keep track of extraction progress.

    Methods
    -------
    run()
        Runs the progress publisher, updating and printing the progress.
    start()
        Starts the progress publisher process.
    join()
        Joins the progress publisher process.

    Examples
    --------
    .. code-block:: python
    
        from pathlib import Path
        from multiprocess import JoinableQueue, Lock
        from ..trackers import ExtractionTracker

        # Initialize parameters
        source_path = Path('/path/to/source')
        storage_path = Path('/path/to/storage')
        in_q = JoinableQueue()
        out_q = JoinableQueue()
        icu_history_df = pd.read_csv('/path/to/icu_history.csv')
        lock = Lock()
        tracker = ExtractionTracker(storage_path)

        # Create and start the progress publisher
        progress_publisher = ProgressPublisher(n_consumers=4,
                                               source_path=source_path,
                                               in_q=out_q,
                                               tracker=tracker)
        progress_publisher.start() 

        # Create and start the event consumer
        consumer = EventConsumer(storage_path=storage_path,
                                 in_q=in_q,
                                 out_q=out_q,
                                 icu_history_df=icu_history_df,
                                 lock=lock)
        consumer.start()

        # Read and process events
        event_reader = EventReader(dataset_folder='/path/to/data', chunksize=1000)
        event_frames, frame_lengths = event_reader.get_chunk()
        events_df = pd.concat(event_frames.values(), ignore_index=True)
        in_q.put((events_df, frame_lengths))

        Processed event rows:
        CHARTEVENTS:    1000/5000
        LABEVENTS:      1000/6000
        OUTPUTEVENTS:   1000/4500
    """

    def __init__(self, n_consumers: int, source_path: Path, in_q: JoinableQueue,
                 tracker: ExtractionTracker):
        super().__init__()
        self._in_q = in_q
        self._tracker = tracker
        self._n_consumers = n_consumers
        self._event_file_lengths = {
            name: count_csv_size(Path(source_path, name))
            for name in ["CHARTEVENTS.csv", "LABEVENTS.csv", "OUTPUTEVENTS.csv"]
        }

    def run(self):
        done_count = 0  # Track consumer finishes

        # Print initial state
        msg = [f"Processed event rows: "]
        for csv_name in self._event_file_lengths.keys():
            csv_event_count = self._tracker.count_subject_events[csv_name]
            total_event_count = self._event_file_lengths[csv_name]
            print_name = csv_name.strip('.csv') + ": "
            msg.append(
                f"{print_name} {csv_event_count:>{len(str(total_event_count))}}/{total_event_count}"
            )
        info_io("\n".join(msg))

        while True:
            # Draw tracking information from queue
            frame_lengths, finished = self._in_q.get()
            # TODO! real crash resilience can only be achieved by updating in the event consumer
            self._tracker.count_subject_events += frame_lengths
            # Track consumer finishes
            if finished:
                done_count += 1
                debug_io(f"Publisher received consumer finished: {done_count}/{self._n_consumers}")
            # Print current state
            msg = [f"Processed event rows: "]
            for csv_name in self._event_file_lengths.keys():
                csv_event_count = self._tracker.count_subject_events[csv_name]
                total_event_count = self._event_file_lengths[csv_name]
                print_name = csv_name.strip('.csv') + ": "
                msg.append(
                    f"{print_name} {csv_event_count:>{len(str(total_event_count))}}/{total_event_count}"
                )

            info_io("\n".join(msg), flush_block=not (int(os.getenv("DEBUG", 0))))
            # Join publisher
            if done_count == self._n_consumers:
                self._tracker.has_subject_events = True
                debug_io("All consumers finished, publisher finishes now.")
                self._in_q.task_done()
                break
            self._in_q.task_done()
        return

    def start(self):
        super().start()
        debug_io("Started publisher")

    def join(self):
        super().join()
        debug_io("Joined publisher")


### FILE: .\src\datasets\extraction\timeseries_processor.py ###
"""
This module provides the TimeseriesProcessor class for processing time series data from ICU records. 
It reads and processes subject event data, diagnoses, and ICU history to generate time series and episodic data.
The timeseries and episodic data are stored in the subject ID labeled directories, while the episodic info is 
condensed into a single CSV file.


Classes
-------
- TimeseriesProcessor(storage_path, source_path, tracker, subject_ids, diagnoses_df, icu_history_df, varmap_df, num_samples)
    Processes time series data for given subjects and stores the results.

Examples
--------
.. code-block:: python

    from pathlib import Path
    from ..trackers import ExtractionTracker
    from datasets.extraction import get_by_subject
    import pandas as pd

    # Initialize parameters
    storage_path = Path('/path/to/storage')
    source_path = Path('/path/to/source')
    tracker = ExtractionTracker(storage_path)
    subject_ids = [123, 456, 789]
    diagnoses_df = pd.read_csv('/path/to/diagnoses.csv')
    icu_history_df = pd.read_csv('/path/to/icu_history.csv')
    varmap_df = pd.read_csv('/path/to/varmap.csv')
    num_samples = 100

    # Prepare subject-specific data
    subject_diagnoses = get_by_subject(diagnoses_df)
    subject_icu_history = get_by_subject(icu_history_df)

    # Create and run the processor
    processor = TimeseriesProcessor(storage_path, source_path, tracker, subject_ids, diagnoses_df, icu_history_df, varmap_df, num_samples)
    processor.run()
"""


import pandas as pd
from utils.IO import *
from settings import *
from pathlib import Path
from pathos.multiprocessing import cpu_count, Pool
from .extraction_functions import extract_timeseries
from ..trackers import ExtractionTracker
from ..writers import DataSetWriter
from ..readers import ExtractedSetReader


class TimeseriesProcessor(object):

    def __init__(self,
                 storage_path: Path,
                 source_path: Path,
                 tracker: ExtractionTracker,
                 subject_ids: list,
                 diagnoses_df: pd.DataFrame,
                 icu_history_df: pd.DataFrame,
                 varmap_df: pd.DataFrame,
                 num_samples: int = None):
        """
        Processes time series data for given subjects and stores the results.

        Parameters
        ----------
        storage_path : Path
            Path to the storage directory where the processed data will be saved.
        source_path : Path
            Path to the source directory containing the raw data files.
        tracker : ExtractionTracker
            Tracker to keep track of extraction progress.
        subject_ids : list of int
            List of subject IDs to process.
        diagnoses_df : pd.DataFrame
            DataFrame containing diagnoses information.
        icu_history_df : pd.DataFrame
            DataFrame containing ICU history information.
        varmap_df : pd.DataFrame
            DataFrame containing variable mappings.
        num_samples : int, optional
            Number of samples to process. Default is None.
        """
        self._storage_path = storage_path
        self._tracker = tracker
        self._dataset_reader = ExtractedSetReader(source_path)
        self._dataset_writer = DataSetWriter(storage_path)
        if subject_ids is None:
            self._subject_ids = [
                int(folder.name)
                for folder in storage_path.iterdir()
                if folder.is_dir() and folder.name.isnumeric()
            ]
            self._subject_ids.sort()
        else:
            self._subject_ids = subject_ids
        self._num_samples = num_samples
        self._subject_diagnoses = diagnoses_df
        self._subject_icu_history = icu_history_df
        self._varmap_df = varmap_df

    def _store_df_chunk(self, episodic_info_df):
        """Stores a chunk of episodic information DataFrame to a CSV file.
        """
        file_path = Path(self._storage_path, "episodic_info_df.csv")
        if not file_path.is_file():
            episodic_info_df.to_csv(file_path, index=False)
        else:
            episodic_info_df.to_csv(file_path, mode='a', header=False, index=False)

    @staticmethod
    def _process_subject(subject_id):
        """Processes data for a single subject to generate episodic and time series data.
        """
        subject_event_df_path = Path(storage_path_pr, str(subject_id), "subject_events.csv")
        subject_event_df = dataset_reader_pr.read_csv(
            subject_event_df_path, dtypes=DATASET_SETTINGS["subject_events"]["dtype"])

        if subject_ids_pr is not None:
            subject_event_df = subject_event_df[subject_event_df["SUBJECT_ID"].isin(subject_ids_pr)]

        if subject_event_df.empty or \
           not subject_id in subject_diagnoses_pr or \
           not subject_id in subject_icu_history_pr:
            return pd.DataFrame(columns=["SUBJECT_ID", "ICUSTAY_ID", "Height", "Weight"])

        curr_subject_event = {subject_id: subject_event_df}
        curr_subject_diagnoses = {subject_id: subject_diagnoses_pr[subject_id]}
        curr_icu_history_pr = {subject_id: subject_icu_history_pr[subject_id]}

        episodic_data, timeseries = extract_timeseries(curr_subject_event, curr_subject_diagnoses,
                                                    curr_icu_history_pr, varmap_df_pr)

        # Store processed subject events
        name_data_pairs = {
            "episodic_data": episodic_data,
            "timeseries": timeseries,
        }
        dataset_writer_pr.write_bysubject(name_data_pairs, exists_ok=True)

        info_dfs = list()
        # Return episodic information for compact storage as it is still needed for subsequent processing
        for subject_id, df in episodic_data.items():
            df["SUBJECT_ID"] = subject_id
            df = df.reset_index()
            df = df.rename(columns={"Icustay": "ICUSTAY_ID"})
            info_dfs.append(df[["SUBJECT_ID", "ICUSTAY_ID", "Height", "Weight"]])

        if not info_dfs:
            return pd.DataFrame(columns=["SUBJECT_ID", "ICUSTAY_ID", "Height", "Weight"])

        return pd.concat(info_dfs)

    @staticmethod
    def _init(storage_path: Path, subject_ids: list, diagnoses: dict, icu_history: dict,
              varmap: pd.DataFrame, dataset_reader: ExtractedSetReader,
              dataset_writer: DataSetWriter):
        """Initializes global variables for multiprocessing pool.
        """
        global storage_path_pr
        global subject_ids_pr
        global subject_diagnoses_pr
        global subject_icu_history_pr
        global varmap_df_pr
        global dataset_writer_pr
        global dataset_reader_pr
        storage_path_pr = storage_path
        subject_ids_pr = subject_ids
        subject_diagnoses_pr = diagnoses
        subject_icu_history_pr = icu_history
        varmap_df_pr = varmap
        dataset_reader_pr = dataset_reader
        dataset_writer_pr = dataset_writer

    def run(self):
        """Runs the time series processing using multiprocessing.

        This method initializes a multiprocessing pool to process subjects in parallel,
        generating episodic and time series data for each subject and storing the results.

        Examples
        --------
        >>> processor = TimeseriesProcessor(storage_path, source_path, tracker, subject_ids, diagnoses_df, icu_history_df, varmap_df, num_samples)
        >>> processor.run()
        """
        with Pool(cpu_count() - 1,
                  initializer=self._init,
                  initargs=(self._storage_path, self._subject_ids, self._subject_diagnoses,
                            self._subject_icu_history, self._varmap_df, self._dataset_reader,
                            self._dataset_writer)) as pool:
            res = pool.imap_unordered(self._process_subject, self._subject_ids, chunksize=500)

            episodic_info_df = None
            for index, info_df in enumerate(res):
                if episodic_info_df is None:
                    episodic_info_df = info_df
                else:
                    episodic_info_df = pd.concat([episodic_info_df, info_df])

                self._tracker.subject_ids.extend(info_df["SUBJECT_ID"].unique())
                info_io(f"Subject directories extracted: {len(self._tracker.subject_ids)}",
                        end="\r",
                        flush=True)

                if index % 100 == 0 and index != 0:
                    self._store_df_chunk(episodic_info_df)
                    episodic_info_df = None

            if episodic_info_df is not None:
                self._store_df_chunk(episodic_info_df)

            self._tracker.has_episodic_data = True
            self._tracker.has_timeseries = True
        return


### FILE: .\src\datasets\extraction\__init__.py ###
"""
Dataset Extraction Module
=========================

This module provides functions and classes for extracting and processing dataset files.
It supports both compact and iterative extraction methods to handle different use cases.

Functions
---------
- compact_extraction(storage_path, source_path, num_subjects, num_samples, subject_ids, task)
    Performs compact extraction of the dataset.
- iterative_extraction(source_path, storage_path, chunksize, num_subjects, num_samples, subject_ids, task)
    Performs iterative extraction of the dataset.
- read_patients_csv(dataset_folder)
    Reads the PATIENTS.csv file from the specified dataset folder.
- read_admission_csv(dataset_folder)
    Reads the ADMISSIONS.csv file from the specified dataset folder.
- read_icustays_csv(dataset_folder)
    Reads the ICUSTAYS.csv file from the specified dataset folder.
- read_icd9codes_csv(dataset_folder)
    Reads the D_ICD_DIAGNOSES.csv file from the specified dataset folder.
- read_events_dictionary(dataset_folder)
    Reads the D_ITEMS.csv file from the specified dataset folder.
- merge_patient_history(patients_df, admissions_df, icustays_df, min_nb_stays, max_nb_stays)
    Merges patient, admission, and ICU stay data to create a patient history DataFrame.
- make_subject_infos(patients_df, admission_info_df, icustays_df, min_nb_stays, max_nb_stays)
    Creates a DataFrame containing information about subjects by merging patient, admission, and ICU stay data.
- make_icu_history(patients_df, admissions_df, icustays_df, min_nb_stays, max_nb_stays)
    Creates a DataFrame describing each ICU stay with admission data, patient data, and mortality.
- make_diagnoses(dataset_folder, icd9codes_df, icu_history_df)
    Creates a DataFrame containing descriptions of diagnoses with direct links to subjects and ICU stays.
- make_phenotypes(diagnoses_df, definition_map)
    Creates a binary matrix with diagnoses over ICU stays.
- get_by_subject(df, sort_by)
    Groups events by subject ID.

Examples
--------
>>> from extraction_module import compact_extraction, iterative_extraction
>>> storage_path = Path("/path/to/storage")
>>> source_path = Path("/path/to/source")
>>> dataset_dict = compact_extraction(storage_path, source_path, num_subjects=100)
>>> reader = iterative_extraction(source_path, storage_path, chunksize=1000)
"""


import pandas as pd
import yaml
import warnings
import random

warnings.simplefilter(action='ignore', category=FutureWarning)

from copy import deepcopy
from pathlib import Path
from typing import List
from utils.IO import *
from settings import *
from .extraction_functions import extract_subject_events, extract_timeseries
from .event_producer import EventProducer
from .timeseries_processor import TimeseriesProcessor
from ..trackers import ExtractionTracker
from ..mimic_utils import *
from ..readers import ExtractedSetReader, EventReader
from ..writers import DataSetWriter


def compact_extraction(storage_path: Path,
                       source_path: Path,
                       num_subjects: int = None,
                       num_samples: int = None,
                       subject_ids: list = None,
                       task: str = None) -> dict:
    """
    Perform single shot extraction of the dataset from the original source. Ensure enough RAM is available.

    Parameters
    ----------
    storage_path : Path
        The path to the storage directory where the extracted data will be saved.
    source_path : Path
        The path to the source directory containing the raw data files.
    num_subjects : int, optional
        The number of subjects to extract. If None, all subjects are extracted. Default is None.
    num_samples : int, optional
        The number of samples to extract. If None, all samples are extracted. Default is None.
    subject_ids : list of int, optional
        List of subject IDs to extract. If None, all subjects are extracted. Default is None.
    task : str, optional
        Specific task to extract data for. If None, all tasks are extracted. Default is None.

    Returns
    -------
    dict
        A dictionary containing the extracted data.

    Examples
    --------
    >>> storage_path = Path("/path/to/storage")
    >>> source_path = Path("/path/to/source")
    >>> dataset = compact_extraction(storage_path, source_path, num_subjects=100)
    """
    original_subject_ids = deepcopy(subject_ids)
    resource_folder = Path(source_path, "resources")
    tracker = ExtractionTracker(storage_path=Path(storage_path, "progress"),
                                num_subjects=num_subjects,
                                num_samples=num_samples,
                                subject_ids=subject_ids)

    dataset_writer = DataSetWriter(storage_path)
    dataset_reader = ExtractedSetReader(storage_path)

    if tracker.is_finished:  # and not compute_data:
        info_io(f"Compact data extraction already finalized in directory:\n{str(storage_path)}")
        if task is not None:
            # Make sure we don't pick empty subjects for the subsequent processing
            icu_history_df = dataset_reader.read_csv(Path(storage_path, "icu_history.csv"),
                                                     dtypes=convert_dtype_dict(
                                                         DATASET_SETTINGS["icu_history"]["dtype"]))

            subject_ids = get_processable_subjects(task, icu_history_df)
            subject_ids = list(set(subject_ids) & set(tracker.subject_ids))
        else:
            subject_ids = tracker.subject_ids

        if num_subjects is not None and num_subjects < len(subject_ids):
            subject_ids = random.sample(subject_ids, k=num_subjects)
        elif original_subject_ids is not None:
            subject_ids = set(original_subject_ids) & set(subject_ids)
        # If we know some processing is comming after we return all possible subjects for that task
        return dataset_reader.read_subjects(read_ids=True, subject_ids=subject_ids)

    info_io("Compact Dataset Extraction: ALL", level=0)
    info_io(f"Starting compact data extraction.")
    info_io(f"Extracting data from source:\n{str(source_path)}")
    info_io(f"Saving data at location:\n{str(storage_path)}")

    # Read Dataframes for ICU history
    if tracker.has_icu_history:
        info_io("ICU history data already extracted")
        icu_history_df = dataset_reader.read_csv(Path(storage_path, "icu_history.csv"),
                                                 dtypes=convert_dtype_dict(
                                                     DATASET_SETTINGS["icu_history"]["dtype"]))
        subject_info_df = dataset_reader.read_csv(Path(storage_path, "subject_info.csv"),
                                                  dtypes=convert_dtype_dict(
                                                      DATASET_SETTINGS["subject_info"]["dtype"]))
    else:
        info_io("Extracting ICU history data")
        patients_df = read_patients_csv(source_path)
        admissions_df, admissions_info_df = read_admission_csv(source_path)
        icustays_df = read_icustays_csv(source_path)

        # Make ICU history dataframe
        subject_info_df = make_subject_infos(patients_df, admissions_info_df, icustays_df)
        icu_history_df = make_icu_history(patients_df, admissions_df, icustays_df)
        tracker.has_icu_history = True
        icu_history_df.to_csv(Path(storage_path, "icu_history.csv"), index=False)
        subject_info_df.to_csv(Path(storage_path, "subject_info.csv"), index=False)

    if tracker.has_diagnoses:
        info_io("Patient diagnosis data already extracted")
        diagnoses_df = dataset_reader.read_csv(Path(storage_path, "diagnoses.csv"),
                                               dtypes=convert_dtype_dict(
                                                   DATASET_SETTINGS["diagnosis"]["dtype"]))
    else:
        info_io("Extracting patient diagnosis data")
        # Read Dataframes for diagnoses
        icd9codes_df = read_icd9codes_csv(source_path)

        #
        diagnoses_df, definition_map = make_diagnoses(source_path, icd9codes_df, icu_history_df)
        diagnoses_df.to_csv(Path(storage_path, "diagnoses.csv"), index=False)

        tracker.has_diagnoses = True

    subject_ids, icu_history_df = get_subject_ids(task=task,
                                                  num_subjects=num_subjects,
                                                  subject_ids=subject_ids,
                                                  existing_subjects=tracker.subject_ids,
                                                  icu_history_df=icu_history_df)

    subject_info_df = reduce_by_subjects(subject_info_df, subject_ids)
    diagnoses_df = reduce_by_subjects(diagnoses_df, subject_ids)

    info_io("Extracting subject ICU history")
    subject_icu_history = get_by_subject(icu_history_df,
                                         DATASET_SETTINGS["ICUHISTORY"]["sort_value"])

    info_io("Extracting subject diagnoses")
    subject_diagnoses = get_by_subject(diagnoses_df[DATASET_SETTINGS["DIAGNOSES"]["columns"]],
                                       DATASET_SETTINGS["DIAGNOSES"]["sort_value"])
    if tracker.has_bysubject_info:
        info_io("Subject diagnoses and subject ICU history already stored")
    else:
        dataset_writer.write_bysubject(
            {
                "subject_icu_history": subject_icu_history,
                "subject_diagnoses": subject_diagnoses
            },
            index=False)

    if tracker.has_subject_events:
        info_io("Subject events already extracted")
        subject_events = dataset_reader.read_events(read_ids=True)
    else:
        info_io("Extracting subject events")
        # Read Dataframes for event table
        event_reader = EventReader(source_path)
        chartevents_df = event_reader.get_all()

        # Make subject event table
        subject_events = extract_subject_events(chartevents_df, icu_history_df)
        dataset_writer.write_bysubject({
            "subject_events": subject_events,
        }, index=False)
        tracker.has_subject_events = True

    if not tracker.has_timeseries or not tracker.has_episodic_data:
        info_io("Extracting subject timeseries and episodic data")
        # Read Dataframes for time series
        varmap_df = read_varmap_csv(resource_folder)
        episodic_data, timeseries = extract_timeseries(subject_events, subject_diagnoses,
                                                    subject_icu_history, varmap_df)
        name_data_pair = {"episodic_data": episodic_data, "timeseries": timeseries}
        dataset_writer.write_bysubject(name_data_pair, exists_ok=True)

        tracker.subject_ids.extend(list(timeseries.keys()))
        tracker.has_episodic_data = True
        tracker.has_timeseries = True
    else:
        info_io("Subject timeseries and episodic data already extracted")

    tracker.is_finished = True
    info_io(f"Finalized data extraction in directory:\n{str(storage_path)}")
    if original_subject_ids is not None:
        original_subject_ids = list(set(original_subject_ids) & set(tracker.subject_ids))
    return dataset_reader.read_subjects(read_ids=True, subject_ids=original_subject_ids)


def iterative_extraction(source_path: Path,
                         storage_path: Path = None,
                         chunksize: int = None,
                         num_subjects: int = None,
                         num_samples: int = None,
                         subject_ids: list = None,
                         task: str = None) -> ExtractedSetReader:
    """
    Perform iterative extraction of the dataset, with specified chunk size. This will require less RAM and run on multiple processes.

    Parameters
    ----------
    source_path : Path
        The path to the source directory containing the raw data files.
    storage_path : Path, optional
        The path to the storage directory where the extracted data will be saved. Default is None.
    chunksize : int, optional
        The size of chunks to read at a time. Default is None.
    num_subjects : int, optional
        The number of subjects to extract. If None, all subjects are extracted. Default is None.
    num_samples : int, optional
        The number of samples to extract. If None, all samples are extracted. Default is None.
    subject_ids : list of int, optional
        List of subject IDs to extract. If None, all subjects are extracted. Default is None.
    task : str, optional
        Specific task to extract data for. If None, all tasks are extracted. Default is None.

    Returns
    -------
    ExtractedSetReader
        The extracted set reader to access the dataset.

    Examples
    --------
    >>> source_path = Path("/path/to/source")
    >>> storage_path = Path("/path/to/storage")
    >>> reader = iterative_extraction(source_path, storage_path, chunksize=1000, num_subjects=100)
    """
    # Not sure
    original_subject_ids = deepcopy(subject_ids)
    resource_folder = Path(source_path, "resources")

    tracker = ExtractionTracker(storage_path=Path(storage_path, "progress"),
                                num_samples=num_samples,
                                num_subjects=num_subjects,
                                subject_ids=subject_ids)

    dataset_writer = DataSetWriter(storage_path)
    dataset_reader = ExtractedSetReader(source_path)

    if tracker.is_finished:
        info_io(f"Iterative data extraction already finalized in directory:\n{storage_path}.")
        if task is not None:
            # Make sure we don't pick empty subjects for the subsequent processing
            icu_history_df = dataset_reader.read_csv(Path(storage_path, "icu_history.csv"),
                                                     dtypes=convert_dtype_dict(
                                                         DATASET_SETTINGS["icu_history"]["dtype"]))

            subject_ids = get_processable_subjects(task, icu_history_df)
            subject_ids = list(set(subject_ids) & set(tracker.subject_ids))
        else:
            subject_ids = tracker.subject_ids

        if num_subjects is not None and num_subjects < len(subject_ids):
            subject_ids = random.sample(subject_ids, k=num_subjects)
        elif original_subject_ids is not None:
            subject_ids = set(original_subject_ids) & set(subject_ids)
        # If we know some processing is comming after we return all possible subjects for that task
        return ExtractedSetReader(storage_path, subject_ids=subject_ids)

    info_io("Iterative Dataset Extraction: ALL", level=0)
    info_io(f"Starting iterative data extraction.")
    info_io(f"Extracting data from source:\n{str(source_path)}")
    info_io(f"Saving data at location:\n{str(storage_path)}")

    # Make ICU history dataframe
    if tracker.has_icu_history:
        info_io("ICU history data already extracted")
        icu_history_df = dataset_reader.read_csv(Path(storage_path, "icu_history.csv"),
                                                 dtypes=convert_dtype_dict(
                                                     DATASET_SETTINGS["icu_history"]["dtype"]))
        subject_info_df = dataset_reader.read_csv(Path(storage_path, "subject_info.csv"),
                                                  dtypes=convert_dtype_dict(
                                                      DATASET_SETTINGS["subject_info"]["dtype"]))
    else:
        # Read Dataframes for ICU history
        info_io("Extracting ICU history data")
        patients_df = read_patients_csv(source_path)

        admissions_df, admission_info_df = read_admission_csv(source_path)
        icustays_df = read_icustays_csv(source_path)
        # Generate history
        subject_info_df = make_subject_infos(patients_df, admission_info_df, icustays_df)
        icu_history_df = make_icu_history(patients_df, admissions_df, icustays_df)

        # TODO! encapsualte in function
        icu_history_df.to_csv(Path(storage_path, "icu_history.csv"), index=False)
        subject_info_df.to_csv(Path(storage_path, "subject_info.csv"), index=False)
        tracker.has_icu_history = True

    # Read Dataframes for diagnoses

    if tracker.has_diagnoses:
        info_io("Patient diagnosis data already extracted")
        diagnoses_df = dataset_reader.read_csv(Path(storage_path, "diagnoses.csv"),
                                               dtypes=convert_dtype_dict(
                                                   DATASET_SETTINGS["diagnosis"]["dtype"]))
    else:
        info_io("Extracting Patient diagnosis data")

        icd9codes_df = read_icd9codes_csv(source_path)
        diagnoses_df, definition_map = make_diagnoses(source_path, icd9codes_df, icu_history_df)
        # TODO! not working
        # make_phenotypes(diagnoses_df,
        #                definition_map).to_csv(Path(storage_path, "phenotype_matrix.csv"))
        diagnoses_df.to_csv(Path(storage_path, "diagnoses.csv"), index=False)
        tracker.has_diagnoses = True

    subject_ids, icu_history_df = get_subject_ids(task=task,
                                                  num_subjects=num_subjects,
                                                  subject_ids=subject_ids,
                                                  existing_subjects=tracker.subject_ids,
                                                  icu_history_df=icu_history_df)

    diagnoses_df = reduce_by_subjects(diagnoses_df, subject_ids)
    subject_diagnoses = get_by_subject(diagnoses_df[DATASET_SETTINGS["DIAGNOSES"]["columns"]],
                                       DATASET_SETTINGS["DIAGNOSES"]["sort_value"])
    subject_icu_history = get_by_subject(icu_history_df,
                                         DATASET_SETTINGS["ICUHISTORY"]["sort_value"])
    if not tracker.has_subject_events:
        name_data_pairs = {
            "subject_diagnoses": {
                subject_id: frame_df for subject_id, frame_df in subject_diagnoses.items()
            },
            "subject_icu_history": {
                subject_id: frame_df for subject_id, frame_df in subject_icu_history.items()
            }
        }
        dataset_writer.write_bysubject(name_data_pairs, index=False)
    else:
        info_io("Subject diagnoses and subject ICU history already stored")

    if not tracker.has_subject_events:
        info_io("Extracting subject events")

        EventProducer(source_path=source_path,
                      storage_path=storage_path,
                      num_samples=num_samples,
                      chunksize=chunksize,
                      tracker=tracker,
                      icu_history_df=icu_history_df,
                      subject_ids=subject_ids).run()
    else:
        info_io("Subject events already extracted")

    varmap_df = read_varmap_csv(resource_folder)

    if not tracker.has_episodic_data or not tracker.has_timeseries:
        info_io("Extraction timeseries data from subject events")

        # Starting the processor pool
        pool_processor = TimeseriesProcessor(storage_path=storage_path,
                                             source_path=source_path,
                                             tracker=tracker,
                                             subject_ids=subject_ids,
                                             diagnoses_df=subject_diagnoses,
                                             icu_history_df=subject_icu_history,
                                             varmap_df=varmap_df,
                                             num_samples=num_samples)

        pool_processor.run()
        info_io(f"Subject directories extracted: {len(tracker.subject_ids)}")

        tracker.has_episodic_data = True
        tracker.has_timeseries = True
    else:
        info_io(f"Timeseries data already created")

    tracker.is_finished = True
    if original_subject_ids is not None:
        original_subject_ids = list(set(original_subject_ids) & set(tracker.subject_ids))
    return ExtractedSetReader(storage_path, subject_ids=original_subject_ids)


def reduce_by_subjects(dataframe: pd.DataFrame, subject_ids: list):
    """
    Reduce the dataframe to only include the specified subject IDs.

    Parameters
    ----------
    dataframe : pd.DataFrame
        The dataframe to reduce.
    subject_ids : list
        The list of subject IDs to retain in the dataframe.

    Returns
    -------
    pd.DataFrame
        The reduced dataframe containing only the specified subject IDs.

    Examples
    --------
    >>> df = pd.DataFrame({"SUBJECT_ID": [10006, 10011, 10019], "value": [10, 20, 30]})
    >>> reduce_by_subjects(df, [10006, 10019])
       SUBJECT_ID  value
    0           10006     10
    2           10019     30
    """
    if subject_ids is not None:
        return dataframe[dataframe["SUBJECT_ID"].isin(subject_ids)]
    return dataframe


def get_subject_ids(task: str,
                    icu_history_df: pd.DataFrame,
                    subject_ids: list = None,
                    num_subjects: int = None,
                    existing_subjects: list = None):

    """
    Get the subject IDs that can be processed for the given task. Many subjects will need to be
    discarded as they do not fulfill the minimum length of stay requirement of 48H for IHM, 4H for DECOMP and LOS.

    Parameters
    ----------
    task : str
        The task for which to get the subject IDs.
    icu_history_df : pd.DataFrame
        The dataframe containing ICU history information.
    subject_ids : list of int, optional
        List of subject IDs to extract. If None, all subjects are extracted. Default is None.
    num_subjects : int, optional
        The number of subjects to extract. If None, all subjects are extracted. Default is None.
    existing_subjects : list of int, optional
        List of existing subjects to exclude from extraction. Default is None.

    Returns
    -------
    tuple
        A tuple containing the list of subject IDs and the reduced ICU history dataframe.
    """
    if existing_subjects is None:
        existing_subjects = []

    if subject_ids is not None:
        all_subjects = get_processable_subjects(task, icu_history_df)
        # Notify unknowns
        unknown_subjects = set(subject_ids) - set(icu_history_df["SUBJECT_ID"].unique())
        if unknown_subjects:
            warn_io(f"Unknown subjects passed as parameter: {*unknown_subjects,}")
        # Notify unprocessable
        unprocessable_subjects = set(subject_ids) - set(all_subjects)
        if unprocessable_subjects:
            warn_io(f"Unprocessable subjects passed as parameter: {*unprocessable_subjects,}")
        # Remove already processed
        subject_ids = list((set(subject_ids) - set(existing_subjects)) & set(all_subjects))
        icu_history_df = reduce_by_subjects(icu_history_df, subject_ids)
    elif num_subjects is not None:
        all_subjects = get_processable_subjects(task, icu_history_df)
        subject_ids = get_subjects_by_number(task, num_subjects, existing_subjects, all_subjects)
        icu_history_df = reduce_by_subjects(icu_history_df, subject_ids)
    else:
        subject_ids = None
    return subject_ids, icu_history_df


def get_subjects_by_number(task: str, num_subjects: int, existing_subjects: List[int],
                           all_subjects: List[int]):
    """
    Get a specified number of subject IDs, excluding the existing subjects.
    """
    # Determined how many are missing
    num_subjects = max(0, num_subjects - len(existing_subjects))
    # Chose from uprocessed subjects
    if num_subjects > len(all_subjects):
        raise warn_io(
            f"Number of subjects requested exceeds available subjects: {len(all_subjects)}")
    remaining_subjects = list(set(all_subjects) - set(existing_subjects))
    # if tracker is None we grab all possible subjects for return to the next processing step
    subject_ids = random.sample(remaining_subjects, k=num_subjects)
    assert len(subject_ids) == num_subjects
    return subject_ids


def get_processable_subjects(task: str, icu_history_df: pd.DataFrame):
    """
    Get the subject IDs that can be processed for a given task.
    """
    if task is not None and "label_start_time" in DATASET_SETTINGS[task]:
        # Some ids will be removed during the preprocessing step
        # We remove them here to avoid errors
        min_los = DATASET_SETTINGS[task]["label_start_time"] + \
            DATASET_SETTINGS[task]["sample_precision"]
        min_los /= 24
        icu_history_df = icu_history_df[icu_history_df["LOS"] >= min_los]
        return icu_history_df[((icu_history_df["DISCHTIME"] - icu_history_df["ADMITTIME"])
                               >= pd.Timedelta(days=min_los))]["SUBJECT_ID"].unique()
    else:
        return icu_history_df["SUBJECT_ID"].unique()


def create_split_info_csv(episodic_info_df: pd.DataFrame, subject_info_df: pd.DataFrame):
    """Create the information used by the dataset split function to split the subjects into demographics groups.
    """
    episodic_info_df["SUBJECT_ID"] = episodic_info_df["SUBJECT_ID"].astype(int)
    episodic_info_df = episodic_info_df.merge(subject_info_df,
                                              how='inner',
                                              left_on=['SUBJECT_ID', 'ICUSTAY_ID'],
                                              right_on=['SUBJECT_ID', 'ICUSTAY_ID'])
    episodic_info_df.to_csv("split_info.csv")

    return


def read_patients_csv(dataset_folder: Path):
    """
    Reads the PATIENTS.csv file from the specified dataset folder with correct dtypes and columns.

    Parameters
    ----------
    dataset_folder : Path
        Path to the dataset folder. Defaults to 'data/mimic-iii-demo/'.

    Returns
    -------
    pd.DataFrame
        DataFrame containing patient data including birth, death, gender, and ethnicity.

    Notes
    -----
    - Column names are converted to uppercase.
    - Columns specified in the configuration are selected.
    - Date columns are converted to datetime.
    """
    csv_settings = DATASET_SETTINGS["PATIENTS"]
    patients_df = pd.read_csv(Path(dataset_folder, "PATIENTS.csv"),
                              dtype=convert_dtype_dict(csv_settings["dtype"]))

    patients_df = upper_case_column_names(patients_df)
    patients_df = patients_df[csv_settings["columns"]].copy()

    for column in csv_settings["convert_datetime"]:
        patients_df[column] = pd.to_datetime(patients_df[column])

    return patients_df


def read_admission_csv(dataset_folder: Path):
    """
    Reads the ADMISSIONS.csv file from the specified dataset folder with correct dtypes and columns.

    Parameters
    ----------
    dataset_folder : Path
        Path to the dataset folder. Defaults to 'data/mimic-iii-demo/'.

    Returns
    -------
    tuple of pd.DataFrame
        A tuple containing:
            - admissions_df: DataFrame with hospital admissions data including admission, discharge, type, and location.
            - admissions_info_df: DataFrame with additional admission information.

    Notes
    -----
    - Column names are converted to uppercase.
    - Columns specified in the configuration are selected.
    - Date columns are converted to datetime.
    """
    csv_settings = DATASET_SETTINGS["ADMISSIONS"]

    admissions_df = pd.read_csv(Path(dataset_folder, "ADMISSIONS.csv"),
                                dtype=convert_dtype_dict(csv_settings["dtype"]))
    admissions_df = upper_case_column_names(admissions_df)
    for column in csv_settings["convert_datetime"]:
        admissions_df[column] = pd.to_datetime(admissions_df[column])

    admissions_info_df = admissions_df[csv_settings["info_columns"]]
    admissions_df = admissions_df[csv_settings["columns"]].copy()

    return admissions_df, admissions_info_df


def read_icustays_csv(dataset_folder: Path):
    """
    Reads the ICUSTAYS.csv file from the specified dataset folder with correct dtypes and columns.

    Parameters
    ----------
    dataset_folder : Path
        Path to the dataset folder. Defaults to 'data/mimic-iii-demo/'.

    Returns
    -------
    pd.DataFrame
        DataFrame with ICU admission data including first & last care unit, ward ID, etc.

    Notes
    -----
    - Column names are converted to uppercase.
    - Columns specified in the configuration are selected.
    - Date columns are converted to datetime.
    """
    csv_settings = DATASET_SETTINGS["ICUSTAYS"]

    icustays_df = pd.read_csv(Path(dataset_folder, "ICUSTAYS.csv"),
                              dtype=convert_dtype_dict(csv_settings["dtype"]))
    icustays_df = upper_case_column_names(icustays_df)
    for column in csv_settings["convert_datetime"]:
        icustays_df[column] = pd.to_datetime(icustays_df[column])

    return icustays_df


def read_icd9codes_csv(dataset_folder: Path):
    """
    Reads the D_ICD_DIAGNOSES.csv file from the specified dataset folder.

    Parameters
    ----------
    dataset_folder : Path
        Path to the dataset folder. Defaults to 'data/mimic-iii-demo/'.

    Returns
    -------
    pd.DataFrame
        DataFrame with dictionaries describing the ICD9 codes.

    Notes
    -----
    - Column names are converted to uppercase.
    - Columns specified in the configuration are selected.
    -  International Classification of Diseases, Ninth Revision, are alphanumeric codes used by healthcare providers to classify and code all diagnoses, symptoms, and procedures recorded in conjunction with hospital care in the United States
    """
    csv_settings = DATASET_SETTINGS["ICD9CODES"]

    icd9codes_df = pd.read_csv(Path(dataset_folder, 'D_ICD_DIAGNOSES.csv'),
                               dtype=convert_dtype_dict(csv_settings["dtype"]))
    icd9codes_df = upper_case_column_names(icd9codes_df)
    icd9codes_df = icd9codes_df[csv_settings["columns"]]

    return icd9codes_df


def read_events_dictionary(dataset_folder: Path):
    """
    Reads the D_ITEMS.csv file from the specified dataset folder.

    Parameters
    ----------
    dataset_folder : Path
        Path to the dataset folder. Defaults to 'data/mimic-iii-demo/'.

    Returns
    -------
    pd.DataFrame
        DataFrame containing the items dictionary with ITEMID and DBSOURCE columns.

    Notes
    -----
    - Column names are converted to uppercase.
    - Only the ITEMID and DBSOURCE columns are selected.
    """
    csv_settings = DATASET_SETTINGS["D_ITEMS"]
    dictionary_df = pd.read_csv(Path(dataset_folder, "D_ITEMS.csv"),
                                dtype=convert_dtype_dict(csv_settings["dtype"]))
    dictionary_df = upper_case_column_names(dictionary_df)
    dictionary_df = dictionary_df[["ITEMID", "DBSOURCE"]]

    return dictionary_df


def merge_patient_history(patients_df, admissions_df, icustays_df, min_nb_stays,
                          max_nb_stays) -> pd.DataFrame:
    """
    Merges patient, admission, and ICU stay data to create a patient history DataFrame of ICU stays, like admission time and mortality.

    Parameters
    ----------
    patients_df : pd.DataFrame
        DataFrame containing patient data.
    admissions_df : pd.DataFrame
        DataFrame containing admission data.
    icustays_df : pd.DataFrame
        DataFrame containing ICU stay data.
    min_nb_stays : int
        Minimum number of ICU stays to include a patient.
    max_nb_stays : int
        Maximum number of ICU stays to include a patient.

    Returns
    -------
    pd.DataFrame
        Merged DataFrame with patient history.

    Notes
    -----
    - Only ICU stays where the first and last care unit and ward ID match are included.
    - Filters patients based on the number of ICU stays between min_nb_stays and max_nb_stays.
    - Calculates patient age and filters out children (age < 18).
    """
    icustays_df = icustays_df[icustays_df["FIRST_CAREUNIT"] == icustays_df["LAST_CAREUNIT"]]
    icustays_df = icustays_df[icustays_df["FIRST_WARDID"] == \
                                    icustays_df["LAST_WARDID"]]

    patient_history_df = icustays_df.merge(admissions_df,
                                           how='inner',
                                           left_on=['SUBJECT_ID', 'HADM_ID'],
                                           right_on=['SUBJECT_ID', 'HADM_ID'])
    patient_history_df = patient_history_df.merge(patients_df,
                                                  how='inner',
                                                  left_on=['SUBJECT_ID'],
                                                  right_on=['SUBJECT_ID'])

    filter = patient_history_df.groupby('HADM_ID').count()[['ICUSTAY_ID']].reset_index()
    filter = filter.loc[(filter.ICUSTAY_ID >= min_nb_stays)
                        & (filter.ICUSTAY_ID <= max_nb_stays)][['HADM_ID']]
    patient_history_df = patient_history_df.merge(filter,
                                                  how='inner',
                                                  left_on='HADM_ID',
                                                  right_on='HADM_ID')

    patient_history_df['AGE'] = (patient_history_df.INTIME.dt.year - patient_history_df.DOB.dt.year)
    patient_history_df.loc[patient_history_df.AGE < 0, 'AGE'] = 90
    # Filter out children
    # patient_history_df = patient_history_df[patient_history_df.AGE > 18]

    return patient_history_df


def make_subject_infos(patients_df,
                       admission_info_df,
                       icustays_df,
                       min_nb_stays=1,
                       max_nb_stays=1) -> pd.DataFrame:
    """
    Creates a DataFrame containing information about subjects by merging patient, admission, and ICU stay data, like gender and insurance.

    Parameters
    ----------
    patients_df : pd.DataFrame
        DataFrame containing patient data.
    admission_info_df : pd.DataFrame
        DataFrame containing additional admission information.
    icustays_df : pd.DataFrame
        DataFrame containing ICU stay data.
    min_nb_stays : int, optional
        Minimum number of ICU stays to include a patient, by default 1.
    max_nb_stays : int, optional
        Maximum number of ICU stays to include a patient, by default 1.

    Returns
    -------
    pd.DataFrame
        DataFrame containing subject information with specified columns.

    Notes
    -----
    - Merges patient history data.
    - Selects and renames columns according to the configuration.
    """
    csv_settings = DATASET_SETTINGS["subject_info"]
    subject_info_df = merge_patient_history(patients_df, admission_info_df, icustays_df,
                                            min_nb_stays, max_nb_stays)
    subject_info_df = subject_info_df[csv_settings["columns"]]
    subject_info_df = subject_info_df.rename(columns={
        "FIRST_WARDID": "WARDID",
        "FIRST_CAREUNIT": "CAREUNIT"
    })

    return subject_info_df


def make_icu_history(patients_df,
                     admissions_df,
                     icustays_df,
                     min_nb_stays=1,
                     max_nb_stays=1) -> pd.DataFrame:
    """
    Creates a DataFrame describing each ICU stay with admission data, patient data, and mortality.

    Parameters
    ----------
    patients_df : pd.DataFrame
        DataFrame containing patient data.
    admissions_df : pd.DataFrame
        DataFrame containing hospital admissions data.
    icustays_df : pd.DataFrame
        DataFrame containing ICU stay data.
    min_nb_stays : int, optional
        Minimum number of ICU stays to include a patient, by default 1.
    max_nb_stays : int, optional
        Maximum number of ICU stays to include a patient, by default 1.

    Returns
    -------
    pd.DataFrame
        DataFrame describing each ICU stay with admission data, patient data, and mortality.
    """
    csv_settings = DATASET_SETTINGS["icu_history"]
    icu_history_df = merge_patient_history(patients_df, admissions_df, icustays_df, min_nb_stays,
                                           max_nb_stays)

    # Inunit mortality
    mortality = icu_history_df.DOD.notnull() & ((icu_history_df.INTIME <= icu_history_df.DOD) &
                                                (icu_history_df.OUTTIME >= icu_history_df.DOD))

    mortality = mortality | (icu_history_df.DEATHTIME.notnull() &
                             ((icu_history_df.INTIME <= icu_history_df.DEATHTIME) &
                              (icu_history_df.OUTTIME >= icu_history_df.DEATHTIME)))
    icu_history_df['MORTALITY_INUNIT'] = mortality.astype(int)

    # Inhospital mortality
    mortality = icu_history_df.DOD.notnull() & ((icu_history_df.ADMITTIME <= icu_history_df.DOD) &
                                                (icu_history_df.DISCHTIME >= icu_history_df.DOD))
    mortality = mortality | (icu_history_df.DEATHTIME.notnull() &
                             ((icu_history_df.ADMITTIME <= icu_history_df.DEATHTIME) &
                              (icu_history_df.DISCHTIME >= icu_history_df.DEATHTIME)))
    icu_history_df['MORTALITY'] = mortality.astype(int)
    icu_history_df['MORTALITY_INHOSPITAL'] = mortality.astype(int)

    icu_history_df = icu_history_df[csv_settings["columns"]]
    icu_history_df = icu_history_df[icu_history_df.AGE >= 18]

    return icu_history_df


def make_diagnoses(dataset_folder, icd9codes_df, icu_history_df):
    """
    Creates a DataFrame containing descriptions of diagnoses with direct links to subjects and ICU stays.

    Parameters
    ----------
    dataset_folder : str
        Path to the dataset folder. Defaults to 'data/mimic-iii-demo/'.
    icd9codes_df : pd.DataFrame
        DataFrame containing ICD-9 code definitions.
    icu_history_df : pd.DataFrame
        DataFrame containing ICU stay descriptions.

    Returns
    -------
    tuple of pd.DataFrame and dict
        - diagnoses_df: DataFrame containing descriptions of diagnoses with links to subjects and ICU stays.
        - definition_map: Dictionary mapping ICD-9 codes to phenotypes and benchmark usage.
    """
    csv_settings = DATASET_SETTINGS["DIAGNOSES_ICD"]
    diagnoses_df = pd.read_csv(Path(dataset_folder, 'DIAGNOSES_ICD.csv'),
                               dtype=convert_dtype_dict(csv_settings["dtype"]))
    diagnoses_df = upper_case_column_names(diagnoses_df)
    diagnoses_df = diagnoses_df.merge(icd9codes_df,
                                      how='inner',
                                      left_on='ICD9_CODE',
                                      right_on='ICD9_CODE')
    diagnoses_df = diagnoses_df.merge(icu_history_df[['SUBJECT_ID', 'HADM_ID',
                                                      'ICUSTAY_ID']].drop_duplicates(),
                                      how='inner',
                                      left_on=['SUBJECT_ID', 'HADM_ID'],
                                      right_on=['SUBJECT_ID', 'HADM_ID'])

    diagnoses_df[['SUBJECT_ID', 'HADM_ID',
                  'SEQ_NUM']] = diagnoses_df[['SUBJECT_ID', 'HADM_ID', 'SEQ_NUM']].astype(int)

    # Clinical Classifications Software CSS defintions of phenotypes
    with Path(dataset_folder, "resources", "hcup_ccs_2015_definitions.yaml").open("r") as file:
        phenotypes_yaml = yaml.full_load(file)

    # Create definition map
    definition_map = dict()

    for phenotype in phenotypes_yaml:
        for code in phenotypes_yaml[phenotype]['codes']:
            definition_map[code] = (phenotype, phenotypes_yaml[phenotype]['use_in_benchmark'])

    diagnoses_df['HCUP_CCS_2015'] = diagnoses_df.ICD9_CODE.apply(lambda c: definition_map[c][0]
                                                                 if c in definition_map else None)
    diagnoses_df['USE_IN_BENCHMARK'] = diagnoses_df.ICD9_CODE.apply(
        lambda c: int(definition_map[c][1]) if c in definition_map else None)

    return diagnoses_df, definition_map


def make_phenotypes(diagnoses_df, definition_map):
    """
    Creates a binary matrix with diagnoses over ICU stays.

    Parameters
    ----------
    diagnoses_df : pd.DataFrame
        DataFrame containing diagnoses descriptions with ICU stay information.
    definition_map : dict
        Dictionary mapping ICD-9 codes to phenotypes and benchmark usage.

    Returns
    -------
    pd.DataFrame
        Binary matrix with diagnoses over ICU stays.
    """
    # Merge definitions to diagnoses
    phenotype_dictionary_df = pd.DataFrame(definition_map).T.reset_index().rename(columns={
        'index': 'ICD9_CODE',
        0: 'HCUP_CCS_2015',
        1: 'USE_IN_BENCHMARK'
    })
    phenotype_dictionary_df['use_in_benchmark'] = phenotype_dictionary_df[
        'use_in_benchmark'].astype(int)

    phenotypes_df = diagnoses_df.merge(phenotype_dictionary_df,
                                       how='inner',
                                       left_on='ICD9_CODE',
                                       right_on='ICD9_CODE')

    # Extract phenotypes from diagnoses
    phenotypes_df = phenotypes_df[['ICUSTAY_ID', 'HCUP_CCS_2015'
                                  ]][phenotypes_df.use_in_benchmark > 0].drop_duplicates()
    phenotypes_df['VALUE'] = 1

    # Definitions again icu stays
    phenotypes_df = phenotypes_df.pivot(index='icuystay_id',
                                        columns='HCUP_CCS_2015',
                                        values='VALUE')

    # Impute values and sort axes
    phenotypes_df = phenotypes_df.fillna(0).astype(int).sort_index(axis=0).sort_index(axis=1)

    return phenotypes_df


def get_by_subject(df, sort_by):
    """
    Groups events by subject ID.
    """
    return {id: x for id, x in df.sort_values(by=sort_by).groupby('SUBJECT_ID')}


if __name__ == "__main__":
    # subject_groups("/home/amadou/Data/ml_data/research-internship/mimic-iii-demo")
    iterative_extraction(
        storage_path=Path("/home/amadou/CodeWorkspace/data/research-internship/processed-trials"),
        source_path=Path("/home/amadou/CodeWorkspace/data/mimic-iii-demo"),
        ehr=None,
        from_storage=False,
        chunksize=10000,
        num_subjects=None)


### FILE: .\src\datasets\processors\discretizers.py ###

import pandas as pd
import numpy as np
import os
import json
import datetime
from multiprocess import Manager
from pathlib import Path
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from pandas.api.types import is_datetime64_any_dtype as is_datetime
from utils import dict_subset

from settings import *
from utils.IO import *
from datasets.readers import ProcessedSetReader
from datasets.writers import DataSetWriter
from datasets.mimic_utils import convert_dtype_value
from datasets.trackers import PreprocessingTracker
from datasets.processors import AbstractProcessor


class MIMICDiscretizer(AbstractProcessor):
    """
    Discretize batch data provided by the reader.

    This class is responsible for discretizing time series data for a specific task,
    provided by a `ProcessedSetReader`, and saving the processed data to a specified
    storage path. The discretizer can handle various imputation strategies and modes
    for processing, bins the data into specified time step intervals and on-hot encodes 
    categorical data.

    Parameters
    ----------
    task : str
        The name of the task. Must be one of the predefined `TASK_NAMES`.
    reader : ProcessedSetReader
        The reader object used to read the batch data.
    tracker : PreprocessingTracker
        The tracker object used to track preprocessing steps.
    storage_path : Path
        The path where the discretized data will be stored.
    time_step_size : float, optional
        The size of each time step for discretization, by default None.
    start_at_zero : bool, optional
        Whether to start the time index at zero, by default True.
    impute_strategy : str, optional
        The strategy for imputing missing values. Can be one of 'normal', 'previous', 'next', or 'zero'.
        Default is 'previous'.
    mode : str, optional
        The mode of discretization. Can be either 'legacy' or 'experimental'. Default is 'legacy'.
    eps : float, optional
        A small value to avoid division by zero errors, by default 1e-6.
    verbose : bool, optional
        If True, print verbose logs during processing. Default is False.
    """

    def __init__(self,
                 task: str,
                 reader: ProcessedSetReader = None,
                 storage_path: Path = None,
                 tracker: PreprocessingTracker = None,
                 time_step_size: float = None,
                 start_at_zero: bool = True,
                 impute_strategy: str = "previous",
                 mode: str = "legacy",
                 eps: float = 1e-6,
                 verbose: bool = False):
        self._storage_path = storage_path
        self._writer = (None if storage_path is None else DataSetWriter(self._storage_path))
        self._source_reader = reader
        if tracker is not None:
            self._tracker = tracker
        else:
            self._tracker = (None if storage_path is None else PreprocessingTracker(
                Path(storage_path, "progress")))
        self._lock = Manager().Lock()
        self._verbose = verbose
        self._discretized_reader = (None if storage_path is None else ProcessedSetReader(
            root_path=storage_path))

        self._time_step_size = time_step_size
        self._start_at_zero = start_at_zero
        self._eps = eps
        if not impute_strategy in ["normal", "previous", "next", "zero"]:
            raise ValueError(
                f"Impute strategy must be one of 'normal', 'previous', 'zero' or 'next'. Impute strategy is {impute_strategy}"
            )
        self._impute_strategy = impute_strategy
        if not mode in ["legacy", "experimental"]:
            raise ValueError(f"Mode must be one of 'legacy' or 'experimental'. Mode is {mode}")
        if mode == "experimental":
            raise NotADirectoryError("Implemented but untested. Will yield nan values.")
        self._mode = mode
        if not task in TASK_NAMES:
            raise ValueError(f"Task name must be one of {TASK_NAMES}. Task name is {task}")
        self._task = task

        with open(Path(os.getenv("CONFIG"), "datasets.json")) as file:
            config_dictionary = json.load(file)
            self._dtypes = config_dictionary["timeseries"]["dtype"]
        with open(Path(os.getenv("CONFIG"), "discretizer_config.json")) as file:
            config_dictionary = json.load(file)
            self._possible_values = config_dictionary['possible_values']
            self._is_categorical = config_dictionary['is_categorical_channel']
            self._impute_values = config_dictionary['normal_values']

    @property
    def tracker(self) -> PreprocessingTracker:
        """
        Get the preprocessing tracker object.

        This tracker keeps a record of the preprocessing steps applied to the data.

        Returns
        -------
        PreprocessingTracker
            The preprocessing tracker.
        """
        return self._tracker

    @property
    def subjects(self) -> list:
        """
        Get the list of subject IDs available in the reader.

        This property retrieves the subject IDs from the reader object.

        Returns
        -------
        list
            A list of subject IDs.
        """
        if self._source_reader is None:
            return []
        return self._source_reader.subject_ids

    def save_data(self, subjects: list = None) -> None:
        """
        Save the discretized data to the storage path.

        If no subjects are specified, all the discretized data will be saved.

        Parameters
        ----------
        subjects : list, optional
            A list of subject IDs to save data for. If None, all data is saved. Default is None.
        """
        if self._writer is None:
            info_io("No storage path provided. Data will not be saved.")
            return
        with self._lock:
            if subjects is None:
                self._writer.write_bysubject({"X": self._X_discretized}, file_type="hdf5")
                self._writer.write_bysubject({"y": self._y_discretized}, file_type="hdf5")
            else:
                self._writer.write_bysubject({"X": dict_subset(self._X_discretized, subjects)},
                                             file_type="hdf5")
                self._writer.write_bysubject({"y": dict_subset(self._y_discretized, subjects)},
                                             file_type="hdf5")

        return

    def transform_subject(self, subject_id: int):
        """
        Transform the data for a specific subject.

        This method reads the data for a specific subject, processes it, and returns
        the discretized data along with tracking information.

        Parameters
        ----------
        subject_id : int
            The ID of the subject to transform data for.

        Returns
        -------
        tuple
            A tuple containing the discretized data and the tracking information.
        """
        X_processed, y_processed = self._source_reader.read_sample(subject_id,
                                                            read_ids=True,
                                                            data_type=pd.DataFrame).values()
        X = {subject_id: X_processed}
        y = {subject_id: y_processed}

        X_discretized = self.transform(X, y)
        if X_discretized is None:
            return None, None
        if self._tracker is None:
            return X_discretized, y

        with self._lock:
            tracking_info = self._tracker.subjects[subject_id]
        return (X_discretized, y), tracking_info

    def transform(self, X_dict, y_dict):
        """
        Transform the entire dataset when passed as dictionary pair.

        This method processes the entire dataset by discretizing the time series data
        and applying the specified imputation strategy.

        Parameters
        ----------
        X_dict : dict
            A dictionary containing the input data, with subject IDs as keys.
        y_dict : dict
            A dictionary containing the output data, with subject IDs as keys.

        Returns
        -------
        dict
            A dictionary containing the discretized data, with subject IDs as keys.
        """
        n_subjects = 0
        n_stays = 0
        n_samples = 0
        n_skip = 0

        if self._verbose:
            info_io(f"Discretizing processed data:\n"
                    f"Discretized subjects: {0}\n"
                    f"Discretized stays: {0}\n"
                    f"Discretized samples: {0}\n"
                    f"Skipped subjects: {0}")

        self._samples_processed = 0

        self._X_discretized = dict()
        self._y_discretized = dict()

        for subject_id in X_dict.keys():
            X_subject = X_dict[subject_id]
            self._X_discretized[subject_id] = dict()
            self._y_discretized[subject_id] = dict()
            tracking_info = dict()

            for stay_id in X_subject:
                X_df = X_subject[stay_id]
                if self._mode == "experimental" and self._impute_strategy in ["previous", "next"]:
                    X_df = self._impute_data(X_df)
                    X_df = self._categorize_data(X_df)
                    X_df = self._bin_data(X_df)
                else:
                    X_df = self._categorize_data(X_df)
                    X_df = self._bin_data(X_df)
                    X_df = self._impute_data(X_df)
                self._X_discretized[subject_id][stay_id] = X_df
                self._y_discretized[subject_id][stay_id] = y_dict[subject_id][stay_id]

                tracking_info[stay_id] = len(y_dict[subject_id][stay_id])

                if self._verbose:
                    info_io(
                        f"Discretizing processed data:\n"
                        f"Discretized subjects: {n_subjects}\n"
                        f"Discretized stays: {n_stays}\n"
                        f"Discretized samples: {n_samples}"
                        f"Skipped subjects: {n_skip}",
                        flush_block=True)

            n_subjects += 1
            if self._tracker is not None:
                with self._lock:
                    self._tracker.subjects.update({subject_id: tracking_info})

            if not len(self._y_discretized[subject_id]) or not len(self._X_discretized[subject_id]):
                del self._y_discretized[subject_id]
                del self._X_discretized[subject_id]
                n_skip += 1
            else:
                n_subjects += 1

        if self._verbose:
            info_io(
                f"Discretizing processed data:\n"
                f"Discretized subjects: {n_subjects}\n"
                f"Discretized stays: {n_stays}\n"
                f"Discretized samples: {n_samples}"
                f"Skipped subjects: {n_skip}",
                flush_block=True)

        return self._X_discretized

    def _bin_data(self, X):
        """
        Bin the time series data into discrete time steps.
        """

        if self._time_step_size is not None:
            # Get data frame parameters
            start_timestamp = (0 if self._start_at_zero else X.index[0])

            if is_datetime(X.index):
                ts = list((X.index - start_timestamp) / datetime.timedelta(hours=1))
            else:
                ts = list(X.index - start_timestamp)

            # Maps sample_periods to discretization bins
            tsid_to_bins = list(map(lambda x: int(x / self._time_step_size - self._eps), ts))
            # Tentative solution
            if self._start_at_zero:
                N_bins = tsid_to_bins[-1] + 1
            else:
                N_bins = tsid_to_bins[-1] - tsid_to_bins[0] + 1

            # Reduce DataFrame to bins, keep original channels
            X['bins'] = tsid_to_bins
            X = X.groupby('bins').last()
            X = X.reindex(range(N_bins))

        # return categorized_data
        return X

    def _impute_data(self, X):
        """
        Impute missing values in the time series data using the specified
        imputation strategy.
        """
        if self._start_at_zero:
            tsid_to_bins = list(map(lambda x: int(x / self._time_step_size - self._eps), X.index))
            start_count = 0
            while not start_count in tsid_to_bins:
                start_count += 1
                X = pd.concat([pd.DataFrame(pd.NA, index=[0], columns=X.columns, dtype="Int32"), X])

        if isinstance(X, np.ndarray):
            X = pd.DataFrame(X, columns=self._possible_values.keys())
        if self._impute_strategy == "normal":
            for column in X:
                if column in self._impute_values.keys():
                    X[column] = X[column].replace(
                        np.nan,
                        convert_dtype_value(self._impute_values[column], self._dtypes[column]))
                elif column.split("->")[0] in self._impute_values.keys():
                    cat_name, cat_value = column.split("->")
                    X[column] = X[column].replace(
                        np.nan, (1.0 if cat_value == self._impute_values[cat_name] else 0.0))

        elif self._impute_strategy in ["previous", "next"]:
            X = (X.ffill() if self._impute_strategy == "previous" else X.bfill())
            for column in X:
                if X[column].isnull().values.any():
                    if column in self._impute_values:
                        impute_value = convert_dtype_value(self._impute_values[column],
                                                           self._dtypes[column])
                    else:
                        cat_name, cat_value = column.split("->")
                        impute_value = (1.0 if cat_value == self._impute_values[cat_name] else 0.0)
                    X[column] = X[column].replace(np.nan, impute_value)
        elif self._impute_strategy == "zero":
            X = X.fillna(0)
        return X

    def _categorize_data(self, X):
        """
        This method transforms categorical variables in the data into a format suitable
        for machine learning models by applying one-hot encoding.

        Parameters
        ----------
        X : pd.DataFrame
            The input data with categorical variables.

        Returns
        -------
        pd.DataFrame
            The data with categorical variables transformed into one-hot encoded format.
        """
        categorized_data = X

        for column in X:
            if not self._is_categorical[column]:
                continue

            categories = [str(cat) for cat in self._possible_values[column]]

            # Finding nan indices
            nan_indices = categorized_data[column].isna()
            if not nan_indices.all() and nan_indices.any():
                # Handling non-NaN values: encode them
                non_nan_values = categorized_data.pop(column).dropna().astype(
                    'string').values.reshape(-1, 1)
                encoder = OneHotEncoder(categories=list(
                    np.array(categories).reshape(1, len(categories))),
                                        handle_unknown='ignore')
                encoded_values = encoder.fit_transform(non_nan_values).toarray()

                # Initialize an array to hold the final encoded values including NaNs
                encoded_channel = np.full((categorized_data.shape[0], len(categories)), np.nan)

                # Fill in the encoded values at the right indices
                non_nan_indices = ~nan_indices
                encoded_channel[non_nan_indices] = encoded_values
            elif not nan_indices.any():
                encoder = OneHotEncoder(categories=[np.array(categories).reshape(-1)],
                                        handle_unknown='ignore')
                encoded_channel = encoder.fit_transform(
                    categorized_data.pop(column).astype('string').values.reshape(-1, 1)).toarray()
            else:
                categorized_data.pop(column)
                encoded_channel = np.full((categorized_data.shape[0], len(categories)), np.nan)

            categorized_data = pd.concat([\
                categorized_data,
                pd.DataFrame(\
                    encoded_channel,
                    columns=[f"{column}->{category}" for category in categories],
                    index=categorized_data.index)
                ],
                axis=1)

        return categorized_data


### FILE: .\src\datasets\processors\feature_engines.py ###
"""
This module provides the MIMICFeatureEngine class to process time series data from the MIMIC-III dataset, applying various feature engineering techniques and storing the processed data for downstream tasks.

Usage Examples
--------------
.. code-block:: python

    from pathlib import Path
    import json
    from datasets.readers import ProcessedSetReader
    from datasets.trackers import PreprocessingTracker
    from datasets.processors import MIMICFeatureEngine

    # Define the path to the dataset, storage, and configuration file
    dataset_path = Path("/path/to/extracted/dataset")
    storage_path = Path("/path/to/store/processed/data")

    # example file located at etc/engineering_config.json
    config_path = Path("/path/to/config.json")

    # Initialize the reader and tracker
    reader = ProcessedSetReader(dataset_path)
    tracker = PreprocessingTracker(storage_path)

    # Initialize the MIMICFeatureEngine for the LOS (length-of-stay) task
    # Tasks are IHM, DECOMP, LOS, PHENO
    feature_engine = MIMICFeatureEngine(
        config_dict=config_path,
        task="LOS",
        reader=reader,
        storage_path=storage_path,
        tracker=tracker,
        verbose=True
    )

    # Transform a subject
    subject_id = 12345
    X, y = feature_engine.transform_subject(subject_id)
    
    # Transform the entire dataset
    dataset = reader.read_samples(read_ids=True)
    X, y, t = feature_engine.transform_dataset(dataset)

    # Or transfrom the reader directly
    reader = feature_engine.transform_reader(reader)

    # Save the transformed data
    feature_engine.save_data()

"""

import numpy as np
import pandas as pd
import json
from typing import Dict, List, Tuple
from scipy.stats import skew
from numpy import random
from multiprocess import Manager
from datasets.readers import ProcessedSetReader
from datasets.writers import DataSetWriter
from utils import dict_subset
from utils.IO import *
from pathlib import Path
from datasets.trackers import PreprocessingTracker
from datasets.processors import AbstractProcessor


class MIMICFeatureEngine(AbstractProcessor):
    """
    This class processes time series data from the MIMIC-III dataset, applying various feature 
    engineering techniques and storing the processed data for downstream tasks.
    The feature engineering is sped up using multiprocessing.
    The features are engineered using min, max, mean, std, skew and len.

    Parameters
    ----------
    config_dict : Path
        The path to the configuration dictionary for feature engineering.
    task : str
        The task name, used to determine specific processing steps.
    reader : ProcessedSetReader, optional
        The reader object for reading batch data, by default None.
    storage_path : Path, optional
        The path where the processed data will be stored, by default None.
    tracker : PreprocessingTracker, optional
        The tracker object for keeping track of preprocessing steps, by default None.
    verbose : bool, optional
        If True, print verbose logs during processing, by default False.
    """

    def __init__(self,
                 config_dict: Path,
                 task: str,
                 reader: ProcessedSetReader = None,
                 storage_path: Path = None,
                 tracker: PreprocessingTracker = None,
                 verbose=False) -> None:

        self._storage_path = storage_path
        self._writer = (DataSetWriter(storage_path) if storage_path is not None else None)
        self._source_reader = reader
        if tracker is not None:
            self._tracker = tracker
        else:
            self._tracker = (PreprocessingTracker(Path(storage_path, "progress"))
                             if storage_path is not None else None)
        self._task = task
        self._save_as_samples = (True if task in ["IHM", "PHENO"]\
                                 else False)
        self._subsample_switch = {
            "first_percentage":
                lambda start_t, end_t, percentage: (start_t, start_t +
                                                    (end_t - start_t) * percentage / 100.0),
            "last_percentage":
                lambda start_t, end_t, percentage: (end_t -
                                                    (end_t - start_t) * percentage / 100.0, end_t)
        }

        self._lock = Manager().Lock()
        self._verbose = verbose

        with open(config_dict) as file:
            config_dict = json.load(file)
            self._sampler_combinations = config_dict["sampler_combinations"]
            self._impute_config = config_dict["channels"]
            self._channel_names = config_dict["channel_names"]

    @property
    def subjects(self) -> list:
        """
        Get the list of subject IDs available in the reader.

        Returns
        -------
        list
            A list of subject IDs.
        """
        return self._source_reader.subject_ids

    def transform_subject(self, subject_id: int):
        """
        Transform the data for a specific subject.

        This method reads the data for a specific subject, processes it, and returns
        the engineered features along with tracking information.

        Parameters
        ----------
        subject_id : int
            The ID of the subject to transform data for.

        Returns
        -------
        tuple
            A tuple containing the engineered features and tracking information.
        """
        X_processed, y_processed = self._source_reader.read_sample(subject_id,
                                                            read_ids=True,
                                                            data_type=pd.DataFrame).values()
        X = {subject_id: X_processed}
        y = {subject_id: y_processed}
        if X is None or y is None:
            return None, None

        X_engineered, y_engineered, _ = self.transform(X, y)
        if X_engineered is None or y_engineered is None:
            return None, None
        if self._tracker is None:
            return X_engineered, y_engineered

        with self._lock:
            tracking_info = self._tracker.subjects[subject_id]
        return (X_engineered, y_engineered), tracking_info

    def transform(self, X_dict: Dict[int, Dict[int, pd.DataFrame]], y_dict: Dict[int, Dict[int, pd.DataFrame]]):
        """
        Save the engineered data to the storage path.

        If no subject IDs are specified, all the engineered data will be saved.

        Parameters
        ----------
        subject_ids : list, optional
            A list of subject IDs to save data for. If None, all data is saved. Default is None.
        """
        n_subjects = 0
        n_stays = 0
        n_samples = 0

        if self._verbose:
            info_io(f"Engineering processed data:\n"
                    f"Engineered subjects: {0}\n"
                    f"Engineered stays: {0}\n"
                    f"Engineered samples: {0}")

        self._samples_processed = 0

        self._X_processed = dict()
        self._y_processed = dict()
        self._t_processed = dict()

        for subject_id in X_dict.keys():
            X_subject = X_dict[subject_id]
            y_subject = y_dict[subject_id]
            self._X_processed[subject_id] = dict()
            self._y_processed[subject_id] = dict()
            self._t_processed[subject_id] = dict()
            tracking_info = dict()

            for stay_id in X_subject:
                X_df = X_subject[stay_id]
                y_df = y_subject[stay_id]

                X_ss, ys, ts = self._engineer_stay(X_df, y_df)
                X_ss, ys, ts = self._convert_feature_dtype(X_ss, ys, ts)
                self._X_processed[subject_id][stay_id] = X_ss
                self._y_processed[subject_id][stay_id] = np.atleast_2d(ys)
                self._t_processed[subject_id][stay_id] = ts
                tracking_info[stay_id] = len(ys)
                n_samples += len(ys)
                n_stays += 1

                if self._verbose:
                    info_io(
                        f"Engineering processed data:\n"
                        f"Engineered subjects: {n_subjects}\n"
                        f"Engineered stays: {n_stays}\n"
                        f"Engineered samples: {n_samples}",
                        flush_block=True)

            n_subjects += 1
            if self._tracker is not None:
                with self._lock:
                    self._tracker.subjects.update({subject_id: tracking_info})

        if self._verbose:
            info_io(
                f"Engineering processed data:\n"
                f"Engineered subjects: {n_subjects}\n"
                f"Engineered stays: {n_stays}\n"
                f"Engineered samples: {n_samples}",
                flush_block=True)

        return self._X_processed, self._y_processed, self._t_processed

    def _engineer_stay(self, X_df: pd.DataFrame, y_df: pd.DataFrame):
        """
        Engineer features for a single ICU stay.

        This method applies feature engineering techniques to the data from a single ICU stay.

        Parameters
        ----------
        X_df : pd.DataFrame
            DataFrame containing the input data for the ICU stay.
        y_df : pd.DataFrame
            DataFrame containing the output data for the ICU stay.

        Returns
        -------
        tuple
            A tuple containing the engineered features, output data, and timestamps.
        """
        X_df = self._impute_categorical_data(X_df)
        Xs, ys, ts = self._read_timeseries_windows(X_df, y_df)

        (Xs, ys, ts) = self._shuffle([Xs, ys, ts])

        X_ss = list()
        ys = list(ys)
        ts = list(ts)

        for df in Xs:
            subsamples = [[
                self._channel_subsampler(df[column], *combination)
                for combination in self._sampler_combinations
            ]
                          for column in self._channel_names]
            # Iterating by channel name from config allows normalization
            # and ensures comparability to ground truth data from original dir

            engineered_features = [
                self._make_engineered_features(channel.values)
                for subsample in subsamples
                for channel in subsample
            ]

            X_ss.append(np.concatenate(engineered_features))

            self._samples_processed += 1

        return X_ss, ys, ts

    def _convert_feature_dtype(self, X, y, t):
        """Does nothing, need because of inheritance.
        """
        return X, y, t

    def save_data(self, subject_ids: list = None) -> None:
        """
        Saves the engineered feature data.

        This method saves the engineered feature data to the specified storage path. If no subject IDs are provided, all 
        engineered data will be saved. The data is saved in HDF5 format and optionally concatenated into CSV format for PHENO and IHM.
        """
        if subject_ids is None:
            name_data_pairs = {
                "X": self._X_processed,
                "y": self._y_processed,
                "t": self._t_processed
            }
        else:
            name_data_pairs = {
                "X": dict_subset(self._X_processed, subject_ids),
                "y": dict_subset(self._y_processed, subject_ids),
                "t": dict_subset(self._t_processed, subject_ids)
            }
        with self._lock:
            self._writer.write_bysubject(name_data_pairs, file_type="hdf5")

        def create_df(data, file_name) -> pd.DataFrame:
            if file_name == "X":
                dfs = pd.DataFrame([([subject_id, stay_id] +
                                     np.squeeze(frame).tolist()) if len(np.squeeze(frame)) > 1 else
                                    ([subject_id, stay_id, float(frame)])
                                    for subject_id, subject_stays in data.items()
                                    for stay_id, frame in subject_stays.items()])

            elif file_name == "y":
                dfs = pd.DataFrame([([subject_id, stay_id] +
                                     frame.tolist()) if isinstance(frame.tolist(), list) else
                                    ([subject_id, stay_id, float(frame)])
                                    for subject_id, subject_stays in data.items()
                                    for stay_id, frame in subject_stays.items()])
            dfs = dfs.rename(columns={0: "subject_id", 1: "stay_id"})
            if not len(dfs):
                return
            return dfs

        def append_data(X: dict, y: dict):

            def append(dfs: pd.DataFrame, file_name: str):
                file = Path(self._storage_path, f"{file_name}.csv")
                if file.is_file():
                    dfs.to_csv(file, mode='a', header=False, index=False)
                else:
                    dfs.to_csv(file, index=False)

            X_df = create_df(X, "X")
            y_df = create_df(y, "y")
            if y_df is None or not len(y_df) or not len(X_df):
                return
            append(X_df, "X")
            append(y_df, "y")

        if self._save_as_samples:
            with self._lock:
                append_data(self._X_processed, self._y_processed)

        return

    def _shuffle(self, data: List[tuple]) -> None:
        """
        Shuffle the data.
        """
        assert len(data) >= 2

        data = list(zip(*data))
        random.shuffle(data)
        data = list(zip(*data))

        return data

    def _impute_categorical_data(self, X):
        """
        Imputes specified columns to categorical data.

        This method replaces specified values in the input DataFrame with their categorical equivalents based on
        the impute configuration. It ensures that the specified columns are treated as categorical data types.
        """
        replace_dict = {'nan': np.nan}

        for channel in self._impute_config.keys():
            if 'values' in self._impute_config[channel].keys():
                replace_dict.update(self._impute_config[channel]['values'])

        with pd.option_context('future.no_silent_downcasting', True):
            X = X.replace(replace_dict).astype(float)
        return X

    def _make_engineered_features(self, data):
        """
        Generate engineered features.

        This method generates engineered features from the input data using statistical functions such as min, max,
        mean, standard deviation, skewness, and length.
        """
        functions = [min, max, np.mean, np.std, skew, len]
        import warnings
        warnings.filterwarnings("error")

        if len(data) == 0:
            engineered_data = np.full((len(functions,)), np.nan)
        else:
            # !TODO DEBUGGING
            engineered_data = [
                fn(data) if fn is not skew or
                (len(data) > 1 and not all(i == data[0]
                                           for i in data) or fn is len) else
                0  #TODO! This will fail and be NaN in Windows
                for fn in functions
            ]
            engineered_data = np.array(engineered_data, dtype=np.float32)

        return engineered_data

    def _channel_subsampler(self, Sr: pd.Series, sampler_function, percentage):
        """
        Subsample a time series channel.

        This method subsamples a time series channel based on specified time window percentage.
        For example this could mean first 25% or last 50% of the time series.
        It returns the subsampled series within the calculated time window.
        """
        Sr = Sr.dropna()

        if len(Sr) == 0:
            return pd.DataFrame()

        start_t = Sr.index[0]
        end_t = Sr.index[-1]

        sampled_start_t, sampled_end_t = self._subsample_switch[sampler_function](start_t, end_t,
                                                                                  percentage)

        return Sr[(Sr.index < sampled_end_t + 1e-6) & (Sr.index > sampled_start_t - 1e-6)]

    def _timeseries_subsampler(self, X: pd.DataFrame, sampler_function, percentage):
        """
        Subsample a time series DataFrame.

        This method subsamples the input DataFrame based on specified time window percentage.
        For example this could mean first 25% or last 50% of the time series.
        It returns a list of subsampled columns.
        """
        if len(X) == 0:
            data = np.full((6), np.nan)
        else:
            start_t = X.index[0]
            end_t = X.index[-1]

            sampled_start_t, sampled_end_t = self._subsample_switch[sampler_function](start_t,
                                                                                      end_t,
                                                                                      percentage)

            data = X[(X.index < sampled_end_t + 1e-6) & (X.index > sampled_start_t - 1e-6)]

            if len(data) == 0:
                data = pd.DataFrame(np.full((len(X,)), np.nan))

        return [data[channel] for channel in data]

    def _read_timeseries_windows(self, X_df: pd.DataFrame, y_df: pd.DataFrame) -> 'tuple[list]':
        """
        Read time series windows.

        This method reads the time series data to create windows for feature engineering, reaching from the
        start time-stamp to the current timestamp. The window is paired with the current label. 
        """
        Xs = list()
        ys = list()
        ts = list()

        for i in range(len(y_df)):
            index = i

            if index < 0 or index >= len(y_df):
                raise ValueError(
                    "Index must be from 0 (inclusive) to number of examples (exclusive).")

            t = y_df.reset_index().iloc[index, 0]
            y = y_df.reset_index().iloc[index, 1:]
            X = X_df[X_df.index < t + 1e-6]

            Xs.append(X)
            ys.append(y)
            ts.append(t)

        return Xs, ys, ts

    def _convert_feature_dtype(self, X, y, t):
        """
        Convert feature data types.

        This method converts the data types of the input features, target values, and timestamps to numpy arrays.
        """
        X = np.stack(X)
        return np.array(X), np.array(y), np.array(t)


### FILE: .\src\datasets\processors\preprocessors.py ###
"""
This module provides the MIMICPreprocessor class to preprocess the MIMIC-III dataset for various tasks such as in-hospital mortality, decompensation, length of stay, and phenotyping.

Usage Example
--------------
.. code-block:: python

    from pathlib import Path
    import yaml
    from datasets.readers import ExtractedSetReader
    from datasets.trackers import PreprocessingTracker
    from datasets.preprocessor import MIMICPreprocessor

    # Define the path to the dataset, storage, and phenotype configuration
    dataset_path = Path("/path/to/extracted/dataset")
    storage_path = Path("/path/to/store/processed/data")
    phenotypes_path = Path("/path/to/phenotypes.yaml")

    # Initialize the reader and tracker
    reader = ExtractedSetReader(dataset_path)
    tracker = PreprocessingTracker(storage_path)

    # Load the phenotypes configuration from a .yaml file
    with open(phenotypes_path, 'r') as file:
        phenotypes_yaml = yaml.safe_load(file)

    # Initialize the MIMICPreprocessor for the LOS (length-of-stay) task
    # Tasks are IHM, DECOMP, LOS, PHENO
    preprocessor = MIMICPreprocessor(
        phenotypes_yaml=phenotypes_yaml,
        task="LOS",
        label_type="sparse",
        reader=reader,
        tracker=tracker,
        storage_path=storage_path,
        verbose=True
    )

    # Transform the entire dataset
    reader = ExtractedSetReader(dataset_path)
    dataset = reader.read_subjects(read_ids=True)

    subject_id = 12345
    (X, y), tracking_info = preprocessor.transform_subject(subject_id)

    X, y = preprocessor.transform_dataset(dataset)

    # Alternatively transform the reader directly
    reader = preprocessor.transform_reader(reader)

    # Save the transformed data
    preprocessor.save_data()
"""
import numpy as np
import dateutil
import pandas as pd
from pathlib import Path
from multiprocess import Manager
from datasets.writers import DataSetWriter
from datasets.readers import ExtractedSetReader, ProcessedSetReader
from datasets.trackers import PreprocessingTracker
from datasets.processors import AbstractProcessor
from utils import dict_subset
from utils.IO import *
from settings import *


class MIMICPreprocessor(AbstractProcessor):
    """
    Preprocesses MIMIC-III dataset for various tasks such as in-hospital mortality,
    decompensation, length of stay, and phenotyping.

    Attributes
    ----------
    _label_type : str
        The type of labels to use ("sparse" or "one-hot").
    _storage_path : Path
        The path to store processed data.
    _task : str
        The task to perform ("IHM", "DECOMP", "LOS", "PHENO").
    _writer : DataSetWriter
        The writer to save processed data.
    _source_reader : ExtractedSetReader
        The reader to read source data.
    _tracker : PreprocessingTracker
        The tracker to keep track of preprocessing progress.
    _processed_set_reader : ProcessedSetReader
        The reader to read processed data.
    _lock : Manager().Lock
        A lock for multiprocessing.
    _phenotypes_yaml : dict
        The phenotypes YAML configuration.
    _verbose : bool
        Flag for verbosity.

    Parameters
    ----------
    phenotypes_yaml : dict
        YAML configuration for phenotypes.
    task : str
        The task to perform ("IHM", "DECOMP", "LOS", "PHENO").
    label_type : str, optional
        The type of labels to use ("sparse" or "one-hot"), by default "sparse".
    reader : ExtractedSetReader, optional
        The reader to read source data, by default None.
    tracker : PreprocessingTracker, optional
        The tracker to keep track of preprocessing progress, by default None.
    storage_path : Path, optional
        The path to store processed data, by default None.
    verbose : bool, optional
        Flag for verbosity, by default False.
    """

    def __init__(self,
                 phenotypes_yaml: dict,
                 task: str,
                 label_type: str = "sparse",
                 reader: ExtractedSetReader = None,
                 tracker: PreprocessingTracker = None,
                 storage_path: Path = None,
                 verbose: bool = False):

        if label_type not in ["sparse", "one-hot"]:
            raise ValueError(f"Type must be one of {*['sparse', 'one-hot'],}")

        self._label_type = label_type
        self._storage_path = storage_path

        if task not in TASK_NAMES:
            raise ValueError(f"Task must be one of {*TASK_NAMES,}")

        self._task = task
        self._writer = (None if storage_path is None else DataSetWriter(self._storage_path))
        self._source_reader = reader  # the set we are trying to read from
        if tracker is not None:
            self._tracker = tracker
        else:
            self._tracker = (None if storage_path is None else PreprocessingTracker(
                Path(storage_path, "progress")))
        self._processed_set_reader = (None if storage_path is None else ProcessedSetReader(
            root_path=storage_path))
        self._lock = Manager().Lock()
        self._phenotypes_yaml = phenotypes_yaml
        self._verbose = verbose

    @property
    def subjects(self) -> list:
        """
        Get the list of subject IDs that can be processed from the reader.

        Returns
        -------
        list
            A list of subject IDs.
        """
        if self._source_reader is None:
            return []
        return self._source_reader.subject_ids

    def transform_subject(self, subject_id: int) -> None:
        """
        Transforms the extracted dataset for the specified task.

        Parameters
        ----------
        dataset : dict
            The dataset to transform.

        Returns
        -------
        tuple
            A tuple containing transformed feature and label data.
        """
        subject_data = self._source_reader.read_subject(subject_id, read_ids=True)
        if not subject_data:
            return None, None
        del subject_data["subject_events"]
        X, y = self.transform({subject_id: subject_data})
        if not X or not y:
            return None, None
        if self._tracker is None:
            return X, y
        with self._lock:
            tracking_info = self._tracker.subjects[subject_id]
        return (X, y), tracking_info

    def transform(self, dataset: dict):
        """
        Transforms the extracted dataset.

        This processes the provided extracted dataset according to the specified task, and returns the 
        processed tasks and labels along with tracking information if available.
        """
        self._X = dict()
        self._y = dict()

        # Tracking variables
        n_subjects = 0
        n_stays = 0
        n_skip = 0
        n_samples = 0

        start_verbose = True

        if self._task in ["LOS"] and self._label_type == "one-hot":
            info_io(f"Only sparse output_type is available for task {self._task}!"
                    f" Argument {self._label_type} disregarded")
            self._label_type = "sparse"

        for subject, subject_data in dataset.items():
            skip_subject = False

            subject_timeseries: pd.DataFrame = subject_data['timeseries']
            diagnoses_df: pd.DataFrame = subject_data['subject_diagnoses']
            icuhistory_df: pd.DataFrame = subject_data['subject_icu_history']
            episodic_data_df: pd.DataFrame = subject_data['episodic_data']

            self._X[subject] = dict()
            self._y[subject] = dict()

            tracking_info = dict()

            if (self._tracker is not None) and \
               (subject in self._tracker.subjects) and \
               (not self._X[subject]):
                # Do not reprocess already existing directories
                self._X[subject], self._y[subject] = self._processed_set_reader.read_sample(
                    str(subject), read_ids=True).values()
                skip_subject = True
                continue
            elif start_verbose:
                if self._verbose:
                    info_io(f"Processing timeseries data:\n"
                            f"Processed subjects: {0}\n"
                            f"Processed stays: {0}\n"
                            f"Processed samples: {0}\n"
                            f"Skipped subjects: {0}")
                    start_verbose = False

            for icustay in subject_timeseries:
                stay_timeseries_df = subject_timeseries[icustay]
                cur_episodic_data_df = episodic_data_df.loc[icustay]
                cur_icuhistory_sr = icuhistory_df.reset_index().set_index("ICUSTAY_ID").loc[icustay]

                if not pd.isna(cur_episodic_data_df["MORTALITY"]):
                    mortality = int(cur_episodic_data_df["MORTALITY"])
                else:
                    continue

                if self._task == "IHM":
                    self._X[subject][icustay], self._y[subject][
                        icustay] = self.make_inhospital_mortality_data(
                            stay_timeseries_df, cur_episodic_data_df, mortality)

                elif self._task == "DECOMP":
                    self._X[subject][icustay], self._y[subject][
                        icustay] = self.make_decompensation_data(stay_timeseries_df,
                                                                 cur_episodic_data_df,
                                                                 cur_icuhistory_sr)
                elif self._task == "LOS":
                    self._X[subject][icustay], self._y[subject][
                        icustay] = self.make_length_of_stay_data(stay_timeseries_df,
                                                                 cur_episodic_data_df)
                elif self._task == "PHENO":
                    stay_diagnoses_df = diagnoses_df[diagnoses_df['ICUSTAY_ID'] == icustay]
                    self._X[subject][icustay], self._y[subject][
                        icustay] = self.make_pheontyping_data(stay_timeseries_df,
                                                              cur_episodic_data_df,
                                                              stay_diagnoses_df,
                                                              self._phenotypes_yaml)
                else:
                    raise ValueError(
                        "Task must be one of: in_hospital_mortality, decompensation, length_of_stay, phenotyping"
                    )
                if self._y[subject][icustay].empty:
                    del self._y[subject][icustay]
                    del self._X[subject][icustay]
                    continue
                else:
                    tracking_info[icustay] = len(self._y[subject][icustay])
                    n_stays += 1
                    n_samples += len(self._y[subject][icustay])
                    if self._verbose:
                        info_io(
                            f"Processing timeseries data:\n"
                            f"Processed subjects: {n_subjects}\n"
                            f"Processed stays: {n_stays}\n"
                            f"Processed samples: {n_samples}\n"
                            f"Skipped subjects: {n_skip}",
                            flush_block=True)

            if skip_subject:
                continue

            if self._tracker is not None and tracking_info:
                with self._lock:
                    self._tracker.subjects.update({subject: tracking_info})

            if not len(self._y[subject]) or not len(self._X[subject]):
                del self._y[subject]
                del self._X[subject]
                n_skip += 1
            else:
                n_subjects += 1
        if self._verbose:
            info_io(
                f"Processing timeseries data:\n"
                f"Processed subjects: {n_subjects}\n"
                f"Processed stays: {n_stays}\n"
                f"Processed samples: {n_samples}\n"
                f"Skipped subjects: {n_skip}",
                flush_block=True)

        return self._X, self._y

    def save_data(self, subjects: list = None) -> None:
        """
        Saves the preprocessed data to the specified storage path.

        If no subject IDs are provided, all preprocessed data will be saved. The data is saved using the DataSetWriter.
        """
        if self._writer is None:
            info_io("No storage path provided. Data will not be saved.")
            return
        with self._lock:
            if subjects is None:
                self._writer.write_bysubject({"X": self._X})
                self._writer.write_bysubject({"y": self._y})
            else:
                self._writer.write_bysubject({"X": dict_subset(self._X, subjects)})
                self._writer.write_bysubject({"y": dict_subset(self._y, subjects)})

        return

    def make_inhospital_mortality_data(self, timeseries_df: pd.DataFrame,
                                       episodic_data_df: pd.DataFrame, mortality: int):
        """
        Prepares data for the in-hospital mortality prediction task.

        This method extracts the timeseries up to the label_start_time specified in the task settings and sets the 
        label to 1 if the patient died in the ICU afterwards.

        - Filters the timeseries data up to the label_start_time.
        - Sets the label to 1 if the patient died in the ICU after the label_start_time.
        - Returns the filtered timeseries data and corresponding labels.
        """
        precision = IHM_SETTINGS['sample_precision']
        label_start_time = IHM_SETTINGS['label_start_time']
        los = 24.0 * episodic_data_df.loc['LOS']  # in hours
        if los < label_start_time:
            return pd.DataFrame(columns=timeseries_df.columns), \
                   pd.DataFrame(columns=['Timestamp', 'y']).set_index('Timestamp')
        X: pd.DataFrame = timeseries_df[(timeseries_df.index < label_start_time + precision)
                                        & (timeseries_df.index > -precision)]
        if X.empty:
            return X, pd.DataFrame(columns=['Timestamp', 'y'])

        y = np.array([(X.index[-1], mortality)])
        y = pd.DataFrame(y, columns=['Timestamp', 'y']).set_index('Timestamp')

        return X, y

    def make_decompensation_data(self, timeseries_df: pd.DataFrame, episodic_data_df: pd.DataFrame,
                                 icu_stay):
        """
        Prepares data for the decompensation prediction task.

        This method extracts relevant features from timeseries data starting at ICU admission time, with the first 
        label generated at the label_start_time. It checks at every sample_rate interval if the patient died within 
        the future_time_interval (sets label to 1 if true).

        - Filters the timeseries data up to the LOS duration.
        - Generates sample times at specified intervals starting from the label_start_time.
        - Checks if the patient died within the future_time_interval at each sample time.
        - Returns the filtered timeseries data and corresponding labels.
        """
        precision = DECOMP_SETTINGS['sample_precision']
        sample_rate = DECOMP_SETTINGS['sample_rate']
        label_start_time = DECOMP_SETTINGS['label_start_time']
        future_time_interval = DECOMP_SETTINGS['future_time_interval']
        # future_time_interval is Hours in which the preson will have to be dead for label to be 1

        mortality = int(episodic_data_df.loc["MORTALITY"])

        los = 24.0 * episodic_data_df.loc['LOS']  # in hours

        deathtime = icu_stay['DEATHTIME']
        intime = icu_stay['INTIME']

        if pd.isnull(deathtime):
            lived_time = 1e18
        else:
            if isinstance(icu_stay['DEATHTIME'], str):
                deathtime = dateutil.parser.parse(deathtime)
            if isinstance(icu_stay['INTIME'], str):
                intime = dateutil.parser.parse(intime)
            lived_time = (deathtime - intime).total_seconds() / 3600.0

        X: pd.DataFrame = timeseries_df[(timeseries_df.index < los + precision)
                                        & (timeseries_df.index > -precision)]
        if X.empty:
            return X, pd.DataFrame(columns=['Timestamp', 'y']).set_index('Timestamp')

        event_times = timeseries_df.index[(timeseries_df.index < los + precision)
                                          & (timeseries_df.index > -precision)]

        sample_times = np.arange(0.0, min(los, lived_time) + precision, sample_rate)
        sample_times = list(filter(lambda x: x > label_start_time, sample_times))

        # At least one measurement
        sample_times = list(filter(lambda x: x > event_times[0], sample_times))

        y = list()

        for hour in sample_times:
            if mortality == 0:
                cur_mortality = 0
            else:
                cur_mortality = int(lived_time - hour < future_time_interval)
            y.append((hour, cur_mortality))

        if not y:
            return pd.DataFrame(columns=X.columns), pd.DataFrame(columns=['Timestamp', 'y'])

        y = pd.DataFrame(y, columns=['Timestamp', 'y']).set_index('Timestamp')

        return X, y

    def make_length_of_stay_data(self, timeseries_df: pd.DataFrame, episodic_data_df: pd.DataFrame):
        """
        Prepares data for the length of stay prediction task.

        This method extracts relevant features from the timeseries data starting at ICU admission time and generates 
        labels for the remaining length of stay (LOS). The first label is generated at the label_start_time, and 
        subsequent labels are generated at intervals specified by the sample_rate. The LOS is divided into categories 
        using specified bins, and labels are created either in "one-hot" format (categorical) or "sparse" format 
        (continuous) based on the remaining LOS.

        - Filters the timeseries data within the LOS duration.
        - Generates sample times at specified intervals.
        - Creates labels in either "one-hot" or "sparse" format based on the remaining LOS.
        - Returns the filtered timeseries data and corresponding labels.
        """
        precision = LOS_SETTINGS['sample_precision']
        sample_rate = LOS_SETTINGS['sample_rate']
        label_start_time = LOS_SETTINGS['label_start_time']
        bins = LOS_SETTINGS['bins']

        los = 24.0 * episodic_data_df.loc['LOS']  # in hours

        X: pd.DataFrame = timeseries_df[(timeseries_df.index < los + precision)
                                        & (timeseries_df.index > -precision)]
        if X.empty:
            return X, pd.DataFrame(columns=['Timestamp', 'y']).set_index('Timestamp')

        event_times = timeseries_df.index[(timeseries_df.index < los + precision)
                                          & (timeseries_df.index > -precision)]

        sample_times = np.arange(0.0, los + precision, sample_rate)
        sample_times = list(filter(lambda x: x > label_start_time, sample_times))
        sample_times = list(filter(lambda x: x > event_times[0], sample_times))

        y = list()

        for hour in sample_times:
            if self._label_type == "one-hot":
                category_index = np.digitize(los - hour, bins) - 1
                remaining_los = [0] * 10
                remaining_los[category_index] = 1
                y.append((hour, remaining_los))

            elif self._label_type == "sparse":
                y.append((hour, los - hour))

        y = pd.DataFrame(y, columns=['Timestamp', 'y']).set_index('Timestamp')
        if y.empty:
            # Will prevent the sample from being used
            return pd.DataFrame(columns=X.columns), y

        return X, y

    def make_pheontyping_data(self, timeseries_df: pd.DataFrame, episodic_data_df: pd.DataFrame,
                              diagnoses_df: pd.DataFrame, phenotypes_yaml: pd.DataFrame):
        """
        Prepares data for the phenotyping task.

        This method extracts relevant features from the timeseries data starting at ICU admission time and creates 
        labels for phenotypes based on the provided YAML configuration. It maps diagnosis codes to phenotype groups, 
        filters the labels to include only those used in the benchmark, and returns the filtered timeseries data and 
        corresponding phenotype labels.

        - Filters the timeseries data within the length of stay (LOS) duration.
        - Maps diagnosis codes to phenotype groups using the YAML configuration.
        - Creates labels for phenotypes, including only those used in the benchmark.
        - Returns the filtered timeseries data and phenotype labels.
        """
        precision = PHENOT_SETTINGS['sample_precision']
        valid_ids = PHENOT_SETTINGS['valid_ids']

        los = 24.0 * episodic_data_df.loc['LOS']  # in hours
        X: pd.DataFrame = timeseries_df[(timeseries_df.index < los + precision)
                                        & (timeseries_df.index > -precision)]
        if X.empty:
            return X, pd.DataFrame(columns=['Timestamp', 'y']).set_index('Timestamp')

        # Dictionary mapping codes to groups
        code_to_group = {}
        for group in phenotypes_yaml:
            codes = phenotypes_yaml[group]['codes']
            for code in codes:
                if code not in code_to_group:
                    code_to_group[code] = group
                else:
                    assert code_to_group[code] == group

        # index is ID of phenotype in the yaml
        id_to_group = sorted(phenotypes_yaml.keys())
        group_to_id = dict((x, i) for (i, x) in enumerate(id_to_group))

        # Index is pheontype designation and item is id
        cur_labels = [0] * len(id_to_group)

        for index, row in diagnoses_df.iterrows():
            if row['USE_IN_BENCHMARK']:
                code = row['ICD9_CODE']
                group = code_to_group[code]
                group_id = group_to_id[group]
                cur_labels[group_id] = 1

        # Only use in benchmark = True labels
        if not valid_ids:
            valid_ids = [
                i for (i, x) in enumerate(cur_labels)
                if phenotypes_yaml[id_to_group[i]]['use_in_benchmark']
            ]
            PHENOT_SETTINGS['valid_ids'] = valid_ids

        cur_labels = [cur_labels[index] for index in valid_ids]

        y = [los] + cur_labels
        y = pd.DataFrame((y,), columns=["Timestamp"] + [f"y{i}" for i in range(len(y) - 1)])
        y = y.set_index('Timestamp')

        return X, y


### FILE: .\src\datasets\processors\__init__.py ###
import random
import os
import pandas as pd
from copy import deepcopy
from itertools import chain
from typing import Dict, List, Tuple
from utils import dict_subset
from pathlib import Path
from abc import ABC, abstractmethod
from pathos.multiprocessing import cpu_count, Pool
from datasets.readers import ExtractedSetReader, ProcessedSetReader
from datasets.mimic_utils import copy_subject_info
from utils.IO import *

class AbstractProcessor(ABC):
    """_summary_
    """

    @abstractmethod
    def __init__(self) -> None:
        """_summary_

        Raises:
            NotImplementedError: _description_
        """
        ...

    @property
    @abstractmethod
    def subjects(self) -> List[int]:
        ...

    @abstractmethod
    def transform(self, *args, **kwargs):
        """_summary_

        Raises:
            NotImplementedError: _description_
        """
        ...

    @abstractmethod
    def transform_subject(self, subject_id: int) -> Tuple[dict, dict, dict]:
        ...

    @abstractmethod
    def save_data(self, subject_ids: list = None) -> None:
        ...

    def transform_dataset(self,
                           dataset: dict,
                           subject_ids: list = None,
                           num_subjects: int = None,
                           source_path: Path = None) -> Dict[str, Dict[str, pd.DataFrame]]:
        """
        _summary_

        Parameters
        ----------
        dataset : dict
            The dataset to process.
        subject_ids : list, optional
            List of subject IDs. Defaults to None.
        num_subjects : int, optional
            Number of subjects to process. Defaults to None.
        source_path : Path, optional
            Source path of the data. Defaults to None.

        Returns
        -------
        Dict[str, Dict[str, pd.DataFrame]]
            Processed data.
        """
        copy_subject_info(source_path, self._storage_path)

        if self._tracker.is_finished:
            info_io(f"Compact data processing already finalized in directory:\n{str(self._storage_path)}")
            if num_subjects is not None:
                subject_ids = random.sample(self._tracker.subject_ids, k=num_subjects)
            return ProcessedSetReader(root_path=self._storage_path,
                                      subject_ids=subject_ids).read_samples(read_ids=True)

        info_io(f"Compact Preprocessing: {self._task}", level=0)

        subject_ids, excluded_subject_ids = self._get_subject_ids(num_subjects=num_subjects,
                                                                subject_ids=subject_ids,
                                                                all_subjects=dataset.keys())
        assert all([len(subject) for subject in dataset.values()])
        missing_subjects = 0
        if num_subjects is not None:
            X_subjects = dict()
            y_subjects = dict()
            while not len(X_subjects) == num_subjects:
                curr_dataset = dict_subset(dataset, subject_ids)
                X, y = self.transform(dataset=curr_dataset)
                X_subjects.update(X)
                y_subjects.update(y)
                it_missing_subjects = set(X.keys()) - set(subject_ids)
                subject_ids, excluded_subject_ids = self.get_subject_ids(num_subjects=num_subjects -
                                                                        len(X_subjects),
                                                                        subject_ids=None,
                                                                        all_subjects=excluded_subject_ids)
                if it_missing_subjects:
                    missing_subjects += len(it_missing_subjects)
                    debug_io(f"Missing subjects are: {*it_missing_subjects,}")
                if not subject_ids:
                    break
                if len(X_subjects) == num_subjects:
                    debug_io(f"Missing {len(X_subjects) - num_subjects} subjects.")
                    debug_io(f"Unprocessable subjects are: {*it_missing_subjects,}")

        else:
            assert all([len(subject) for subject in dataset.values()])
            dataset = dict_subset(dataset, subject_ids)
            assert all([len(subject) for subject in dataset.values()])
            (X_subjects, y_subjects) = self.transform(dataset=dataset)
        if self._storage_path is not None:
            self.save_data()
            info_io(f"Finalized data preprocessing for {self._task} in directory:\n{str(self._storage_path)}")
        else:
            info_io(f"Finalized data preprocessing for {self._task}.")
        self._tracker.is_finished = True
        return {"X": X_subjects, "y": y_subjects}

    def transform_reader(self,
                             reader: ExtractedSetReader,
                             subject_ids: list = None,
                             num_subjects: int = None) -> ProcessedSetReader:
        """
        _summary_

        Parameters
        ----------
        reader : ExtractedSetReader
            Reader for the extracted set.
        subject_ids : list, optional
            List of subject IDs. Defaults to None.
        num_subjects : int, optional
            Number of subjects to process. Defaults to None.

        Returns
        -------
        ProcessedSetReader
            Reader for the processed set.
        """
        orig_verbose = self._verbose
        self._verbose = False
        self._source_reader = reader
        original_subject_ids = deepcopy(subject_ids)

        copy_subject_info(reader.root_path, self._storage_path)

        if self._tracker.is_finished:
            info_io(f"Data preprocessing for {self._task} is already in directory:\n{str(self._storage_path)}.")
            if num_subjects is not None:
                subject_ids = random.sample(self._tracker.subject_ids, k=num_subjects)
            self._verbose = orig_verbose
            return ProcessedSetReader(self._storage_path, subject_ids=subject_ids)

        info_io(f"Iterative Preprocessing: {self._task}", level=0)
        info_io(f"Preprocessing data for task {self._task}.")

        # Tracking info
        n_processed_subjects = len(self._tracker.subject_ids)
        n_processed_stays = len(self._tracker.stay_ids)
        n_processed_samples = self._tracker.samples

        # Parallel processing logic
        def process_subject(subject_id: str):
            """_summary_"""
            _, tracking_infos = self.transform_subject(subject_id)

            if tracking_infos:
                self.save_data([subject_id])
                return subject_id, tracking_infos

            return subject_id, None

        def init(preprocessor):
            global processor_pr
            processor_pr = preprocessor

        subject_ids, excluded_subject_ids = self._get_subject_ids(num_subjects=num_subjects,
                                                                subject_ids=subject_ids,
                                                                all_subjects=reader.subject_ids,
                                                                processed_subjects=self._tracker.subject_ids)

        for ids in subject_ids:
            process_subject(ids)

        info_io(f"Processing timeseries data:\n"
                f"Processed subjects: {n_processed_subjects}\n"
                f"Processed stays: {n_processed_stays}\n"
                f"Processed samples: {n_processed_samples}\n"
                f"Skipped subjects: {0}")

        # Start the run
        with Pool(cpu_count() - 1, initializer=init, initargs=(self,)) as pool:
            res = pool.imap_unordered(process_subject, subject_ids, chunksize=500)

            empty_subjects = 0
            missing_subjects = 0
            while True:
                try:
                    subject_id, tracker_data = next(res)
                    if tracker_data is None:
                        empty_subjects += 1
                        # Add new samples if to meet the num subjects target
                        if num_subjects is None:
                            continue
                        debug_io(f"Missing subject is: {subject_id}")
                        try:
                            subj = excluded_subject_ids.pop()
                            res = chain(res, [pool.apply_async(process_subject, args=(subj,)).get()])
                        except IndexError:
                            missing_subjects += 1
                            debug_io(
                                f"Could not replace missing subject. Excluded subjects is: {excluded_subject_ids}"
                            )
                    else:
                        n_processed_subjects += 1
                        n_processed_stays += len(tracker_data) - 1
                        n_processed_samples += tracker_data["total"]

                    info_io(
                        f"Processing timeseries data:\n"
                        f"Processed subjects: {n_processed_subjects}\n"
                        f"Processed stays: {n_processed_stays}\n"
                        f"Processed samples: {n_processed_samples}\n"
                        f"Skipped subjects: {empty_subjects}",
                        flush_block=(True and not int(os.getenv("DEBUG", 0))))
                except StopIteration as e:
                    self._tracker.is_finished = True
                    info_io(f"Finalized for task {self._task} in directory:\n{str(self._storage_path)}")
                    if num_subjects is not None and missing_subjects:
                        warn_io(
                            f"The subject target was not reached, missing {missing_subjects} subjects.")
                    break
        self._verbose = orig_verbose
        if original_subject_ids is not None:
            original_subject_ids = list(set(original_subject_ids) & set(self._tracker.subject_ids))
        return ProcessedSetReader(self._storage_path, subject_ids=original_subject_ids)



    def _get_subject_ids(self, 
                         num_subjects: int,
                        subject_ids: list,
                        all_subjects: list,
                        processed_subjects: list = list()):
        remaining_subject_ids = list(set(all_subjects) - set(processed_subjects))
        # Select subjects to process logic
        n_processed_subjects = len(processed_subjects)
        if num_subjects is not None:
            num_subjects = max(num_subjects - n_processed_subjects, 0)
            selected_subjects_ids = random.sample(remaining_subject_ids, k=num_subjects)
            remaining_subject_ids = list(set(remaining_subject_ids) - set(selected_subjects_ids))
            random.shuffle(remaining_subject_ids)
        elif subject_ids is not None:
            unknown_subjects = set(subject_ids) - set(all_subjects)
            if unknown_subjects:
                warn_io(f"Unknown subjects: {*unknown_subjects,}")
            selected_subjects_ids = list(set(subject_ids) & set(all_subjects))
            remaining_subject_ids = list(set(remaining_subject_ids) - set(selected_subjects_ids))
        else:
            selected_subjects_ids = remaining_subject_ids
        return selected_subjects_ids, remaining_subject_ids


### FILE: .\src\datasets\split\splitters.py ###
"""
Data Splitting Module
=====================

This module provides classes for splitting datasets into training, validation, and test sets.
The splits can be performed based on predefined ratios or by demographic groups.
This feature is useful for creating willfully biased datasets or inducing concept drift.

Classes
-------
- AbstractSplitter(max_iter, tolerance)
    Provides abstract methods for splitting datasets.
- ReaderSplitter(max_iter, tolerance)
    Splits datasets loaded by a reader based on specified criteria.
- CompactSplitter(max_iter, tolerance)
    Splits dictionaries containing data based on specified criteria.
"""
import random
import numpy as np
import pandas as pd
from typing import List, Dict
from pathlib import Path
from utils.IO import *
from datasets.readers import ProcessedSetReader, SplitSetReader
from sklearn import model_selection
from pathlib import Path
from datasets.trackers import DataSplitTracker, PreprocessingTracker
from pathos.multiprocessing import Pool, cpu_count
from utils import dict_subset
from collections import OrderedDict
from itertools import chain
from abc import ABC, abstractmethod


class AbstractSplitter(ABC):
    """
    Abstract base class for data splitting. Implements methods used for reader and dictionary splitting.
    
    Parameters
    ----------
    max_iter : int, optional
        Maximum number of iterations for ratio-based splitting. Default is 100.
    tolerance : float, optional
        Tolerance level for ratio deviations. Default is 0.005.
    """

    def __init__(self, max_iter: int = 100, tolerance: float = 0.005) -> None:
        self._max_iter = max_iter
        self._tolerance = tolerance

    @staticmethod
    def _print_ratio(prefix: str, ratio: dict):
        """
        Prints the ratio of the splits.

        Parameters
        ----------
        prefix : str
            Prefix for the printed message.
        ratio : dict
            Dictionary containing the ratios for each split.

        Examples
        --------
        >>> ratio = {"train": 0.7, "val": 0.15, "test": 0.15}
        >>> AbstractSplitter._print_ratio("Target", ratio)
        Target:
         train size:  0.700
         val size:    0.150
         test size:   0.150
        """
        message = [prefix + ":"]
        set_names = ["test", "val", "train"]
        for set_name in set_names:
            if set_name in ratio and ratio[set_name]:
                set_string = f" {set_name} size: ".ljust(12)
                message.append(f"{set_string}{ratio[set_name]:0.3f}")
        info_io("\n".join(message))

    def _reduce_by_ratio(self,
                         subjects: Dict[str, List[int]],
                         sample_counts: dict,
                         test_size: float = 0.0,
                         val_size: float = 0.0) -> tuple:
        """
        Reduces the number of subjects based in the datasplit to enforce specified ratios.
        
        Parameters
        ----------
        subjects : dict
            Dictionary containing lists of subjects for each split.
        sample_counts : dict
            Dictionary containing sample counts for each subject.
        test_size : float, optional
            Ratio of the test set. Default is 0.0.
        val_size : float, optional
            Ratio of the validation set. Default is 0.0.
        
        Returns
        -------
        tuple
            Updated subjects and their real ratios.
        """
        # If val_size is specified but no val subjects are present raise an error
        if val_size and (not "val" in subjects or not subjects["val"]):
            raise ValueError(f"'val_size' parameter specified but no val subjects "
                             f"in reduce by ratio function! Val size is: {val_size}")
        # Same for test
        if test_size and (not "test" in subjects or not subjects["test"]):
            raise ValueError(f"'val_size' parameter specified but no val subjects "
                             f"in reduce by ratio function! Val size is: {val_size}")

        # Remember target ratios
        target_ratios = dict()
        if test_size and "test" in subjects:
            target_ratios["test"] = test_size
        if val_size and "val" in subjects:
            target_ratios["val"] = val_size
        if test_size and "train" in subjects:
            target_ratios["train"] = 1 - test_size - val_size

        ratio_df = self._create_ratio_df(list(chain.from_iterable(subjects.values())),
                                         sample_counts)

        processed_split_names = list()
        input_ratios = {
            set_name: ratio_df[ratio_df["participant"].isin(split_subject)]["ratio"].sum()
            for set_name, split_subject in subjects.items()
        }

        # Deviation from target ratios
        target_to_input_ratios = OrderedDict({
            set_name: input_ratios[set_name] / target_ratios[set_name]
            for set_name in target_ratios
            if not set_name in processed_split_names
        })

        # Reduction based on the smallest input/target ratio (smalles set)
        target_to_input_ratios = OrderedDict(
            sorted(target_to_input_ratios.items(), key=lambda item: item[1]))
        base = target_to_input_ratios.popitem(last=False)[0]
        base_len = ratio_df[ratio_df["participant"].isin(subjects[base])]["ratio"].sum()
        real_ratios = dict()

        # Length once reduced
        new_length = base_len * (
            1 + sum([target_ratios[name] for name in target_to_input_ratios]) / target_ratios[base])
        ratio_df["ratio"] *= 1 / new_length

        for set_name in target_to_input_ratios:
            if not (1 - target_ratios[set_name]):
                continue

            subjects[set_name], _ = self._subjects_for_ratio(
                ratio_df[ratio_df["participant"].isin(subjects[set_name])], target_ratios[set_name])

            processed_split_names.append(set_name)

        real_ratios = {
            set_name: ratio_df[ratio_df["participant"].isin(split_subject)]["ratio"].sum()
            for set_name, split_subject in subjects.items()
        }
        return subjects, real_ratios

    def _check_settings(self, settings, parent_key='', set_name=None):
        """
        Recursively checks if any setting in a nested dictionary is empty.
        Raises ValueError if an empty setting is found.
        """
        for key, value in settings.items():
            # Construct the full key path for better error messages
            full_key = f"{parent_key}.{key}" if parent_key else key

            # Check if the value is a dictionary and if so, recurse into it
            if isinstance(value, dict) and value:
                self._check_settings(value, full_key)
            else:
                # Check if the value is considered empty (covers empty lists, None, empty strings, etc.)
                if not value:  # This checks for empty lists, None, empty strings, and other "falsy" values
                    error_msg = ""
                    if set_name:
                        error_msg += f"For set '{set_name}': "
                    error_msg += f"{full_key} setting is empty"
                    raise ValueError(error_msg)

    def _split_by_demographics(self, subject_ids: List[int], source_path: Path,
                               demographic_split: dict):
        """
        Splits subjects based on demographic settings.
        """
        return_ratios = dict()
        return_subjects = dict()

        # Set wise demographics config for test, val and train
        for set_name, setting in demographic_split.items():
            assert set_name in ["test", "val", "train"], "Invalid split attribute"
            self._check_settings(setting, set_name=set_name)
            curr_subject_ids = self._get_demographics(set_name.capitalize(), source_path,
                                                      subject_ids, setting)
            return_ratios[set_name] = len(curr_subject_ids) / len(subject_ids)
            return_subjects[set_name] = curr_subject_ids

        # If not train specified select the substraction of the other sets
        if not "train" in demographic_split:
            train_subjects = set(subject_ids)
            prefix = "Train"
            for set_name, setting in demographic_split.items():
                train_subjects.intersection_update(
                    self._get_demographics(prefix, source_path, subject_ids, setting, invert=True))
                prefix = ""
            return_subjects["train"] = list(train_subjects)
            return_ratios["train"] = len(train_subjects) / len(subject_ids)

        return return_subjects, return_ratios

    def _create_ratio_df(self, subject_ids: List[int], sample_counts: dict):
        """
        Creates a DataFrame with the ratio of subject samples to total samples for each subject.
        """
        subject_ratios = [
            (subject_id, sample_counts[subject_id]["total"]) for subject_id in subject_ids
        ]
        ratio_df = pd.DataFrame(subject_ratios, columns=['participant', 'ratio'])
        total_len = ratio_df["ratio"].sum()
        ratio_df["ratio"] = ratio_df["ratio"] / total_len
        ratio_df = ratio_df.sort_values('ratio')
        return ratio_df

    def _split_by_ratio(self,
                        subject_ids: List[int],
                        sample_counts: dict,
                        test_size: float = 0.0,
                        val_size: float = 0.0):
        """
        Splits subjects based on specified ratios.
        """
        if test_size < 0 or test_size > 1:
            raise ValueError("Invalid test size")
        if val_size < 0 or val_size > 1:
            raise ValueError("Invalid val size")
        return_ratios = dict()
        return_subjects = dict()
        ratio_df = self._create_ratio_df(subject_ids, sample_counts)

        train_subjects = set(subject_ids)

        def create_split(total_subjects, ratio_df, size):
            """_summary_
            """
            subjects, split_ratio = self._subjects_for_ratio(ratio_df, size)
            remaining_subjects = total_subjects - set(subjects)
            updated_ratio_df = ratio_df[~ratio_df.participant.isin(subjects)]
            return list(subjects), remaining_subjects, updated_ratio_df, split_ratio

        if test_size:
            return_subjects["test"], \
            train_subjects, \
            ratio_df, \
            return_ratios["test"] = create_split(total_subjects=train_subjects,
                                                 ratio_df=ratio_df,
                                                 size=test_size)
        if val_size:
            return_subjects["val"],\
            train_subjects, \
            ratio_df, \
            return_ratios["val"] = create_split(total_subjects=train_subjects,
                                                ratio_df=ratio_df,
                                                size=val_size)

        if train_subjects:
            return_subjects["train"] = list(train_subjects)[:]
            return_ratios["train"] = 1 - sum(list(return_ratios.values()))

        return return_subjects, return_ratios

    def _get_demographics(self,
                          prefix: str,
                          source_path: Path,
                          subject_ids: list,
                          settings: dict,
                          invert=False):
        """
        Filters subjects based on demographic group.
        """
        if source_path is None or settings is None:
            return subject_ids
        self._check_settings(settings)
        subject_info_df = pd.read_csv(Path(source_path, "subject_info.csv"))
        subject_info_df = subject_info_df[subject_info_df["SUBJECT_ID"].isin(subject_ids)]

        if prefix is not None and prefix:
            message = [prefix + ":"]
        else:
            message = []

        def get_subjects(condition: pd.Series):
            return subject_info_df[condition]["SUBJECT_ID"].unique()

        def get_categorical_message(choice):
            choice = list(choice)
            if len(choice) > 1:
                return f"is one of " + ", ".join(
                    str(entry) for entry in choice[:-1]) + " or " + str(choice[-1])
            else:
                return f"is {choice[0]}"

        def check_setting(setting, attribute):
            if not attribute in subject_info_df.columns:
                raise ValueError(f"Invalid demographic. Choose from {*subject_info_df.columns,}\n"
                                 f"Demographic is: {attribute}")
            if "geq" in setting:
                check_range(setting["geq"], setting)
                assert not "greater" in setting, "Invalid setting, cannot have both less and leq"

            if "greater" in setting:
                check_range(setting["greater"], setting)

            if "leq" in setting and "less" in setting:
                raise ValueError("Invalid setting, cannot have both less and leq")

            if ("geq" in setting or "leq" in setting or "less" in setting or
                    "greater" in setting) and "choice" in setting:
                raise ValueError("Invalid setting, cannot have both range and choice")

        def check_range(greater_value, setting):
            for key in ["leq", "less"]:
                if key in setting:
                    if greater_value > setting[key]:
                        raise ValueError(
                            f"Invalid range: greater={greater_value} > leq={setting[key]}")

        if invert:
            exclude_subjects = set(subject_ids)
        else:
            exclude_subjects = set()

        for attribute, setting in settings.items():
            attribute_message = " "
            check_setting(setting, attribute)
            attribute_data = subject_info_df[attribute]

            if "geq" in setting:
                # We use this reversed logic to avoid including any subjects where on stay
                # may fail the specification
                if invert:
                    exclude_subjects.intersection_update(
                        get_subjects(attribute_data >= setting["geq"]))
                    attribute_message += f"{setting['geq']:0.3f} > "
                else:
                    exclude_subjects.update(get_subjects(attribute_data < setting["geq"]))
                    attribute_message += f"{setting['geq']:0.3f} =< "

            if "greater" in setting:
                if invert:
                    exclude_subjects.intersection_update(
                        get_subjects(attribute_data > setting["greater"]))
                    attribute_message += f"{setting['greater']:0.3f} >= "
                else:
                    exclude_subjects.update(get_subjects(attribute_data <= setting["greater"]))
                    attribute_message += f"{setting['greater']:0.3f} < "

            if invert and ("geq" in setting or "greater" in setting) and\
                          ("leq" in setting or "less" in setting):
                attribute_message += f"{attribute} or "
            elif invert and not ("geq" in setting or "greater" in setting) and \
                                ("leq" in setting or "less" in setting):
                pass
            else:
                attribute_message += f"{attribute} "

            if "leq" in setting:
                if invert:
                    exclude_subjects.intersection_update(
                        get_subjects(attribute_data < setting["leq"]))
                    attribute_message += f"{setting['leq']:0.3f} < {attribute}"
                else:
                    exclude_subjects.update(get_subjects(attribute_data >= setting["leq"]))
                    attribute_message += f"<= {setting['leq']:0.3f}"

            if "less" in setting:
                if invert:
                    exclude_subjects.intersection_update(
                        get_subjects(attribute_data < setting["less"]))
                    attribute_message += f"{setting['less']:0.3f} <= {attribute}"
                else:
                    exclude_subjects.update(get_subjects(attribute_data >= setting["less"]))
                    attribute_message += f"< {setting['less']:0.3f}"

            if "choice" in setting:
                categories = attribute_data.unique()
                not_choices = set(categories) - set(setting["choice"])
                if invert:
                    exclude_subjects.intersection_update(
                        get_subjects(attribute_data.isin(setting["choice"])))
                    attribute_message += get_categorical_message(not_choices)
                else:
                    exclude_subjects.update(get_subjects(attribute_data.isin(not_choices)))
                    attribute_message += get_categorical_message(setting["choice"])

            message.append(attribute_message)

        if prefix is not None:
            info_io("\n".join(message))
        subject_info_df = subject_info_df[~subject_info_df["SUBJECT_ID"].isin(exclude_subjects)]
        return subject_info_df["SUBJECT_ID"].unique().tolist()

    def _subjects_for_ratio(self, ratio_df: pd.DataFrame, target_size: float):
        """
        Selects subjects to match the target ratio.
        """
        assert "participant" in ratio_df.columns
        assert "ratio" in ratio_df.columns
        best_diff = 1e18
        iter = 0

        def compute_ratios(random_state):
            current_size = 0
            remaining_pairs_df = ratio_df_pr
            subjects = list()
            sample_size = int(min(1, np.floor(target_size_pr / remaining_pairs_df.ratio.max())))

            while current_size < target_size_pr:
                current_to_rarget_diff = target_size_pr - current_size
                remaining_pairs_df = remaining_pairs_df[remaining_pairs_df['ratio'] <
                                                        current_to_rarget_diff]

                if remaining_pairs_df.empty:
                    break

                next_subject = remaining_pairs_df.sample(sample_size, random_state=random_state)

                current_size += sum(next_subject.ratio.tolist())
                subject_names = next_subject.participant.tolist()
                remaining_pairs_df = remaining_pairs_df[~remaining_pairs_df.participant.
                                                        isin(subject_names)]

                subjects.extend(subject_names)
                if remaining_pairs_df.empty:
                    break

                large_sample_bound = int(np.floor(
                                        abs(target_size_pr - current_size)\
                                        / remaining_pairs_df.ratio.max()))
                sample_size = min(large_sample_bound, len(remaining_pairs_df))

            diff = abs(target_size_pr - current_size)
            return diff, current_size, subjects

        def init(ratio_df, target_size):
            global ratio_df_pr, target_size_pr
            ratio_df_pr = ratio_df
            target_size_pr = target_size

        pool = Pool()  # Create a process pool
        n_cpus = cpu_count() - 1
        with Pool(n_cpus, initializer=init, initargs=(ratio_df, target_size)) as pool:
            res = pool.imap_unordered(compute_ratios,
                                      list(range(self._max_iter)),
                                      chunksize=int(np.ceil(self._max_iter / n_cpus)))
            while best_diff > self._tolerance and iter < self._max_iter:
                diff, current_size, subjects = next(res)
                iter += 1
                if diff < best_diff:
                    best_subjects, best_size, best_diff = subjects, current_size, diff

        return best_subjects, best_size

    def _split_val_from_train(self, val_size: float, split_dictionary: dict, ratios: dict,
                              split_tracker: DataSplitTracker):
        """
        Splits validation set from training set.
        """
        adj_val_size = val_size / (val_size + ratios["train"])
        sub_split_dictionary, sub_ratios = self._split_by_ratio(
            subject_ids=split_dictionary["train"],
            sample_counts=split_tracker.subjects,
            val_size=adj_val_size)
        split_dictionary["val"] = sub_split_dictionary["val"]
        split_dictionary["train"] = sub_split_dictionary["train"]
        ratios["val"] = sub_ratios["val"] * (val_size + ratios["train"])
        ratios["train"] = sub_ratios["train"] * (val_size + ratios["train"])
        return split_dictionary, ratios


class ReaderSplitter(AbstractSplitter):
    """
    Splits datasets reader based either by set ratios or demgoraphic groups.
    """

    def split_reader(self,
                     reader: ProcessedSetReader,
                     test_size: float = 0.0,
                     val_size: float = 0.0,
                     train_size: int = None,
                     demographic_split: dict = None,
                     demographic_filter: dict = None,
                     storage_path: Path = None):
        """
        Splits the dataset into training, validation, and test sets. The attribute can be found in the split_info_df.csv file.

        Parameters
        ----------
        reader : ProcessedSetReader
            Reader object to load the dataset.
        test_size : float, optional
            Proportion of the dataset to include in the test split. Default is 0.0.
        val_size : float, optional
            Proportion of the dataset to include in the validation split. Default is 0.0.
        train_size : int, optional
            Number of samples to include in the training split. If specified, overrides the proportion-based split for training set. Default is None.
        demographic_split : dict, optional
            Dictionary specifying demographic criteria for splitting the dataset. Default is None.
        demographic_filter : dict, optional
            Dictionary specifying demographic criteria for filtering the dataset before splitting. Default is None.
        storage_path : Path, optional
            Path to the directory where the split information will be saved. Default is None, which uses the reader's root path.

        Returns
        -------
        SplitSetReader
            Reader object for the split dataset.
        """
        info_io(f"Splitting reader", level=0)
        self._print_ratio("Target", {
            "train": 1 - test_size - val_size,
            "test": test_size,
            "val": val_size
        })
        # Resotre split state
        storage_path = Path((storage_path \
                             if storage_path is not None \
                             else reader.root_path), "split")
        # Get subject counts
        preprocessing_tracker = PreprocessingTracker(Path(reader.root_path, "progress"))

        split_tracker = DataSplitTracker(storage_path,
                                         tracker=preprocessing_tracker,
                                         test_size=test_size,
                                         val_size=val_size,
                                         demographic_split=demographic_split,
                                         demographic_filter=demographic_filter)
        # Identical split settings
        if split_tracker.is_finished:
            info_io(f"Restoring split from directory:\n{reader.root_path}")
            self._print_ratio("Real", split_tracker.ratios)
            return SplitSetReader(reader.root_path, split_tracker.split)

        info_io(f"Splitting in directory:\n{reader.root_path}")
        # Apply demographic filter
        subject_ids = self._get_demographics(prefix="Demographic filter",
                                             source_path=reader.root_path,
                                             subject_ids=reader.subject_ids,
                                             settings=demographic_filter)

        # Split by demographics
        if demographic_split is not None and demographic_split:
            # Apply split
            split_dictionary, ratios = self._split_by_demographics(
                subject_ids=subject_ids,
                source_path=reader.root_path,
                demographic_split=demographic_split)

            # Enforce ratio
            if test_size or val_size:
                split_dictionary, ratios = self._reduce_by_ratio(
                    subjects=split_dictionary,
                    sample_counts=split_tracker.subjects,
                    test_size=test_size,
                    val_size=(val_size if "val" in split_dictionary else 0))

            if train_size is not None and val_size and not "val" in split_dictionary:
                # In this case we split the val set from the train set
                split_dictionary, ratios = self._split_val_from_train(
                    val_size=val_size,
                    split_dictionary=split_dictionary,
                    ratios=ratios,
                    split_tracker=split_tracker)

            # Update tracker
            split_tracker.split = split_dictionary
            split_tracker.ratios = ratios
        else:
            # Split by ratio
            split_dictionary, ratios = self._split_by_ratio(subject_ids=subject_ids,
                                                            sample_counts=split_tracker.subjects,
                                                            test_size=test_size,
                                                            val_size=val_size)
            if train_size is not None:
                # Reduce train size if specified
                if train_size > len(subject_ids):
                    warn_io(f"Train size {train_size} is larger than the number of subjects")
                else:
                    split_dictionary["train"] = random.sample(split_dictionary["train"], train_size)

                split_dictionary, ratios = self._reduce_by_ratio(
                    subjects=split_dictionary,
                    sample_counts=split_tracker.subjects,
                    test_size=test_size,
                    val_size=val_size)
            # Update tracker
            split_tracker.split = split_dictionary
            split_tracker.ratios = ratios

        self._print_ratio("Real", ratios)
        split_tracker.is_finished = True

        return SplitSetReader(reader.root_path, split_dictionary)


class CompactSplitter(AbstractSplitter):

    def split_dict(self,
                   X_subjects: Dict[str, Dict[str, pd.DataFrame]],
                   y_subjects: Dict[str, Dict[str, pd.DataFrame]],
                   test_size: float = 0.0,
                   val_size: float = 0.0,
                   demographic_split: dict = None,
                   demographic_filter: dict = None,
                   source_path: Path = None):
        """Not yet implemented
        """
        info_io(f"Splitting reader", level=0)

        target_ratios = {"train": 1 - test_size - val_size, "test": test_size, "val": val_size}
        message = ""
        for set_name in target_ratios:
            if target_ratios[set_name]:
                message += f"Target {set_name} size: {target_ratios[set_name]:0.3f}\n"
        info_io(message)
        info_io(f"Splitting in directory: {source_path}")
        if (demographic_filter is not None or demographic_split is not None) and not source_path:
            raise ValueError("Demographic split requires source path"
                             " to locate the subject_info.csv file")

        if demographic_filter is not None:
            subject_ids = self._get_demographics(source_path, list(X_subjects.keys()),
                                                 demographic_filter)
        else:
            subject_ids = list(X_subjects.keys())

        if source_path:
            tracker = PreprocessingTracker(Path(source_path, "progress"), subject_ids=subject_ids)
            split_tracker = DataSplitTracker(Path(source_path, "split"), tracker, test_size,
                                             val_size)
            if split_tracker.is_finished:
                ...
                # return SplitSetReader(source_path, )
        else:
            split_tracker = None

        if demographic_split is not None and ("test" in demographic_split or "train"
                                              in demographic_split or "val" in demographic_split):
            subjects, ratios = self._split_by_demographics(subject_ids, source_path,
                                                           demographic_split)
            if test_size or val_size:
                subjects, ratios = self._reduce_by_ratio(subjects=subjects,
                                                         input_ratios=ratios,
                                                         test_size=test_size,
                                                         val_size=val_size)
            if split_tracker is not None:
                for split in subjects:
                    setattr(split_tracker, split, subjects[split])
                split_tracker.ratios = ratios
        else:
            subject_counts = self._compute_subject_counts(y_subjects)
            subjects, ratios = self._split_by_ratio(subject_ids, subject_counts, test_size,
                                                    val_size)
            if split_tracker is not None:
                for split in subjects:
                    setattr(split_tracker, split, subjects[split])
                split_tracker.ratios = ratios

        message = ""
        for set_name in ratios:
            message += f"Real {set_name} size: {ratios[set_name]:0.3f}\n"
        info_io(message)

        if split_tracker is not None:
            split_tracker.is_finished = True
        dataset = {
            set_name: (dict_subset(X_subjects, subjects), dict_subset(y_subjects, subjects))
            for set_name, subjects in subjects.items()
        }

        return dataset

    def _compute_subject_counts(self, y_subjects: Dict[str, Dict[str, pd.DataFrame]]):
        subject_counts = {
            subject_id: {
                stay_id: len(stay_data) for stay_id, stay_data in subject_data.items()
            } for subject_id, subject_data in y_subjects.items()
        }
        for subject in subject_counts:
            subject_counts[subject]["total"] = sum(subject_counts[subject].values())
        return subject_counts


### FILE: .\src\datasets\split\__init__.py ###
"""
Data Splitting Module
=====================

This module provides functionality to split datasets into training, validation, and test sets.
Splits can be performed based on predefined ratios or demographic filters, and the module
supports both dictionary-based and reader-based dataset structures.

Functions
---------
- train_test_split(X_subjects, y_subjects, test_size, val_size, train_size, demographic_split, demographic_filter, source_path)
    Splits the dictionary-based dataset into training, validation, and test sets.
- train_test_split(reader, test_size, val_size, train_size, demographic_split, demographic_filter, storage_path)
    Splits the reader-based dataset into training, validation, and test sets.

Examples
--------
>>> # Split a reader by ratio
>>> reader = ProcessedSetReader(root_path="/path/to/data")
>>> split_reader = ReaderSplitter().split_reader(reader, test_size=0.2, val_size=0.1)
>>> split_reader.test.subject_ids
>>> [ ... ]

>>> # Split a reader by demographics with ratio control
>>> demographic_split = {
...     "test": {
...         "AGE": {
...             "greater": 60
...         }
...     },
...     "val": {
...         "AGE": {
...             "leq": 60,
...             "greater": 40
...         }
...     }
... }
>>> split_reader = ReaderSplitter().split_reader(reader, 
...                                              test_size=0.2, 
...                                              val_size=0.1, 
...                                              demographic_split=demographic_split)
>>> split_reader.test.subject_ids
>>> [ ... ]

>>> # Or split a reader by demographics only
>>> demographic_split = {
...     "test": {
...         "AGE": {
...             "greater": 60
...         }
...     },
...     "val": {
...         "AGE": {
...             "leq": 60,
...             "greater": 40
...         }
...     }
... }
>>> split_reader = ReaderSplitter().split_reader(demographic_split=demographic_split)

>>> # Reduce a reader to a subdemographic and split by ratio
>>> demographic_filter = {
...     "AGE": {
...         "greater": 60
...     }
... }
>>> split_reader = ReaderSplitter().split_reader(reader, 
...                                              test_size=0.2, 
...                                              val_size=0.1, 
...                                              demographic_split=demographic_split)

>>> # Demographic split with categorical attribute
>>> demographic_split = {
...     "test": {
...         "GENDER": {
...             "choice": ["M"]
...         }
...     },
...     "val": {
...         "GENDER": {
...             "choice": ["F"]
...         }
...     }
... }
>>> split_reader = ReaderSplitter().split_reader(reader, 
...                                              test_size=0.2, 
...                                              val_size=0.1, 
...                                              demographic_split=demographic_split)

"""

import pandas as pd
from pathlib import Path
from typing import Dict, Union
from multipledispatch import dispatch
from numbers import Number
from utils.IO import *
from settings import *
from .splitters import ReaderSplitter, CompactSplitter
from ..readers import ProcessedSetReader, SplitSetReader

__all__ = ['train_test_split']


@dispatch(dict,
          dict,
          test_size=float,
          val_size=float,
          train_size=Number,
          demographic_split=dict,
          demographic_filter=dict,
          source_path=Path)
def train_test_split(X_subjects: Dict[str, Dict[str, pd.DataFrame]],
                     y_subjects: Dict[str, Dict[str, pd.DataFrame]],
                     test_size: float = 0.0,
                     val_size: float = 0.0,
                     train_size: int = None,
                     demographic_split: dict = None,
                     demographic_filter: dict = None,
                     source_path: Path = None) -> Dict[str, Dict[str, Dict[str, pd.DataFrame]]]:
    """
    Splits the dictionary-based dataset into training, validation, and test sets.

    Parameters
    ----------
    X_subjects : dict
        Dictionary containing feature data for subjects.
    y_subjects : dict
        Dictionary containing label data for subjects.
    test_size : float, optional
        Proportion of the dataset to include in the test split. Default is 0.0.
    val_size : float, optional
        Proportion of the dataset to include in the validation split. Default is 0.0.
    train_size : int, optional
        Number of samples to include in the training split. If specified, overrides the proportion-based split for the training set. Default is None.
    demographic_split : dict, optional
        Dictionary specifying demographic criteria for splitting the dataset. Default is None.
    demographic_filter : dict, optional
        Dictionary specifying demographic criteria for filtering the dataset before splitting. Default is None.
    source_path : Path, optional
        Path to the directory containing the dataset. Default is None.

    Returns
    -------
    Dict[str, Dict[str, Dict[str, pd.DataFrame]]]
        Dictionary containing split data for training, validation, and test sets.
    """
    return CompactSplitter().split_dict(X_subjects=X_subjects,
                                        y_subjects=y_subjects,
                                        test_size=test_size,
                                        val_size=val_size,
                                        train_size=train_size,
                                        source_path=source_path,
                                        demographic_split=demographic_split,
                                        demographic_filter=demographic_filter)


@dispatch(ProcessedSetReader,
          test_size=float,
          val_size=float,
          train_size=Number,
          demographic_split=dict,
          demographic_filter=dict,
          storage_path=Path)
def train_test_split(reader: ProcessedSetReader,
                     test_size: float = 0.0,
                     val_size: float = 0.0,
                     train_size: int = None,
                     demographic_split: dict = None,
                     demographic_filter: dict = None,
                     storage_path=None) -> SplitSetReader:
    """
    Splits the reader-based dataset into training, validation, and test sets.

    Parameters
    ----------
    reader : ProcessedSetReader
        Reader object to load the dataset.
    test_size : float, optional
        Proportion of the dataset to include in the test split. Default is 0.0.
    val_size : float, optional
        Proportion of the dataset to include in the validation split. Default is 0.0.
    train_size : int, optional
        Number of samples to include in the training split. If specified, overrides the proportion-based split for the training set. Default is None.
    demographic_split : dict, optional
        Dictionary specifying demographic criteria for splitting the dataset. Default is None.
    demographic_filter : dict, optional
        Dictionary specifying demographic criteria for filtering the dataset before splitting. Default is None.
    storage_path : Path, optional
        Path to the directory where the split information will be saved. Default is None, which uses the reader's root path.

    Returns
    -------
    SplitSetReader
        Reader object for the split dataset.
    """
    return ReaderSplitter().split_reader(reader=reader,
                                         test_size=test_size,
                                         val_size=val_size,
                                         train_size=train_size,
                                         demographic_split=demographic_split,
                                         demographic_filter=demographic_filter,
                                         storage_path=storage_path)


@dispatch(ProcessedSetReader,
          float,
          val_size=float,
          train_size=Number,
          demographic_split=dict,
          demographic_filter=dict,
          storage_path=Path)
def train_test_split(reader: ProcessedSetReader,
                     test_size: float = 0.0,
                     val_size: float = 0.0,
                     train_size: int = None,
                     demographic_split: dict = None,
                     demographic_filter: dict = None,
                     storage_path=None) -> SplitSetReader:
    return ReaderSplitter().split_reader(reader=reader,
                                         test_size=test_size,
                                         val_size=val_size,
                                         train_size=train_size,
                                         demographic_split=demographic_split,
                                         demographic_filter=demographic_filter,
                                         storage_path=storage_path)


@dispatch(ProcessedSetReader,
          float,
          float,
          train_size=Number,
          demographic_split=dict,
          demographic_filter=dict,
          storage_path=Path)
def train_test_split(reader: ProcessedSetReader,
                     test_size: float = 0.0,
                     val_size: float = 0.0,
                     train_size: int = None,
                     demographic_split: dict = None,
                     demographic_filter: dict = None,
                     storage_path=None) -> SplitSetReader:
    return ReaderSplitter().split_reader(reader=reader,
                                         test_size=test_size,
                                         val_size=val_size,
                                         train_size=train_size,
                                         demographic_split=demographic_split,
                                         demographic_filter=demographic_filter,
                                         storage_path=storage_path)


@dispatch(ProcessedSetReader,
          float,
          float,
          Number,
          demographic_split=dict,
          demographic_filter=dict,
          storage_path=Path)
def train_test_split(reader: ProcessedSetReader,
                     test_size: float = 0.0,
                     val_size: float = 0.0,
                     train_size: int = None,
                     demographic_split: dict = None,
                     demographic_filter: dict = None,
                     storage_path=None) -> SplitSetReader:
    return ReaderSplitter().split_reader(reader=reader,
                                         test_size=test_size,
                                         val_size=val_size,
                                         train_size=train_size,
                                         demographic_split=demographic_split,
                                         demographic_filter=demographic_filter,
                                         storage_path=storage_path)


@dispatch(ProcessedSetReader,
          float,
          float,
          Number,
          dict,
          demographic_filter=dict,
          storage_path=Path)
def train_test_split(reader: ProcessedSetReader,
                     test_size: float = 0.0,
                     val_size: float = 0.0,
                     train_size: int = None,
                     demographic_split: dict = None,
                     demographic_filter: dict = None,
                     storage_path=None) -> SplitSetReader:
    return ReaderSplitter().split_reader(reader=reader,
                                         test_size=test_size,
                                         val_size=val_size,
                                         train_size=train_size,
                                         demographic_split=demographic_split,
                                         demographic_filter=demographic_filter,
                                         storage_path=storage_path)


@dispatch(ProcessedSetReader, float, float, Number, dict, dict, storage_path=Path)
def train_test_split(reader: ProcessedSetReader,
                     test_size: float = 0.0,
                     val_size: float = 0.0,
                     train_size: int = None,
                     demographic_split: dict = None,
                     demographic_filter: dict = None,
                     storage_path=None) -> SplitSetReader:
    return ReaderSplitter().split_reader(reader=reader,
                                         test_size=test_size,
                                         val_size=val_size,
                                         train_size=train_size,
                                         demographic_split=demographic_split,
                                         demographic_filter=demographic_filter,
                                         storage_path=storage_path)


@dispatch(ProcessedSetReader, float, float, dict, dict, Path)
def train_test_split(reader: ProcessedSetReader,
                     test_size: float = 0.0,
                     val_size: float = 0.0,
                     train_size: int = None,
                     demographic_split: dict = None,
                     demographic_filter: dict = None,
                     storage_path=None) -> SplitSetReader:
    return ReaderSplitter().split_reader(reader=reader,
                                         test_size=test_size,
                                         val_size=val_size,
                                         train_size=train_size,
                                         demographic_split=demographic_split,
                                         demographic_filter=demographic_filter,
                                         storage_path=storage_path)


### FILE: .\src\generators\pytorch.py ###
import torch
from preprocessing.scalers import AbstractScaler
from datasets.readers import ProcessedSetReader
from torch.utils.data import Dataset
from utils.IO import *
from torch.utils.data import DataLoader
from . import AbstractGenerator


class TorchGenerator(DataLoader):

    def __init__(self,
                 reader: ProcessedSetReader,
                 scaler: AbstractScaler = None,
                 batch_size: int = 8,
                 shuffle: bool = True,
                 num_workers: int = 1,
                 drop_last: bool = False,
                 bining: str = "none"):
        super().__init__(dataset=TorchDataset(reader=reader,
                                              scaler=scaler,
                                              batch_size=1,
                                              shuffle=shuffle,
                                              bining=bining),
                         batch_size=batch_size,
                         shuffle=shuffle,
                         drop_last=drop_last,
                         num_workers=num_workers,
                         collate_fn=self.collate_fn)

    def collate_fn(self, batch):
        samples, labels = zip(*batch)
        samples = torch.stack(self._zeropad_samples(samples))
        labels = torch.cat(labels)
        if labels.dim() == 1:
            return samples, labels.unsqueeze(1)
        return samples, labels

    @staticmethod
    def _zeropad_samples(data):
        """_summary_

        Args:
            data (_type_): _description_

        Returns:
            _type_: _description_
        """
        # Ensure data is a list of PyTorch tensors
        if not all(isinstance(x, torch.Tensor) for x in data):
            raise ValueError("All items in the data list must be PyTorch tensors")

        # Determine the dtype and device from the first tensor
        dtype = data[0].dtype
        device = data[0].device

        # Find the maximum length along the first dimension
        max_len = max(x.shape[0] for x in data)

        # Pad each tensor to the maximum length
        padded_data = [
            torch.cat(
                [x,
                 torch.zeros((max_len - x.shape[0],) + x.shape[1:], dtype=dtype, device=device)],
                dim=0) for x in data
        ]

        # Stack all padded tensors into a single tensor
        return padded_data


class TorchDataset(AbstractGenerator, Dataset):

    def __init__(self,
                 reader: ProcessedSetReader,
                 scaler: AbstractScaler = None,
                 batch_size: int = 8,
                 shuffle: bool = True,
                 bining: str = "none"):
        super(TorchDataset, self).__init__(reader=reader,
                                           scaler=scaler,
                                           batch_size=batch_size,
                                           shuffle=shuffle,
                                           bining=bining)

    def __getitem__(self, index=None):
        X, y = super().__getitem__(index)
        X = X.squeeze()
        return torch.from_numpy(X).to(torch.float32), torch.from_numpy(y).to(torch.int8)


### FILE: .\src\generators\stream.py ###
import torch
import pandas as pd
import numpy as np
from typing import List
from preprocessing.scalers import AbstractScaler
from datasets.readers import ProcessedSetReader
from torch.utils.data import Dataset
from utils.IO import *
from torch.utils.data import DataLoader
from . import AbstractGenerator


class RiverGenerator(AbstractGenerator, Dataset):

    def __init__(self,
                 reader: ProcessedSetReader,
                 scaler: AbstractScaler,
                 shuffle: bool = True,
                 bining: str = "none"):
        super(RiverGenerator, self).__init__(reader=reader,
                                             scaler=scaler,
                                             batch_size=1,
                                             shuffle=shuffle,
                                             bining=bining)
        self._names: List[str] = None
        self._labels: List[str] = None
        self._index = 0
        self._row_only = True

    def __iter__(self):
        self._index = 0
        return self

    def __next__(self, *args, **kwargs):
        if self._index >= self.steps:
            raise StopIteration
        X, y = super().__getitem__()
        X = np.squeeze(X)
        if self._names is None:
            self._names = [str(i) for i in range(714)]
        X = dict(zip(self._names, X))
        y = np.squeeze(y)
        if y.shape:
            if self._labels is None:
                self._labels = [str(i) for i in range(len(y))]
            y = dict(zip(self._labels, y))
        else:
            y = float(y)
        self._index += 1
        return X, y


### FILE: .\src\generators\tf2.py ###
from preprocessing.scalers import AbstractScaler
from datasets.readers import ProcessedSetReader
from tensorflow.keras.utils import Sequence
from utils.IO import *
from . import AbstractGenerator


class TFGenerator(AbstractGenerator, Sequence):

    def __init__(self,
                 reader: ProcessedSetReader,
                 scaler: AbstractScaler = None,
                 batch_size: int = 8,
                 shuffle: bool = True,
                 bining: str = "none"):
        super(TFGenerator, self).__init__(reader=reader,
                                          scaler=scaler,
                                          batch_size=batch_size,
                                          shuffle=shuffle,
                                          bining=bining)

    def __getitem__(self, index=None):
        return super().__getitem__(index)


### FILE: .\src\generators\__init__.py ###
"""Preprocessing file

This file provides the implemented preprocessing functionalities.

"""

import numpy as np
import pandas as pd
import random

from copy import deepcopy
from pathlib import Path
from utils.IO import *
from preprocessing.scalers import AbstractScaler
from datasets.trackers import PreprocessingTracker
from datasets.readers import ProcessedSetReader
from metrics import CustomBins, LogBins


class AbstractGenerator(object):

    def __init__(self,
                 reader: ProcessedSetReader,
                 scaler: AbstractScaler,
                 batch_size: int = 8,
                 shuffle: bool = True,
                 bining: str = "none"):
        super().__init__()
        self._batch_size = batch_size
        self._shuffle = shuffle
        self._reader = reader
        self._columns = None
        self._tracker = PreprocessingTracker(storage_path=Path(reader.root_path, "progress"))
        self._steps = self._count_batches()
        self._subject_ids = reader.subject_ids
        self._scaler = scaler
        self._remaining_ids = deepcopy(self._reader.subject_ids)
        self.generator = self._generator()
        self._row_only = False

        if bining not in ["none", "log", "custom"]:
            raise ValueError("Bining must be one of ['none', 'log', 'custom']")
        self._bining = bining

    @property
    def steps(self):
        return self._steps

    def __getitem__(self, index=None):
        X_batch, y_batch = list(), list()
        for _ in range(self._batch_size):
            X, y = next(self.generator)
            X_batch.append(X)
            y_batch.append(y)
        X_batch = self._zeropad_samples(X_batch)
        y_batch = np.array(y_batch)
        return X_batch.astype(np.float32), y_batch.astype(np.float32)

    def _count_batches(self):
        """
        """
        return int(
            np.floor(
                sum([
                    self._tracker.subjects[subject_id]["total"]
                    for subject_id in self._reader.subject_ids
                ])) / self._batch_size)

    def __len__(self):
        'Denotes the number of batches per epoch'
        return self._steps

    def on_epoch_end(self):
        ...

    def _generator(self):

        while True:
            data, subjects = self._reader.random_samples(return_ids=True)
            X_subject, y_subject = data.values()
            for X_stay, y_stay in zip(X_subject, y_subject):
                if self._columns is None:
                    self._columns = X_stay.columns
                X_stay[self._columns] = self._scaler.transform(X_stay[self._columns])

                Xs, ys, ts = self.read_timeseries(X_df=X_stay,
                                                  y_df=y_stay,
                                                  row_only=self._row_only,
                                                  bining=self._bining)

                (Xs, ys, ts) = self._shuffled_data([Xs, ys, ts])

                index = 0
                while index < len(ys):
                    yield Xs[index], ys[index]
                    index += 1

    @staticmethod
    def read_timeseries(X_df: pd.DataFrame, y_df: pd.DataFrame, row_only=False, bining="none"):
        """
        """
        Xs = list()
        ys = list()
        ts = list()

        for timestamp in y_df.index:

            y = np.squeeze(y_df.loc[timestamp].values)
            if not y.shape:
                y = float(y)
                if bining == "log":
                    LogBins.get_bin_log(y)
                elif bining == "custom":
                    CustomBins.get_bin_custom(y)

            if row_only:
                X = X_df.loc[timestamp].values
            else:
                X = X_df.loc[:timestamp].values

            Xs.append(X)
            ys.append(y)
            ts.append(timestamp)

        return Xs, ys, ts

    def _shuffled_data(self, data):
        """_summary_

        Args:
            data (_type_): _description_

        Returns:
            _type_: _description_
        """
        assert len(data) >= 2

        if type(data[0][0]) == pd.DataFrame:
            data[0] = [x.values for x in data[0]]

        data = list(zip(*data))

        if self._shuffle:
            random.shuffle(data)

        residual_length = len(data) % self._batch_size
        head = data[:len(data) - residual_length]
        residual = data[len(data) - residual_length:]

        head.sort(key=(lambda x: x[0].shape[0]))

        batches = [head[i:i + self._batch_size] for i in range(0, len(head), self._batch_size)]

        if self._shuffle:
            random.shuffle(batches)

        data = list()

        for batch in batches:
            data += batch

        data += residual
        data = list(zip(*data))

        return data

    @staticmethod
    def _zeropad_samples(data):
        """_summary_

        Args:
            data (_type_): _description_

        Returns:
            _type_: _description_
        """
        dtype = data[0].dtype
        max_len = max([x.shape[0] for x in data])
        ret = [
            np.concatenate([x, np.zeros((max_len - x.shape[0],) + x.shape[1:], dtype=dtype)],
                           axis=0) for x in data
        ]
        return np.array(ret)


### FILE: .\src\metrics\stream.py ###
import collections
import numpy as np
from typing import Dict
from river import metrics as river_metrics
from scipy import integrate
from metrics import CustomBins
from river import metrics
import warnings

# Suppress the specific deprecation warning
warnings.filterwarnings(
    "ignore",
    category=DeprecationWarning,
)


class LOSClassificationReport(river_metrics.ClassificationReport):

    def update(self, y_true, y_pred, w=1):
        y_true_bin = CustomBins.get_bin_custom(y_true)
        y_pred_bin = CustomBins.get_bin_custom(y_pred)
        return super().update(y_true_bin, y_pred_bin, w)


class LOSCohenKappa(river_metrics.CohenKappa):

    def __init__(self, cm=None, weights=None):
        super().__init__(cm)
        self.weights = weights
        self.nbins = CustomBins.nbins

    def update(self, y_true, y_pred):
        y_true_bin = CustomBins.get_bin_custom(y_true)
        y_pred_bin = CustomBins.get_bin_custom(y_pred)
        self.cm.update(y_true_bin, y_pred_bin)
        return self

    def get(self):
        # Number of classes
        classes = self.cm.classes
        n_classes = len(classes)

        # Calculate expected matrix
        expected = np.zeros((n_classes, n_classes))
        sum0 = self.cm.sum_col
        sum1 = self.cm.sum_row
        total_sum0 = self.cm.n_samples

        # Calculate the numerator and denominator for kappa
        numerator = 0
        denominator = 0

        # Fill weight matrix according to the specified weights
        for i in range(n_classes):
            for j in range(n_classes):
                expected[i, j] = (sum0[j] * sum1[i]) / total_sum0
                if self.weights is None:
                    weights = 0 if i == j else 1
                elif self.weights == "linear":
                    weights = abs(i - j)
                else:  # quadratic
                    weights = (i - j)**2
                numerator += weights * self.cm[i][j]
                denominator += weights * expected[i, j]

        # Compute kappa
        if denominator == 0:
            return 0.0
        k = numerator / denominator
        return 1 - k

    def revert(self, y_true, y_pred):
        y_true_bin = CustomBins.get_bin_custom(y_true)
        y_pred_bin = CustomBins.get_bin_custom(y_pred)
        self.cm.revert(y_true_bin, y_pred_bin)
        return self


class PRAUC(river_metrics.ROCAUC):

    def __init__(self, n_thresholds=2, pos_val=True):
        self.pos_val = pos_val
        self.n_thresholds = n_thresholds
        self.thresholds = [i / (n_thresholds - 1) for i in range(n_thresholds)]
        self.thresholds[0] -= 1e-7
        self.thresholds[-1] += 1e-7
        self.cms = [river_metrics.ConfusionMatrix() for _ in range(n_thresholds)]

    def update(self, y_true, y_pred, w=1.0):
        p_true = y_pred.get(True, 0.0) if isinstance(y_pred, dict) else y_pred
        for t, cm in zip(self.thresholds, self.cms):
            cm.update(y_true == self.pos_val, p_true > t, w)

    def get(self):
        tprs = [0] * self.n_thresholds
        fprs = [0] * self.n_thresholds

        def safe_div(a, b):
            if a == 0 and b == 0:
                return 1.0
            try:
                return a / b
            except ZeroDivisionError:
                return 0.0

        for i, cm in enumerate(self.cms):
            tp = cm.true_positives(self.pos_val)
            fp = cm.false_positives(self.pos_val)
            fn = cm.false_negatives(self.pos_val)
            tprs[i] = safe_div(tp, tp + fp)  # precision
            fprs[i] = safe_div(tp, tp + fn)  # recall

        return -integrate.trapezoid(x=fprs, y=tprs)


class MicroROCAUC(river_metrics.ROCAUC):

    def __init__(self, n_thresholds=10, pos_val=True):
        self._y_true_all = []
        self._y_pred_all = []
        super().__init__(n_thresholds=n_thresholds, pos_val=pos_val)

    def update(self, y_true: dict, y_pred: dict):
        for yt, yp in zip(y_true.values(), y_pred.values()):
            super().update(yt, yp)
        return self

    def revert(self, y_true: dict, y_pred: dict):
        for yt, yp in zip(y_true.values(), y_pred.values()):
            super().revert(yt, yp)
        return self

    def works_with(self, y_true: dict, y_pred: dict):
        return isinstance(y_true, dict) and isinstance(y_pred, dict)


class MacroROCAUC(river_metrics.base.MultiClassMetric):

    def __init__(self, n_thresholds=10, pos_val=True):
        self._n_thresholds = n_thresholds
        self._pos_val = pos_val
        self._per_class_rocaucs = collections.defaultdict(river_metrics.ROCAUC)
        self._classes = set()

    def update(self, y_true: dict, y_pred: dict):
        for label_name, y_label in y_pred.items():
            self._classes.add(label_name)
            self._per_class_rocaucs[label_name].update(y_true[label_name], y_label)
        return self

    def get(self):
        # Calculate the macro-average ROC AUC
        return np.mean([rocauc.get() for rocauc in self._per_class_rocaucs.values()])

    def revert(self, y_true, y_pred):
        for label_name, y_label in y_pred.items():
            self._per_class_rocaucs[label_name].revert(y_true[label_name], y_label)
        return self

    @property
    def bigger_is_better(self):
        return True

    def works_with(self, y_true, y_pred):
        return isinstance(y_true, dict) and isinstance(y_pred, dict)


if __name__ == '__main__':

    y_pred = [23, 60, 100, 130, 160, 190, 220, 250, 360, 830]
    y_true = [11.45, 35.07, 59.20, 83.38, 107.48, 131.57, 155.64, 179.66, 254.30, 585.32]

    # ---------------- Comparing Cohen's Kappa using River with sklearn --------------------
    print("--- Comparing Cohen's Kappa ---")
    metric = LOSCohenKappa(weights='linear')
    for yt, yp in zip(y_true, y_pred):
        metric.update(yt, yp)
    """
    from time import time
    start = time()
    for _ in range(100000):
        metric.get()
    end = time()
    print("Time taken new implementation", end - start)
    #
    metric = river_metrics.CohenKappa()
    for yt, yp in zip(y_true, y_pred):
        metric.update(yt, yp)
    from time import time
    start = time()
    for _ in range(100000):
        metric.get()
    end = time()
    print("Time taken legacy", end - start)
    metric.get()
    """
    print("Cohen's Kappa (river)", metric)

    from sklearn.metrics import cohen_kappa_score

    class CustomBins:
        inf = 1e18
        bins = [(-inf, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 14),
                (14, +inf)]
        nbins = len(bins)
        # TODO: whats this
        means = [
            11.450379, 35.070846, 59.206531, 83.382723, 107.487817, 131.579534, 155.643957,
            179.660558, 254.306624, 585.325890
        ]

    def get_bin_custom(x, nbins, one_hot=False):
        for i in range(nbins):
            a = CustomBins.bins[i][0] * 24.0
            b = CustomBins.bins[i][1] * 24.0
            if a <= x < b:
                if one_hot:
                    ret = np.zeros((CustomBins.nbins,))
                    ret[i] = 1
                    return ret
                return i
        return None

    y_true_bins = [get_bin_custom(x, CustomBins.nbins) for x in y_true]
    prediction_bins = [get_bin_custom(x, CustomBins.nbins) for x in y_pred]
    kappa = cohen_kappa_score(y_true_bins, prediction_bins, weights='linear')

    print("Cohen's Kappa (scikit)", kappa)

    from river import metrics
    from sklearn.metrics import roc_auc_score, precision_recall_curve, auc

    # Binary classification data
    y_true = [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]
    y_pred = [0.1, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.35, 0.0, 0.8, 0.0, 0.2, 0.8]

    # ---------------- Comparing ROC AUC using River with sklearn --------------------
    print("--- Comparing AUC ROC ---")
    metric = metrics.ROCAUC(n_thresholds=20)
    for yt, yp in zip(y_true, y_pred):
        metric.update(yt, yp)
    print("Auc-roc (river)", metric)
    # Compute roc curve
    auc_roc = roc_auc_score(y_true, y_pred)
    print("Auc-roc (scikit)", auc_roc)

    # ---------------- Comparing Precision-Recall AUC using River with sklearn --------------------
    print("--- Comparing AUC PR ---")
    metric = PRAUC()
    for yt, yp in zip(y_true, y_pred):
        metric.update(yt, yp)
    pr_auc = metric.get()
    print("Auc-pr (river)", str(metric))
    # Compute precision-recall curve
    precision, recall, _ = precision_recall_curve(y_true, np.array(y_pred).astype(int).tolist())
    # Compute the area under the curve
    pr_auc = auc(recall, precision)
    print("Auc-pr (scikit)", pr_auc)

    # Multi-class classification data
    y_true_multi = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0],
                    [0, 0, 1], [1, 0, 0], [1, 0, 1]]
    y_true_labeled = list()
    for y in y_true_multi:
        y_true_labeled.append({str(i): yt for i, yt in enumerate(y)})
    y_pred_multi = [[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.1, 0.3, 0.6], [0.7, 0.2, 0.1],
                    [0.1, 0.6, 0.3], [0.2, 0.1, 0.7], [0.6, 0.3, 0.1], [0.3, 0.5, 0.2],
                    [0.2, 0.1, 0.7], [0.7, 0.2, 0.1]]
    y_pred_labeled = list()
    for y in y_pred_multi:
        y_pred_labeled.append({str(i): yp for i, yp in enumerate(y)})

    # ---------------- Comparing Micro-Macro ROC AUC using River with sklearn --------------------
    print("--- Comparing Micro-Macro ROC AUC ---")
    micro_rocauc = MicroROCAUC()
    macro_rocauc = MacroROCAUC()

    for yt, yp in zip(y_true_labeled, y_pred_labeled):
        micro_rocauc = micro_rocauc.update(yt, yp)
        macro_rocauc = macro_rocauc.update(yt, yp)

    print(f'Micro-average Auc-roc (river): {micro_rocauc.get():.4f}')
    print(f'Macro-average Auc-roc (river): {macro_rocauc.get():.4f}')
    from sklearn.metrics import roc_auc_score

    # Compute micro-average ROC AUC using sklearn
    micro_rocauc_sklearn = roc_auc_score(y_true_multi,
                                         y_pred_multi,
                                         average='micro',
                                         multi_class='ovr')
    print(f'Micro-average auc-roc (sklearn): {micro_rocauc_sklearn:.4f}')

    # Compute macro-average ROC AUC using sklearn
    macro_rocauc_sklearn = roc_auc_score(y_true_multi,
                                         y_pred_multi,
                                         average='macro',
                                         multi_class='ovr')
    print(f'Macro-average auc-roc (sklearn): {macro_rocauc_sklearn:.4f}')


### FILE: .\src\metrics\__init__.py ###
import numpy as np
import bisect
from settings import *


class CustomBins:
    inf = 1e18
    bins = [(-np.inf, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 14),
            (14, np.inf)]
    means = [
        11.450379, 35.070846, 59.206531, 83.382723, 107.487817, 131.579534, 155.643957, 179.660558,
        254.306624, 585.325890
    ]
    nbins = len(bins)
    # Precompute scaled bin boundaries
    scaled_bins = [(a * 24.0, b * 24.0) for a, b in bins]
    lower_bounds = [a * 24.0 for a, b in bins]

    @staticmethod
    def get_bin_custom(x, one_hot=False):
        index = bisect.bisect_right(CustomBins.lower_bounds, x) - 1
        if one_hot:
            ret = np.zeros((CustomBins.nbins,))
            ret[index] = 1
            return ret
        return index


class LogBins:
    nbins = 10
    means = [
        0.611848, 2.587614, 6.977417, 16.465430, 37.053745, 81.816438, 182.303159, 393.334856,
        810.964040, 1715.702848
    ]

    def get_bin_log(x, nbins=10, one_hot=False):
        binid = int(np.log(x + 1) / 8.0 * nbins)
        if binid < 0:
            binid = 0
        if binid >= nbins:
            binid = nbins - 1

        if one_hot:
            ret = np.zeros((LogBins.nbins,))
            ret[binid] = 1
            return ret
        return binid


### FILE: .\src\models\callbacks.py ###
"https://stackoverflow.com/questions/60727279/save-history-of-model-fit-for-different-epochs"
"https://stackoverflow.com/questions/69595923/how-to-decrease-the-learning-rate-every-10-epochs-by-a-factor-of-0-9"

import json
import tensorflow.keras.backend as K
from pathlib import Path
from tensorflow.keras import callbacks, backend


class HistoryCheckpoint(callbacks.Callback):
    """
    """

    def __init__(self, storage_path):
        """
        """
        self.storage_file = Path(storage_path, "history.json")
        super().__init__()

    def on_epoch_end(self, epoch, logs=None):
        """
        """

        if ('lr' not in logs.keys()):
            logs.setdefault('lr', 0)
            logs['lr'] = K.get_value(self.model.optimizer.lr)

        if self.storage_file.is_file():
            with open(self.storage_file, 'r+') as file:
                eval_hist = json.load(file)
        else:
            eval_hist = dict()

        for key, value in logs.items():
            if not key in eval_hist:
                eval_hist[key] = list()

            eval_hist[key].append(float(value))

        with open(self.storage_file, 'w') as file:
            json.dump(eval_hist, file, indent=4)

        return


class DecayLearningRate(callbacks.Callback):

    def __init__(self, freq, factor):
        """
        """
        self.freq = freq
        self.factor = factor

    def on_epoch_end(self, epoch, logs=None):
        """
        """
        if epoch % self.freq == 0 and not epoch == 0:  # adjust the learning rate
            lr = float(backend.get_value(self.model.optimizer.lr))  # get the current learning rate
            new_lr = lr * self.factor
            backend.set_value(self.model.optimizer.lr,
                              new_lr)  # set the learning rate in the optimizer


### FILE: .\src\models\trackers.py ###
import numpy as np
from pathlib import Path
from utils.IO import *
from storable import storable
from utils import update_json
from settings import *


@storable
class ModelHistory():

    train_loss: dict = {}
    val_loss: dict = {}
    best_val: dict = {"loss": float('inf'), "epoch": np.nan}
    best_train: dict = {"loss": float('inf'), "epoch": np.nan}
    test_loss: float = np.nan

    def to_json(self):
        update_json(Path(self._path.parent, f"{self._path.stem}.json"), self._progress)
        return self._progress


class LocalModelHistory():

    train_loss: dict = {}
    val_loss: dict = {}
    best_val: dict = {"loss": float('inf'), "epoch": np.nan}
    best_train: dict = {"loss": float('inf'), "epoch": np.nan}
    test_loss: float = np.nan

    def to_json(self):
        return {
            "train_loss": self.train_loss,
            "val_loss": self.val_loss,
            "best_val": self.best_val,
            "best_train": self.best_train,
            "test_loss": self.test_loss
        }


@storable
class RiverHistory():

    train_metrics: dict = {}
    val_metrics: dict = {}
    test_metrics: dict = {}

    def _allowed_key(self, key: str):
        return not any([key.endswith(metric) for metric in TEXT_METRICS])

    def to_text(self, report_path: Path, report_dict: dict):
        report = ""
        for key, value in report_dict.items():
            report += f"{key}:\n {value}\n"

        with open(report_path, 'w') as file:
            file.write(report)
        return report

    def to_json(self):
        numeric_dict = {}
        for attribute, history in self._progress.items():
            numeric_dict[attribute] = dict()
            for metric, value in history.items():
                if self._allowed_key(metric):
                    numeric_dict[attribute][metric] = value
        report_dict = {}
        for attribute, history in self._progress.items():
            for metric, value in history.items():
                if not metric in numeric_dict[attribute]:
                    report_dict[metric] = value

        update_json(Path(self._path.parent, f"{self._path.stem}.json"), numeric_dict)
        self.to_text(Path(self._path.parent, f"{self._path.stem}.txt"), report_dict)
        return self._progress


class LocalRiverHistory():

    train_metrics: dict = {}
    val_metrics: dict = {}
    test_metrics: dict = {}

    def _allowed_key(self, key: str):
        return not any([key.endswith(metric) for metric in self._non_numeric_metrics])

    def to_json(self):
        return {
            "train_metrics": self.train_metrics,
            "val_metrics": self.val_metrics,
            "test_metrics": self.test_metrics
        }


### FILE: .\src\models\__init__.py ###


### FILE: .\src\models\pytorch\lstm.py ###
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pickle
from typing import Union, List, Dict
from models.trackers import ModelHistory, LocalModelHistory
from torch.utils.data import DataLoader
from collections import defaultdict
from pathlib import Path
from torchmetrics import Metric
from tensorflow.keras.utils import Progbar
from utils import to_snake_case
from torch.optim import Optimizer
from utils.IO import *
from settings import *
from .mappings import *


class LSTMNetwork(nn.Module):

    def __init__(self,
                 layer_size: Union[List[int], int],
                 dropout: float,
                 input_dim: int,
                 bidirectional: bool = False,
                 recurrent_dropout: float = 0.,
                 final_activation: str = None,
                 target_repl: bool = False,
                 output_dim: int = 1,
                 depth: int = 1,
                 model_path: Path = None):
        super(LSTMNetwork, self).__init__()
        self._model_path = model_path
        if self._model_path is not None:
            # Persistent history
            self._model_path.mkdir(parents=True, exist_ok=True)
            self._history = ModelHistory(Path(self._model_path, "history"))
        else:
            # Mimics the storable
            self._history = LocalModelHistory()
        self._layer_size = layer_size
        self._dropout_rate = dropout
        self._recurrent_dropout = recurrent_dropout
        self._depth = depth
        self._bidirectional = bidirectional

        if final_activation is None:
            if output_dim == 1:
                self._final_activation = nn.Sigmoid()
            else:
                self._final_activation = nn.Softmax(dim=-1)
        else:
            self._final_activation = activation_mapping[final_activation]

        self._output_dim = output_dim

        if output_dim == 1:
            self._task = "binary"
            self._num_classes = 1
        elif isinstance(self._final_activation, nn.Softmax):
            self._task = "multiclass"
            self._num_classes = output_dim
        elif isinstance(self._final_activation, nn.Sigmoid):
            self._task = "multilabel"
            self._num_classes = output_dim

        if isinstance(layer_size, int):
            self._hidden_sizes = [layer_size] * depth
        else:
            self._hidden_sizes = layer_size
            if depth != 1:
                warn_io("Specified hidden sizes and depth are not consistent. "
                        "Using hidden sizes and ignoring depth.")

        self.lstm_layers = nn.ModuleList()
        input_size = input_dim
        for i, hidden_size in enumerate(self._hidden_sizes):
            self.lstm_layers.append(
                nn.LSTM(input_size=input_size,
                        hidden_size=hidden_size,
                        num_layers=1,
                        batch_first=True,
                        dropout=(recurrent_dropout if i < depth - 1 else 0),
                        bidirectional=bidirectional))
            input_size = hidden_size * (2 if bidirectional else 1)

        self.dropout = nn.Dropout(dropout)
        self.output_layer = nn.Linear(input_size, self._output_dim)

    @property
    def optimizer(self):
        if hasattr(self, "_optimizer"):
            return self._optimizer

    @property
    def loss(self):
        if hasattr(self, "_loss"):
            return self._loss

    def save(self, epoch):
        """_summary_
        """
        if self._model_path is not None:
            checkpoint_path = Path(self._model_path, f"cp-{epoch:04}.ckpt")
            torch.save(self.state_dict(), checkpoint_path)

    def load(self, epochs: int, weights_only=False):
        """_summary_

        Returns:
            _type_: _description_
        """
        if self._model_path is not None:
            latest_epoch = self._latest_epoch(epochs, self._model_path)
            if weights_only:
                checkpoint_path = Path(self._model_path, f"cp-{latest_epoch:04}.ckpt")
                if checkpoint_path.is_file():
                    model_state = torch.load(checkpoint_path)
                    self.load_state_dict(model_state)
                    print(f"Loaded model parameters from {checkpoint_path}")
                    return 1
            else:
                checkpoint_path = Path(self._model_path, f"cp-{latest_epoch:04}.pkl")
                if checkpoint_path.is_file():
                    with open(checkpoint_path, "rb") as load_file:
                        load_params = pickle.load(load_file)
                    for key, value in load_params.items():
                        setattr(self, key, value)
                    print(f"Loaded model from {checkpoint_path}")

                    return 1
        return 0

    def _allowed_key(self, key: str):
        return not any([key.endswith(metric) for metric in TEXT_METRICS])

    def _clean_directory(self, best_epoch: int, epochs: int, keep_latest: bool = True):
        """_summary_

        Args:
            best_epoch (int): _description_
            keep_latest (bool, optional): _description_. Defaults to True.
        """

        [
            folder.unlink()
            for i in range(epochs + 1)
            for folder in self._model_path.iterdir()
            if f"{i:04d}" in folder.name and ((i != epochs) or not keep_latest) and
            (i != best_epoch) and (".ckpt" in folder.name) and folder.is_file()
        ]

    def forward(self, x):
        x.to(self._device)
        # Masking is not natively supported in PyTorch LSTM, assume x is already preprocessed if necessary
        for lstm in self.lstm_layers:
            x, _ = lstm(x)
            x = self.dropout(x)
        x = x[:, -1, :]
        x = self.output_layer(x)
        if self._final_activation:
            x = self._final_activation(x)
        return x

    def _get_metrics(self, metrics: Dict[str, Metric]):
        keys = list([metric for metric in metrics.keys() if self._allowed_key(metric)])
        values = [metrics[key].get() for key in keys]
        return tuple(zip(keys, values))

    def _init_metrics(self, metrics, prefix: str = None) -> Dict[str, Metric]:
        settings = {"task": self._task, "num_classes": self._num_classes}
        return_metrics = dict()
        for metric in metrics:
            if isinstance(metric, str):
                metric_name = metric
                metric = metric_mapping[metric]
            else:
                metric_name = to_snake_case(metric.__name__)
            if isinstance(metric, type):
                metric = metric(**settings)
            if prefix is not None:
                metric_name = f"{prefix}_{metric_name}"
            return_metrics[metric_name] = metric
        return return_metrics

    def compile(self,
                optimizer: Dict[str, Union[str, type, Optimizer]] = None,
                loss=None,
                metrics: Dict[str, Union[str, type, Metric]] = None,
                class_weight=None):
        if isinstance(optimizer, str):
            if optimizer in optimizer_mapping:
                self._optimizer = optimizer_mapping[optimizer](self.parameters(), lr=0.001)
            else:
                raise ValueError(f"Optimizer {optimizer} not supported."
                                 f"Supported optimizers are {optimizer_mapping.keys()}")
        elif optimizer is None:
            self._optimizer = optim.Adam(self.parameters(), lr=0.001)
        else:
            self._optimizer = optimizer

        if isinstance(loss, str):
            if loss in loss_mapping:
                self._loss = loss_mapping[loss](weight=class_weight)
            else:
                raise ValueError(f"Loss {loss} not supported."
                                 f"Supported losses are {loss_mapping.keys()}")
        else:
            self._loss = loss
        if metrics is not None:
            self._metrics = self._init_metrics(metrics)
            self._train_metrics = self._init_metrics(metrics)
            self._test_metrics = self._init_metrics(metrics, prefix="test")
            self._val_metrics = self._init_metrics(metrics, prefix="val")

        self._device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.to(self._device)

    def _latest_epoch(self, epochs: int, filepath: Path):
        """_summary_

        Returns:
            _type_: _description_
        """
        if self._model_path is None:
            return 0
        check_point_epochs = [
            i for i in range(epochs + 1) for folder in filepath.iterdir()
            if f"{i:04d}" in folder.name
        ]

        if check_point_epochs:
            return max(check_point_epochs)

        return 0

    def _update_metrics(self, metrics: Dict[str, Metric], y_pred, y_true):
        # https://torchmetrics.readthedocs.io/en/v0.8.0/pages/classification.html
        # What do we need?
        for _, metric in metrics.items():
            metric.update(y_pred, y_true.astype(int))

    def _train(self,
               train_generator: DataLoader,
               epoch: int = 0,
               epochs: int = 1,
               sample_weights: dict = None,
               has_val: bool = False):
        print(f'\nEpoch {epoch}/{epochs}')
        self.train()
        train_losses = []
        generator_size = len(train_generator)
        self._train_progbar = Progbar(generator_size)

        for batch_idx, (inputs, labels) in enumerate(train_generator):
            inputs = inputs.to(self._device)
            labels = labels.to(self._device)
            self._optimizer.zero_grad()
            outputs = self(inputs)
            if sample_weights is not None:
                loss = self._loss(outputs, labels, sample_weight=sample_weights)
            else:
                loss = self._loss(outputs, labels)
            self._update_metrics(self._train_metrics, outputs, labels)
            loss.backward()
            self._optimizer.step()
            train_losses.append(loss.item())

            self._train_progbar.update(batch_idx + 1,
                                       values=[('loss', loss.item())] +
                                       self._get_metrics(self._train_metrics),
                                       finalize=(batch_idx == generator_size and not has_val))

        avg_train_loss = np.mean(train_losses)
        self._history.train_loss[epoch] = avg_train_loss
        if self._history.best_train["loss"] > avg_train_loss:
            self._history.best_train["loss"] = avg_train_loss
            self._history.best_train["epoch"] = epoch

    def _evaluate(self,
                  val_generator: DataLoader,
                  val_frequency: int,
                  epoch: int,
                  is_test: bool = False):

        if val_generator is not None and (epoch) % val_frequency == 0:
            self.eval()
            val_losses = []
            with torch.no_grad():
                for val_inputs, val_labels in val_generator:
                    val_inputs = val_inputs.to(self._device)
                    val_labels = val_labels.to(self._device)
                    val_outputs = self(val_inputs)
                    val_loss = self._loss(val_outputs, val_labels)
                    val_losses.append(val_loss.item())

            avg_val_loss = np.mean(val_losses)
            if is_test:
                self._history.test_loss = avg_val_loss
            else:
                self._history.val_loss[epoch] = avg_val_loss
                if self._history.best_val["loss"] > avg_val_loss:
                    self._history.best_val["loss"] = avg_val_loss
                    self._history.best_val["epoch"] = epoch
                if hasattr(self, "_train_progbar"):
                    self._train_progbar.update(self._train_progbar.target,
                                               values=[('loss', avg_val_loss),
                                                       ('val_loss', avg_val_loss)])
            return avg_val_loss

    def evaluate(self, test_generator: DataLoader):
        self._evaluate(val_generator=test_generator, val_frequency=0, epoch=0)

    def fit(self,
            train_generator: DataLoader,
            epochs: int,
            patience: int = None,
            save_best_only: bool = True,
            restore_best_weights: bool = True,
            sample_weights: dict = None,
            val_frequency: int = 1,
            val_generator: DataLoader = None,
            model_path: Path = None):
        if model_path is not None:
            self._model_path = model_path
            self._model_path.mkdir(parents=True, exist_ok=True)
        if patience is None:
            patience = epochs
        if val_generator is None:
            warn_io("WARNING:tensorflow:Early stopping conditioned on metric `val_loss` "
                    "which is not available. Available metrics are: loss")
        self._patience_counter = 0
        initial_epoch = self._latest_epoch(epochs, self._model_path)
        self.load(epochs, weights_only=True)

        for epoch in range(initial_epoch + 1, epochs + 1):

            self._train(train_generator=train_generator,
                        epoch=epoch,
                        epochs=epochs,
                        sample_weights=sample_weights,
                        has_val=val_generator is not None)

            self._evaluate(val_generator=val_generator, val_frequency=val_frequency, epoch=epoch)

            if val_generator is not None:
                best_epoch = self._history.best_val["epoch"]
            else:
                best_epoch = self._history.best_train["epoch"]
            if self._checkpoint(save_best_only=save_best_only,
                                restore_best_weights=restore_best_weights,
                                epoch=epoch,
                                epochs=epochs,
                                best_epoch=best_epoch,
                                patience=patience):
                break
        return self._history.to_json()

    def _checkpoint(self, save_best_only, restore_best_weights, epoch, epochs, best_epoch,
                    patience):
        if self._model_path is None:
            return False
        if save_best_only and best_epoch == epoch:
            self.save(epoch)
            self._clean_directory(best_epoch, epochs, True)
            self._patience_counter = 0
        elif not save_best_only and self._model_path is not None:
            self.save(epoch)
            self._patience_counter += 1
            if self._patience_counter >= patience:
                if restore_best_weights:
                    self.load_state_dict(
                        torch.load(Path(self._model_path, f"cp-{best_epoch:04}.ckpt")))
                return True

        return False


if __name__ == "__main__":
    from tests.settings import *
    import datasets
    from preprocessing.scalers import MinMaxScaler
    from generators.pytorch import TorchGenerator
    reader = datasets.load_data(chunksize=75836,
                                source_path=TEST_DATA_DEMO,
                                storage_path=SEMITEMP_DIR,
                                discretize=True,
                                time_step_size=1.0,
                                start_at_zero=True,
                                impute_strategy='previous',
                                task="IHM")

    reader = datasets.train_test_split(reader, test_size=0.2, val_size=0.1)

    from models.tf2.lstm import LSTMNetwork
    from tests.settings import *
    scaler = MinMaxScaler().fit_reader(reader.train)
    train_generator = TorchGenerator(reader=reader.train, scaler=scaler, batch_size=2, shuffle=True)
    val_generator = TorchGenerator(reader=reader.val, scaler=scaler, batch_size=2, shuffle=True)

    import torch
    import torch.nn as nn
    import torch.optim as optim

    from models.pytorch.lstm import LSTM
    model_path = Path(TEMP_DIR, "torch_lstm")
    model_path.mkdir(parents=True, exist_ok=True)
    model = LSTMNetwork(10,
                        0.2,
                        59,
                        bidirectional=False,
                        recurrent_dropout=0.,
                        task=None,
                        target_repl=False,
                        output_dim=1,
                        depth=2)  #,
    #model_path=model_path)

    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    model.compile(optimizer=optimizer, loss=criterion)
    # Example training loop
    history = model.fit(train_generator=train_generator, val_generator=val_generator, epochs=40)
    print(history)


### FILE: .\src\models\pytorch\mappings.py ###
import torch.optim as optim
import torch.nn as nn
import torchmetrics

__all__ = ["optimizer_mapping", "loss_mapping", "metric_mapping", "activation_mapping"]

optimizer_mapping = {
    "sgd": optim.SGD,
    "adam": optim.Adam,
    "adadelta": optim.Adadelta,
    "adagrad": optim.Adagrad,
    "adamax": optim.Adamax,
    "rmsprop": optim.RMSprop,
    "nadam": optim.NAdam
}

loss_mapping = {
    "binary_crossentropy": nn.BCELoss,
    "logits_binary_crossentropy": nn.BCEWithLogitsLoss,
    "categorical_crossentropy": nn.CrossEntropyLoss,
    "kld": nn.KLDivLoss,
    "mean_squared_error": nn.MSELoss,
    "mean_absolute_error": nn.L1Loss,
    "hinge": nn.HingeEmbeddingLoss,
    "poisson": nn.PoissonNLLLoss,
    "cosine_similarity": nn.CosineEmbeddingLoss,
    "huber": nn.SmoothL1Loss,
}

metric_mapping = {
    "accuracy": torchmetrics.Accuracy,
    "precision": torchmetrics.Precision,
    "recall": torchmetrics.Recall,
    "f1": torchmetrics.F1Score,
    "roc_auc": torchmetrics.AUROC,
    "msl_error": torchmetrics.MeanSquaredLogError,
    "mean_squared_error": torchmetrics.MeanSquaredError,
    "mean_absolute_error": torchmetrics.MeanAbsoluteError,
    "mae": torchmetrics.MeanAbsoluteError,
    "mse": torchmetrics.MeanSquaredError,
    "r2": torchmetrics.R2Score,
    "mape": torchmetrics.MeanAbsolutePercentageError,
    "confusion_matrix": torchmetrics.ConfusionMatrix,
    "cohen_kappa": torchmetrics.CohenKappa,
    "pr_auc": torchmetrics.PrecisionRecallCurve
}

activation_mapping = {"sigmoid": nn.Sigmoid(), "softmax": nn.Softmax(dim=-1), "relu": nn.ReLU()}


### FILE: .\src\models\stream\ensemble.py ###
from river.ensemble import ADWINBaggingClassifier, AdaBoostClassifier, BaggingClassifier, LeveragingBaggingClassifier, SRPClassifier, StackingClassifier, VotingClassifier


### FILE: .\src\models\stream\forest.py ###
from pathlib import Path
from models.stream import AbstractRiverModel
from river.forest import ARFClassifier as _ARFClassifier
from river.forest import AMFClassifier as _AMFClassifier

__all__ = ["ARFClassifier", "AMFClassifier"]


class ARFClassifier(AbstractRiverModel, _ARFClassifier):

    def __init__(self, model_path: Path = None, metrics: list = [], *args, **kwargs):
        self._default_name = "arf_classifier"
        AbstractRiverModel.__init__(self, model_path, metrics)
        _ARFClassifier.__init__(self, *args, **kwargs)


class AMFClassifier(AbstractRiverModel, _AMFClassifier):

    def __init__(self, model_path: Path = None, metrics: list = [], *args, **kwargs):
        self._default_name = "amf_classifier"
        AbstractRiverModel.__init__(self, model_path, metrics)
        _AMFClassifier.__init__(self, *args, **kwargs)


### FILE: .\src\models\stream\linear_model.py ###
from pathlib import Path
from models.stream import AbstractRiverModel, AbstractMultioutputClassifier
from river.linear_model import ALMAClassifier as _ALMAClassifier
from river.linear_model import LogisticRegression as _LogisticRegression
from river.linear_model import LinearRegression as _LinearRegression

__all__ = ["ALMAClassifier", "LogisticRegression"]


class ALMAClassifier(AbstractRiverModel, _ALMAClassifier):

    def __init__(self, model_path: Path = None, metrics: list = [], *args, **kwargs):
        self._default_name = "alma_classifier"
        AbstractRiverModel.__init__(self, model_path, metrics)
        _ALMAClassifier.__init__(self, *args, **kwargs)


class LogisticRegression(AbstractRiverModel, _LogisticRegression):

    def __init__(self, model_path: Path = None, metrics: list = [], *args, **kwargs):
        self._default_name = "log_reg_classifier"
        AbstractRiverModel.__init__(self, model_path, metrics)
        _LogisticRegression.__init__(self, *args, **kwargs)


class LinearRegression(AbstractRiverModel, _LinearRegression):

    def __init__(self, model_path: Path = None, metrics: list = [], *args, **kwargs):
        self._default_name = "log_reg_classifier"
        AbstractRiverModel.__init__(self, model_path, metrics)
        _LinearRegression.__init__(self, *args, **kwargs)


class MultiOutputLogisticRegression(AbstractRiverModel, AbstractMultioutputClassifier):

    def __init__(self, model_path: Path = None, metrics: list = [], *args, **kwargs):
        self._default_name = "log_reg_classifier"
        AbstractRiverModel.__init__(self, model_path, metrics)
        AbstractMultioutputClassifier.__init__(self, _LogisticRegression(*args, **kwargs))


if __name__ == '__main__':

    import datasets
    from river import optim
    from tests.settings import *
    from metrics.stream import MacroROCAUC, MicroROCAUC, PRAUC
    from river.metrics import ROCAUC
    from generators.stream import RiverGenerator
    from preprocessing.scalers import MinMaxScaler
    from preprocessing.imputers import PartialImputer

    ### Try LOS ###
    reader = datasets.load_data(chunksize=75836,
                                source_path=TEST_DATA_DEMO,
                                storage_path=SEMITEMP_DIR,
                                engineer=True,
                                task="LOS")

    imputer = PartialImputer().fit_reader(reader)
    scaler = MinMaxScaler(imputer=imputer).fit_reader(reader)
    # reader = datasets.train_test_split(reader, test_size=0.2, val_size=0.1)
    model = LinearRegression(metrics=["cohen_kappa", "mae"], optimizer=optim.SGD(0.00005))
    generator = RiverGenerator(reader, scaler=scaler)
    model.fit(generator)
    ### Try IHM ###
    reader = datasets.load_data(chunksize=75836,
                                source_path=TEST_DATA_DEMO,
                                storage_path=SEMITEMP_DIR,
                                engineer=True,
                                task="IHM")

    imputer = PartialImputer().fit_reader(reader)
    scaler = MinMaxScaler(imputer=imputer).fit_reader(reader)
    # reader = datasets.train_test_split(reader, test_size=0.2, val_size=0.1)
    model = LogisticRegression(metrics=[PRAUC, ROCAUC])
    generator = RiverGenerator(reader, scaler=scaler)
    model.fit(generator)

    ### Try PHENO ###
    reader = datasets.load_data(chunksize=75836,
                                source_path=TEST_DATA_DEMO,
                                storage_path=SEMITEMP_DIR,
                                engineer=True,
                                task="PHENO")

    imputer = PartialImputer().fit_reader(reader)
    scaler = MinMaxScaler(imputer=imputer).fit_reader(reader)
    # reader = datasets.train_test_split(reader, test_size=0.2, val_size=0.1)
    model = MultiOutputLogisticRegression(metrics=[MacroROCAUC, MicroROCAUC])
    generator = RiverGenerator(reader, scaler=scaler)
    model.fit(generator)
    """
    metric1 = MacroROCAUC()
    metric2 = MicroROCAUC()
    for x, y in generator:
        model.learn_one(x, y)
        y_pred = model.predict_proba_one(x, predict_for=True)
        metric1.update(y, y_pred)
        metric2.update(y, y_pred)
        print(metric1, end="\r")
    print(metric1)
    print(metric2)
    """


### FILE: .\src\models\stream\mappings.py ###
import river.metrics as metrics
from metrics.stream import PRAUC, MacroROCAUC, MicroROCAUC, LOSCohenKappa, LOSClassificationReport

metric_mapping = {
    "accuracy": metrics.Accuracy,
    "precision": metrics.Precision,
    "recall": metrics.Recall,
    "f1": metrics.F1,
    "roc_auc": metrics.ROCAUC(n_thresholds=20),
    "log_loss": metrics.LogLoss,
    "mae": metrics.MAE,
    "mse": metrics.MSE,
    "rmse": metrics.RMSE,
    "r2": metrics.R2,
    "mape": metrics.MAPE,
    "smape": metrics.SMAPE,
    "confusion_matrix": metrics.ConfusionMatrix,
    "classification_report": metrics.ClassificationReport,
    "los_classification_report": LOSClassificationReport,
    "cohen_kappa": LOSCohenKappa(weights="linear"),
    "pr_auc": PRAUC(n_thresholds=20),
    "micro_roc_auc": MicroROCAUC(n_thresholds=20),
    "macro_roc_auc": MacroROCAUC(n_thresholds=20)
}


### FILE: .\src\models\stream\naive_bayes.py ###
from pathlib import Path
from models.stream import AbstractRiverModel
from river.naive_bayes import GaussianNB as _GaussianNB

__all__ = ["GaussianNBClassifier"]


class GaussianNBClassifier(AbstractRiverModel, _GaussianNB):

    def __init__(self, model_path: Path = None, metrics: list = [], *args, **kwargs):
        self._default_name = "gaussian_nb_classifier"
        AbstractRiverModel.__init__(self, model_path, metrics)
        _GaussianNB.__init__(self, *args, **kwargs)


### FILE: .\src\models\stream\neighbors.py ###
from pathlib import Path
from models.stream import AbstractRiverModel
from river.neighbors import KNNClassifier as _KNNClassifier

__all__ = ["KNNClassifier"]


class KNNClassifier(AbstractRiverModel, _KNNClassifier):

    def __init__(self, model_path: Path = None, metrics: list = [], *args, **kwargs):
        self._default_name = "knn_classifier"
        AbstractRiverModel.__init__(self, model_path, metrics)
        _KNNClassifier.__init__(self, *args, **kwargs)


### FILE: .\src\models\stream\tree.py ###
from pathlib import Path
from models.stream import AbstractRiverModel
from river.tree import HoeffdingAdaptiveTreeClassifier as _HoeffdingAdaptiveTreeClassifier
from river.tree import HoeffdingTreeClassifier as _HoeffdingTreeClassifier

__all__ = ["HoeffdingAdaptiveTreeClassifier", "HoeffdingTreeClassifier"]


class HoeffdingAdaptiveTreeClassifier(AbstractRiverModel, _HoeffdingAdaptiveTreeClassifier):

    def __init__(self, model_path: Path = None, metrics: list = [], *args, **kwargs):
        self._default_name = "ha_tree_classifier"
        AbstractRiverModel.__init__(self, model_path, metrics)
        _HoeffdingAdaptiveTreeClassifier.__init__(self, *args, **kwargs)


class HoeffdingTreeClassifier(AbstractRiverModel, _HoeffdingTreeClassifier):

    def __init__(self, model_path: Path = None, metrics: list = [], *args, **kwargs):
        self._default_name = "h_tree_classifier"
        AbstractRiverModel.__init__(self, model_path, metrics)
        _HoeffdingTreeClassifier.__init__(self, *args, **kwargs)


### FILE: .\src\models\stream\__init__.py ###
import os
import collections
import copy
import functools
import pickle
import warnings
from typing import Dict
from river.metrics.base import Metric
from river import base
from river import linear_model
from pathlib import Path
from models.stream.mappings import metric_mapping
from generators.stream import RiverGenerator
from models.trackers import RiverHistory, LocalRiverHistory
from tensorflow.keras.utils import Progbar
from utils import to_snake_case, dict_subset
from abc import ABC
from copy import deepcopy
from settings import *


class AbstractRiverModel(ABC):

    def __init__(self, model_path: Path = None, metrics: list = [], name: str = None):
        self._name = (name if name is not None else self._default_name)
        self._model_path = model_path

        self._history = self._init_history(path=model_path)

        if not self.load():
            self._metrics = self._init_metrics(metrics)
            self._train_metrics = self._init_metrics(metrics)
            self._test_metrics = self._init_metrics(metrics, prefix="test")
            self._val_metrics = self._init_metrics(metrics, prefix="val")

    def _init_history(self, path: Path):
        if self._model_path is not None:
            # Persistent history
            self._model_path.mkdir(parents=True, exist_ok=True)
            return RiverHistory(Path(path, "history"))
        else:
            # Mimics the storable
            return LocalRiverHistory()

    def _init_metrics(self, metrics, prefix: str = None) -> Dict[str, Metric]:
        return_metrics = dict()
        for metric in metrics:
            if isinstance(metric, str):
                metric_name = metric
                metric = metric_mapping[metric]
            else:
                metric_name = to_snake_case(metric.__name__)
            if isinstance(metric, type):
                metric = metric()
            if prefix is not None:
                metric_name = f"{prefix}_{metric_name}"
            return_metrics[metric_name] = metric
        return return_metrics

    def _update_metrics(self, metrics: Dict[str, Metric], x, y_true, y_pred):
        y_label = None

        for _, metric in metrics.items():
            if not hasattr(metric, "requires_labels") or not metric.requires_labels:
                metric.update(y_true, y_pred)
            else:
                if y_label is None:
                    y_label = self.predict_one(x)
                metric.update(y_true, y_label)

    def _update_history(self, history_dict: dict, metrics: Dict[str, Metric]):
        for name, metric in metrics.items():
            try:
                history_dict[name] = metric.get()
            except NotImplementedError as e:
                history_dict[name] = metric

    def _allowed_key(self, key: str):
        return not any([key.endswith(metric) for metric in TEXT_METRICS])

    def fit(self,
            train_generator: RiverGenerator,
            val_generator: RiverGenerator = None,
            model_path: Path = None):
        if model_path is not None:
            self._model_path = model_path
            self._history = self._init_history(path=model_path)
        self.load()
        self.train(generator=train_generator, has_val=val_generator is not None)
        if val_generator is not None:
            self.evaluate(generator=val_generator, is_val=True)

        self.save()
        return self._history.to_json()

    def train(self, generator: RiverGenerator, has_val: bool = False):
        generator_size = len(generator)
        self._train_progbar = Progbar(generator_size)

        for batch_idx, (x, y) in enumerate(generator):
            self.learn_one(x, y)
            if hasattr(self, "predict_proba_one"):
                y_pred = self.predict_proba_one(x)
            else:
                y_pred = self.predict_one(x)
            self._update_metrics(metrics=self._train_metrics, x=x, y_true=y, y_pred=y_pred)
            self._train_progbar.update(batch_idx + 1,
                                       values=self._get_metrics(self._train_metrics),
                                       finalize=(batch_idx == generator_size and not has_val))

        self._update_history(history_dict=self._history.train_metrics, metrics=self._train_metrics)
        return self._train_metrics

    def _get_metrics(self, metrics: Dict[str, Metric]):
        keys = list([metric for metric in metrics.keys() if self._allowed_key(metric)])
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=DeprecationWarning)
            values = [metrics[key].get() for key in keys]
        return tuple(zip(keys, values))

    def predict(self):
        pass

    def test(self, generator: RiverGenerator):
        metrics = self.evaluate(generator=generator, is_test=True)
        self._history.to_json()
        return metrics

    def evaluate(self, generator: RiverGenerator, is_val: bool = False, is_test: bool = False):
        if is_val:
            eval_metric = self._val_metrics
        elif is_test:
            eval_metric = self._test_metrics
        else:
            eval_metric = deepcopy(self._metrics)

        for _, (x, y) in enumerate(generator):
            y_pred = self.predict_one(x)
            self._update_metrics(metrics=eval_metric, x=x, y_true=y, y_pred=y_pred)

        if is_val:
            self._train_progbar.update(self._train_progbar.target,
                                       values=self._get_metrics(self._train_metrics) +
                                       self._get_metrics(self._val_metrics))

            self._update_history(history_dict=self._history.val_metrics, metrics=self._val_metrics)

        if is_test:
            self._update_history(history_dict=self._history.test_metrics,
                                 metrics=self._test_metrics)

        return eval_metric

    def save(self, model_path=None):
        """_summary_
        """
        if model_path is not None:
            storage_path = Path(model_path, f"{self._name}.pkl")
        elif self._model_path is not None and self._name is not None:
            storage_path = Path(self._model_path, f"{self._name}.pkl")
        else:
            storage_path = None
        if storage_path is not None:
            with open(storage_path, "wb") as save_file:
                # Storables can't be pickled
                save_dict = dict_subset(
                    self.__dict__,
                    list(set(self.__dict__.keys()) - set(["_history", "_train_progbar"])))
                pickle.dump(obj=save_dict, file=save_file, protocol=2)

    def load(self):
        """_summary_

        Returns:
            _type_: _description_
        """

        def inner_load(self, path: Path):
            if path.is_file():
                if os.path.getsize(path) > 0:
                    with open(path, "rb") as load_file:
                        load_params = pickle.load(load_file)
                    for key, value in load_params.items():
                        setattr(self, key, value)
                    return 1
            return 0

        if self._model_path is not None and self._name is not None:
            return inner_load(self, Path(self._model_path, f"{self._name}.pkl"))

        return 0


class AbstractMultioutputClassifier(base.Wrapper, base.Classifier):

    def __init__(self, classifier):
        self.classifier = classifier
        new_clf = functools.partial(copy.deepcopy, classifier)
        self.classifiers = collections.defaultdict(new_clf)
        self.classes = set()

    @property
    def _wrapped_model(self):
        return self.classifier

    @property
    def _multiclass(self):
        return True

    @classmethod
    def _unit_test_params(cls):
        yield {"classifier": linear_model.LogisticRegression()}

    def learn_one(self, x, y, **kwargs):
        for label_name, y_label in y.items():
            self.classes.add(label_name)
            self.classifiers[label_name].learn_one(x, y_label)

    def predict_one(self, x, **kwargs):
        predictions = {}
        for label in self.classifiers:
            predictions[label] = self.classifiers[label].predict_one(x)

        return predictions

    def predict_proba_one(self, x, predict_for=None):
        predictions = {}
        for label in self.classifiers:
            predictions[label] = self.classifiers[label].predict_proba_one(x)
            if predict_for is not None:
                predictions[label] = predictions[label][predict_for]

        return predictions


### FILE: .\src\models\tf2\logistic_regression.py ###
#
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras import layers
from tensorflow.keras.optimizers import RMSprop, Adam
from managers import ReducedCheckpointManager


class IncrementalLogReg(object):

    def __init__(self,
                 task: str,
                 input_dim: int = 714,
                 alpha: float = 0.001,
                 max_iter: int = 1000,
                 output_dim: int = None,
                 random_state: int = 42) -> None:

        self.max_iter = max_iter
        # TODO! not realy happy with this solution
        self.alpha = alpha
        activation = {
            "DECOMP": 'sigmoid',
            "IHM": 'sigmoid',
            "LOS": 'softmax',
            "PHENO": 'softmax',
            None: None if output_dim == 1 else 'softmax'
        }

        num_classes = {"DECOMP": 1, "IHM": 1, "LOS": 1, "PHENO": 25, None: output_dim}

        self._num_outputs = num_classes[task]

        input = layers.Input(shape=(input_dim), name='x')
        x = layers.Dense(self._num_outputs, activation=activation[task], input_dim=input_dim)(input)
        self._model = Model(inputs=input, outputs=x)
        self._output_tensor = x

    def load(self, checkpoint_folder):
        manager = ReducedCheckpointManager(checkpoint_folder)

        if not manager.is_empty():
            self._model = manager.load_model()

        return

    def latest_epoch(self, checkpoint_folder):
        manager = ReducedCheckpointManager(checkpoint_folder)
        return manager.latest_epoch()

    def __getattr__(self, name: str):
        """ Surrogate to the _model attributes internals.

        Args:
            name (str): name of the method/attribute

        Returns:
            any: method/attribute of _model
        """
        if name in ["load", "_model", "compile"]:
            return self.__getattribute__(name)
        return getattr(self._model, name)

    def compile(self,
                optimizer="rmsprop",
                metrics=None,
                loss_weights=None,
                weighted_metrics=None,
                run_eagerly=None,
                steps_per_execution=None,
                jit_compile=None,
                **kwargs):
        """_summary_

        Args:
            optimizer (str, optional): _description_. Defaults to "rmsprop".
            metrics (_type_, optional): _description_. Defaults to None.
            loss_weights (_type_, optional): _description_. Defaults to None.
            weighted_metrics (_type_, optional): _description_. Defaults to None.
            run_eagerly (_type_, optional): _description_. Defaults to None.
            steps_per_execution (_type_, optional): _description_. Defaults to None.
            jit_compile (_type_, optional): _description_. Defaults to None.

        Returns:
            _type_: _description_
        """
        if isinstance(optimizer, str):
            optimizer_switch = {"rmsprop": RMSprop(self.alpha), "adam": Adam(self.alpha)}
            optimizer = optimizer_switch[optimizer]
        else:
            optimizer.learning_rate = self.alpha
        if self._num_outputs == 1:
            loss = "binary_crossentropy"
        else:
            loss = "categorical_crossentropy"
        return self._model.compile(optimizer=optimizer,
                                   loss=loss,
                                   metrics=metrics,
                                   loss_weights=loss_weights,
                                   weighted_metrics=weighted_metrics,
                                   run_eagerly=run_eagerly,
                                   steps_per_execution=steps_per_execution,
                                   **kwargs)


### FILE: .\src\models\tf2\lstm.py ###
import pdb
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras import layers
from tensorflow.keras.layers import Bidirectional


class LSTMNetwork(Model):
    """
    """

    def __init__(self,
                 layer_size,
                 dropout,
                 input_dim,
                 bidirectional=False,
                 recurrent_dropout=0.,
                 task=None,
                 target_repl=False,
                 output_dim=1,
                 depth=1):
        """
        """
        self.layer_size = layer_size
        self.dropout_rate = dropout
        self.recurrent_dropout = recurrent_dropout
        self.depth = depth

        final_activation = {
            "DECOMP": 'sigmoid',
            "IHM": 'sigmoid',
            "LOS": 'softmax',
            "PHENO": 'softmax',
            None: None if output_dim == 1 else 'softmax'
        }

        num_classes = {"DECOMP": 1, "IHM": 1, "LOS": 10, "PHENO": 25, None: output_dim}

        # Input layers and masking
        input = layers.Input(shape=(None, input_dim), name='x')

        x = layers.Masking()(input)

        # TODO: compare bidirectional runs to one directiona

        if type(layer_size) == int:
            iterator = [layer_size] * (depth - 1)
        else:
            iterator = layer_size[:-1]
            layer_size = layer_size[-1]

        for i, size in enumerate(iterator):
            if bidirectional:
                num_units = size // 2
                x = Bidirectional(
                    layers.LSTM(units=num_units,
                                activation='tanh',
                                return_sequences=True,
                                recurrent_dropout=recurrent_dropout,
                                dropout=dropout,
                                name=f"lstm_hidden_{i}"))(x)
            else:
                x = layers.LSTM(units=size,
                                activation='tanh',
                                return_sequences=True,
                                recurrent_dropout=recurrent_dropout,
                                dropout=dropout,
                                name=f"lstm_hidden_{i}")(x)

        # Output module of the network
        return_sequences = target_repl
        '''        
        x = Bidirectional(layers.LSTM(units=layer_size,
                                      activation='tanh',
                                      return_sequences=return_sequences,
                                      dropout=dropout_rate,
                                      recurrent_dropout=recurrent_dropout,
                                      name=f"lstm_hidden_{depth}"))(x)
        '''
        x = layers.LSTM(units=layer_size,
                        activation='tanh',
                        return_sequences=return_sequences,
                        dropout=dropout,
                        recurrent_dropout=recurrent_dropout)(x)

        x = layers.Dense(num_classes[task], activation=final_activation[task])(x)

        super(LSTMNetwork, self).__init__(inputs=[input], outputs=[x])


### FILE: .\src\models\tf2\transformer.py ###
"""https://keras.io/examples/timeseries/timeseries_transformer_classification/"""

from tensorflow import keras
from tensorflow.keras import layers


def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # Normalization and Attention
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = layers.MultiHeadAttention(key_dim=head_size,
                                  num_heads=num_heads,
                                  dropout=dropout)(x, x)
    x = layers.Dropout(dropout)(x)
    res = x + inputs

    # Feed Forward Part
    x = layers.LayerNormalization(epsilon=1e-6)(res)
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    return x + res


def Transformer(input_shape,
                head_size,
                num_heads,
                ff_dim,
                num_transformer_blocks,
                mlp_units,
                dropout=0,
                mlp_dropout=0,
                n_classes=2):
    inputs = keras.Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)

    x = layers.GlobalAveragePooling1D(data_format="channels_first")(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation="relu")(x)
        x = layers.Dropout(mlp_dropout)(x)
    outputs = layers.Dense(n_classes, activation="softmax")(x)
    return keras.Model(inputs, outputs)


### FILE: .\src\models\tf2\__init__.py ###


### FILE: .\src\pipelines\pytorch.py ###
import datasets
from pathlib import Path
from generators.pytorch import TorchGenerator
from preprocessing.scalers import AbstractScaler
from utils.IO import *
from datasets.readers import ProcessedSetReader
from pipelines import AbstractPipeline


class TorchPipeline(AbstractPipeline):

    def _create_generator(self, reader: ProcessedSetReader, scaler: AbstractScaler,
                          **generator_options):
        return TorchGenerator(reader=reader, scaler=scaler, **generator_options)

    def fit(self,
            epochs: int,
            no_subdirs: bool = False,
            result_name: str = None,
            patience: int = None,
            save_best_only: bool = True,
            restore_best_weights: bool = True,
            sample_weights: dict = None,
            val_frequency=1,
            restore_last_run: bool = False):

        self._init_result_path(result_name=result_name,
                               restore_last_run=restore_last_run,
                               no_subdirs=no_subdirs)

        info_io(f"Training model in directory\n{self._result_path}")
        self._model.fit(self._train_generator,
                        model_path=self._result_path,
                        epochs=epochs,
                        patience=patience,
                        save_best_only=save_best_only,
                        restore_best_weights=restore_best_weights,
                        sample_weights=sample_weights,
                        val_frequency=val_frequency,
                        val_generator=self._val_generator)

        return self._model


if __name__ == "__main__":
    from models.pytorch.lstm import LSTMNetwork
    from tests.settings import *
    reader = datasets.load_data(chunksize=75836,
                                source_path=TEST_DATA_DEMO,
                                storage_path=SEMITEMP_DIR,
                                discretize=True,
                                time_step_size=1.0,
                                start_at_zero=True,
                                impute_strategy='previous',
                                task="IHM")

    reader = datasets.train_test_split(reader, test_size=0.2, val_size=0.1)

    model = LSTMNetwork(10,
                        0.2,
                        59,
                        bidirectional=False,
                        recurrent_dropout=0.,
                        task=None,
                        target_repl=False,
                        output_dim=1,
                        depth=1)

    pipe = TorchPipeline(storage_path=Path(TEMP_DIR, "torch_pipeline"),
                         reader=reader,
                         model=model,
                         compile_options={
                             "optimizer": "adam",
                             "loss": "logits_binary_crossentropy"
                         },
                         generator_options={
                             "batch_size": 1,
                             "shuffle": True
                         }).fit(epochs=10, save_best_only=False)


### FILE: .\src\pipelines\stream.py ###
import datasets
from typing import Union
from pathlib import Path
from generators.stream import RiverGenerator
from preprocessing.scalers import AbstractScaler
from preprocessing.imputers import AbstractImputer, PartialImputer
from datasets.readers import ProcessedSetReader, SplitSetReader
from pipelines import AbstractPipeline
from utils.IO import *
from settings import *


class RiverPipeline(AbstractPipeline):

    def __init__(self,
                 storage_path: Path,
                 reader: Union[ProcessedSetReader, SplitSetReader],
                 model,
                 generator_options: dict = {},
                 model_options: dict = {},
                 scaler_options: dict = {},
                 split_options: dict = {},
                 imputer_options: dict = {},
                 scaler: AbstractScaler = None,
                 scaler_type: str = "minmax",
                 imputer: AbstractImputer = None):

        self._reader = self._split_data(data_split_options=split_options, reader=reader)
        self._imputer = self._init_imputer(storage_path=storage_path,
                                           imputer=imputer,
                                           imputer_options=imputer_options,
                                           reader=reader)

        scaler_options["imputer"] = self._imputer

        super().__init__(storage_path=storage_path,
                         reader=reader,
                         model=model,
                         generator_options=generator_options,
                         model_options=model_options,
                         scaler_options=scaler_options,
                         compile_options={},
                         split_options=split_options,
                         scaler=scaler,
                         scaler_type=scaler_type)

    def _init_imputer(self, storage_path: Path, imputer_options: dict, imputer: AbstractImputer,
                      reader: Union[ProcessedSetReader, SplitSetReader]):
        if isinstance(reader, SplitSetReader):
            reader = reader.train
        if imputer is not None and isinstance(imputer, AbstractImputer):
            return imputer.fit_reader(reader)
        elif imputer is not None and imputer(imputer, type):
            imputer = imputer(storage_path=storage_path, **imputer_options)
            return imputer.fit_reader(reader)
        elif imputer is None:
            imputer = PartialImputer(storage_path=storage_path, **imputer_options)
            return imputer.fit_reader(reader)

    def _create_generator(self, reader: ProcessedSetReader, scaler: AbstractScaler,
                          **generator_options):
        return RiverGenerator(reader=reader, scaler=scaler, **generator_options)

    def fit(self,
            result_name: str = None,
            no_subdirs: bool = False,
            restore_last_run: bool = False,
            **kwargs):

        self._init_result_path(result_name=result_name,
                               restore_last_run=restore_last_run,
                               no_subdirs=no_subdirs)

        info_io(f"Training model in directory\n{self._result_path}")
        self._model.fit(self._train_generator,
                        model_path=self._result_path,
                        val_generator=self._val_generator,
                        **kwargs)

        return self._model

    def _allowed_key(self, key: str):
        return not any([key.endswith(metric) for metric in TEXT_METRICS])

    def print_metrics(self, metrics: dict):
        stringified_dict = {k: str(v) for k, v in metrics.items()}

        # Find the maximum length of keys and values
        max_key_length = max(len(key) for key in stringified_dict.keys()) + 4
        max_value_length = max(
            len(value) for key, value in stringified_dict.items() if self._allowed_key(key)) + 4

        # Print the header with calculated padding
        msg = ""
        msg += f"{'Key':<{max_key_length}} {'Value':<{max_value_length}}\n"
        msg += '-' * (max_key_length + max_value_length + 1) + "\n"

        # Print the dictionary items with calculated padding
        non_numeric_values = None
        for key, value in stringified_dict.items():
            if self._allowed_key(key):
                msg += f"{key:<{max_key_length}} {value:<{max_value_length}}" + "\n"
            else:
                non_numeric_values = (key, value)
        if non_numeric_values is not None:
            msg += "\n" + "\n".join(non_numeric_values)

        info_io(msg)

    def test(self, **kwargs):

        info_io(f"Testing model in directory\n{self._result_path}")
        metrics = self._model.test(self._test_generator, **kwargs)
        self.print_metrics(metrics)

        return metrics


if __name__ == '__main__':
    import datasets
    from tests.settings import *
    # from models.stream.linear_model import LogisticRegression
    from metrics.stream import PRAUC

    reader = datasets.load_data(chunksize=75836,
                                source_path=TEST_DATA_DEMO,
                                storage_path=SEMITEMP_DIR,
                                engineer=True,
                                task="IHM")

    model_path = Path(TEMP_DIR, "arf_ihm")
    reader = datasets.train_test_split(reader, test_size=0.2, val_size=0.1)
    # model = LogisticRegression(model_path=model_path, metrics=["accuracy", PRAUC])
    pipe = RiverPipeline(storage_path=Path(TEMP_DIR, "river_pipeline"),
                         reader=reader,
                         model=model,
                         generator_options={
                             "shuffle": True
                         }).fit()


### FILE: .\src\pipelines\tf2.py ###
import datasets
import re
from pathlib import Path
from models.callbacks import HistoryCheckpoint
from generators.tf2 import TFGenerator
from preprocessing.scalers import AbstractScaler
from managers import HistoryManager
from managers import CheckpointManager
from datasets.readers import ProcessedSetReader
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from utils.IO import *
from pipelines import AbstractPipeline


class TFPipeline(AbstractPipeline):

    def _create_generator(self, reader: ProcessedSetReader, scaler: AbstractScaler,
                          **generator_options):
        return TFGenerator(reader=reader, scaler=scaler, **generator_options)

    def _init_callbacks(self,
                        patience: int = None,
                        restore_best_weights=True,
                        save_weights_only: bool = False,
                        save_best_only: bool = True):
        self._standard_callbacks = []
        if "val" in self._split_names:
            if patience is not None:
                es_callback = EarlyStopping(patience=patience,
                                            restore_best_weights=restore_best_weights)
                self._standard_callbacks.append(es_callback)

        if self._result_path is not None:
            cp_pattern = str(Path(self._result_path, "cp-{epoch:04d}.ckpt"))
            cp_callback = ModelCheckpoint(filepath=cp_pattern,
                                          save_weights_only=save_weights_only,
                                          save_best_only=save_best_only,
                                          verbose=0)
            self._standard_callbacks.append(cp_callback)

            hist_callback = HistoryCheckpoint(self._result_path)
            self._standard_callbacks.append(hist_callback)

    def _init_managers(self, epochs: int):
        self._hist_manager = HistoryManager(str(self._result_path))

        self._manager = CheckpointManager(str(self._result_path), epochs, custom_objects={})

    def fit(self,
            epochs: int,
            no_subdirs: bool = False,
            result_name: str = None,
            patience: int = None,
            restore_best_weights=True,
            save_weights_only: bool = False,
            class_weight: dict = None,
            sample_weight: dict = None,
            save_best_only: bool = True,
            callbacks: list = [],
            validation_freq: int = 1,
            restore_last_run: bool = False):

        self._init_result_path(result_name=result_name,
                               restore_last_run=restore_last_run,
                               no_subdirs=no_subdirs)
        info_io(f"Training model in directory\n{self._result_path}")
        self._init_managers(epochs)
        self._init_callbacks(patience=patience,
                             restore_best_weights=restore_best_weights,
                             save_weights_only=save_weights_only,
                             save_best_only=save_best_only)

        self._model.fit(self._train_generator,
                        validation_data=self._val_generator,
                        epochs=epochs,
                        steps_per_epoch=self._train_generator.steps,
                        callbacks=callbacks + self._standard_callbacks,
                        class_weight=class_weight,
                        sample_weight=sample_weight,
                        initial_epoch=self._manager.latest_epoch(),
                        validation_steps=self._val_steps,
                        validation_freq=validation_freq)

        self._hist_manager.finished()
        _, best_epoch = self._hist_manager.best
        self._manager.clean_directory(best_epoch)

        return self._model


if __name__ == "__main__":
    from models.tf2.lstm import LSTMNetwork
    from tests.settings import *
    reader = datasets.load_data(chunksize=75836,
                                source_path=TEST_DATA_DEMO,
                                storage_path=SEMITEMP_DIR,
                                discretize=True,
                                time_step_size=1.0,
                                start_at_zero=True,
                                impute_strategy='previous',
                                task="IHM")

    reader = datasets.train_test_split(reader, test_size=0.2, val_size=0.1)

    model = LSTMNetwork(10,
                        0.2,
                        59,
                        bidirectional=False,
                        recurrent_dropout=0.,
                        task=None,
                        target_repl=False,
                        output_dim=1,
                        depth=1)

    pipe = TFPipeline(storage_path=Path(TEMP_DIR, "tf_pipeline"),
                      reader=reader,
                      model=model,
                      compile_options={
                          "optimizer": "adam",
                          "loss": "binary_crossentropy"
                      },
                      generator_options={
                          "batch_size": 1,
                          "shuffle": True
                      }).fit(epochs=10, save_best_only=False)

    pipe = TFPipeline(storage_path=Path(TEMP_DIR, "tf_pipeline"),
                      reader=reader,
                      model=model,
                      compile_options={
                          "optimizer": "adam",
                          "loss": "binary_crossentropy"
                      },
                      generator_options={
                          "batch_size": 1,
                          "shuffle": True
                      }).fit(epochs=10, save_best_only=False)

    pipe = TFPipeline(storage_path=Path(TEMP_DIR, "tf_pipeline"),
                      reader=reader,
                      model=model,
                      compile_options={
                          "optimizer": "adam",
                          "loss": "binary_crossentropy"
                      },
                      generator_options={
                          "batch_size": 1,
                          "shuffle": True
                      }).fit(epochs=20, save_best_only=False, restore_last_run=True)


### FILE: .\src\pipelines\__init__.py ###
import re
import datasets
from typing import Union
from pathlib import Path
from generators.tf2 import TFGenerator
from preprocessing.scalers import AbstractScaler, MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler
from datasets.readers import ProcessedSetReader
from datasets.readers import ProcessedSetReader, SplitSetReader
from abc import ABC, abstractmethod
from utils.IO import *


class AbstractPipeline(ABC):
    """
    Abstract base class for pipelines used in data processing and model training.

    Parameters
    ----------
    storage_path : Path
        Path to the directory where results and intermediate files will be stored.
    reader : ProcessedSetReader or SplitSetReader
        Reader object to read the processed data.
    model : object
        The model to be trained.
    generator_options : dict, optional
        Options for the data generator.
    model_options : dict, optional
        Options for the model.
    scaler_options : dict, optional
        Options for the scaler.
    compile_options : dict, optional
        Options for model compilation.
    split_options : dict, optional
        Options for data splitting.
    scaler : AbstractScaler, optional
        Scaler object to scale the data.
    scaler_type : str, optional
        Type of scaler to use ('minmax', 'standard', 'maxabs', 'robust').

    Attributes
    ----------
    _storage_path : Path
        Path to the directory where results and intermediate files will be stored.
    _model : object
        The model to be trained.
    _generator_options : dict
        Options for the data generator.
    _data_split_options : dict
        Options for data splitting.
    _split_names : list
        Names of data splits.
    _reader : Union[ProcessedSetReader, SplitSetReader]
        Reader object to read the processed data.
    _scaler : AbstractScaler
        Scaler object to scale the data.
    _train_generator : TFGenerator
        Data generator for the training set.
    _val_generator : TFGenerator
        Data generator for the validation set.
    _val_steps : int
        Number of steps in the validation set.
    _test_generator : TFGenerator
        Data generator for the test set.
    _result_path : Path
        Path to the directory where results will be stored.
    """

    def __init__(
        self,
        storage_path: Path,
        reader: Union[ProcessedSetReader, SplitSetReader],
        model,
        generator_options: dict = {},
        model_options: dict = {},
        scaler_options: dict = {},
        compile_options: dict = {},
        split_options: dict = {},
        scaler: AbstractScaler = None,
        scaler_type: str = "minmax",
    ):
        self._storage_path = storage_path
        self._storage_path.mkdir(parents=True, exist_ok=True)
        self._model = model
        self._generator_options = generator_options
        self._data_split_options = split_options
        self._split_names = ["train"]
        self._reader = self._split_data(data_split_options=split_options, reader=reader)

        self._scaler = self._init_scaler(storage_path=storage_path,
                                         scaler_type=scaler_type,
                                         scaler_options=scaler_options,
                                         scaler=scaler,
                                         reader=reader)

        self._init_generators(generator_options=generator_options,
                              scaler=self._scaler,
                              reader=reader)

        self._init_model(model=model, model_options=model_options, compiler_options=compile_options)

    def _init_model(self, model, model_options, compiler_options):
        """
        Initializes the model.
        """
        if isinstance(model, type):
            self._model = model(**model_options)
        if hasattr(model, "optimizer") and model.optimizer is None:
            self._model.compile(**compiler_options)

    def _split_data(self, data_split_options: dict, reader: Union[ProcessedSetReader,
                                                                  SplitSetReader]):
        """
        Splits the data according to the provided options.
        """
        if isinstance(reader, ProcessedSetReader) and data_split_options:
            return datasets.train_test_split(reader, **data_split_options)
        return reader

    @abstractmethod
    def _create_generator(self, reader: ProcessedSetReader, scaler: AbstractScaler,
                          **generator_options):
        """
        Creates a data generator.
        """
        ...

    def _init_scaler(self, storage_path: Path, scaler_type: str, scaler_options: dict,
                     scaler: Union[AbstractScaler, type], reader: Union[ProcessedSetReader,
                                                                        SplitSetReader]):
        """
        Initializes the scaler.
        """
        if isinstance(reader, SplitSetReader):
            reader = reader.train
        if not scaler_type in ["minmax", "standard", "maxabs", "robust"]:
            raise ValueError(
                f"Invalid scaler type: {scaler_type}. Must be either 'minmax' or 'standard'.")

        if scaler is not None and isinstance(scaler, AbstractScaler):
            return scaler.fit_reader(reader)
        elif scaler is not None and isinstance(scaler, type):
            scaler = scaler(storage_path=storage_path, **scaler_options)
            return scaler.fit_reader(reader)
        elif scaler_type == "minmax":
            scaler = MinMaxScaler(storage_path=storage_path, **scaler_options)
            return scaler.fit_reader(reader)
        elif scaler_type == "standard":
            scaler = StandardScaler(storage_path=storage_path, **scaler_options)
            return scaler.fit_reader(reader)
        elif scaler_type == "maxabs":
            scaler = MaxAbsScaler(storage_path=storage_path, **scaler_options)
            return scaler.fit_reader(reader)
        elif scaler_type == "robust":
            scaler = RobustScaler(storage_path=storage_path, **scaler_options)
            return scaler.fit_reader(reader)

    @staticmethod
    def _check_generator_sanity(set_name: str, reader: ProcessedSetReader, generator: TFGenerator,
                                generator_options: dict):
        """
        Checks wether the generator is empty.
        """
        if not len(generator):
            if reader.subject_ids:
                msg = f"{set_name.capitalize()} generator has no steps, while {len(reader.subject_ids)}"
                msg += f" subjects are present in reader. "
                if "batch_size" in generator_options:
                    msg += f"Consider reducing batch size: {generator_options['batch_size']}."

                raise ValueError(msg)
            else:
                raise ValueError(
                    f"{set_name.capitalize()} reader has no subjects. Consider adjusting the {set_name} split size."
                )

    def _init_generators(self, generator_options: dict, scaler: AbstractScaler,
                         reader: Union[ProcessedSetReader, SplitSetReader]):
        """
        Initializes the test, val and train data generators if test, val and train are part of
        the split.
        """
        if isinstance(reader, ProcessedSetReader):
            self._train_generator = self._create_generator(reader=reader,
                                                           scaler=scaler,
                                                           **generator_options)

            self._val_generator = None
            self._val_steps = None
        elif isinstance(reader, SplitSetReader):
            self._train_generator = self._create_generator(reader=reader.train,
                                                           scaler=scaler,
                                                           **generator_options)

            self._check_generator_sanity(set_name="train",
                                         generator_options=generator_options,
                                         reader=reader.train,
                                         generator=self._train_generator)

            if "val" in reader.split_names:
                self._split_names.append("val")
                self._val_generator = self._create_generator(reader=reader.val,
                                                             scaler=scaler,
                                                             **generator_options)

                self._val_steps = len(self._val_generator)
                self._check_generator_sanity(set_name="val",
                                             generator_options=generator_options,
                                             reader=reader.val,
                                             generator=self._val_generator)

            else:
                self._val_generator = None
                self._val_steps = None

            if "test" in reader.split_names:
                self._split_names.append("test")
                self._test_generator = self._create_generator(reader=reader.test,
                                                              scaler=scaler,
                                                              **generator_options)

                self._check_generator_sanity(set_name="test",
                                             generator_options=generator_options,
                                             reader=reader.test,
                                             generator=self._test_generator)

    def _init_result_path(self,
                          result_name: str,
                          restore_last_run: bool = False,
                          no_subdirs: bool = False):
        """
        Initializes the result path. If restore last run is set to True, the result path will be the
        the previous numerical result path. If no_subdirs is set to True, the result path will be the
        the storage path. If a result name is provided, the result path will be the storage path with
        the result name.
        """
        if no_subdirs:
            if result_name is not None:
                warn_io("Ignoring result_name, as no_subdirs is set to True.")
            self._result_path = self._storage_path
        elif result_name is not None:
            self._result_path = Path(self._storage_path, result_name)
        else:
            # Iterate over files in the directory
            pattern = re.compile(r"(\d+)")
            result_numbers = []
            for file in self._storage_path.iterdir():
                if file.is_dir() and file.name.startswith("results"):
                    match = pattern.search(file.name)
                    if match:
                        result_numbers.append(int(match.group(0)))

            # Determine the next number
            if not result_numbers:
                next_number = 0
            else:
                next_number = max(result_numbers, default=0) + int(not restore_last_run)
            self._result_path = Path(self._storage_path, f"results{next_number:04d}")
        self._result_path.mkdir(parents=True, exist_ok=True)

    @abstractmethod
    def fit(self, epochs: int, result_name: str = None, no_subdirs: bool = False, *args, **kwargs):
        """
        Fits the model to the data.
        """
        ...

    # def test(self):
    #     self._model.test(self._test_generator)


### FILE: .\src\preprocessing\imputers.py ###
import time
import pickle
import os
import numpy as np
import pandas as pd
import warnings
from pathlib import Path
from sklearn.impute import SimpleImputer
from utils.IO import *
from utils import is_allnan
from preprocessing import AbstractScikitProcessor as AbstractImputer


class PartialImputer(SimpleImputer, AbstractImputer):

    def __init__(self,
                 missing_values=np.nan,
                 strategy='mean',
                 verbose=1,
                 copy=True,
                 fill_value=0,
                 add_indicator=False,
                 keep_empty_features=True,
                 storage_path=None):
        """
        An imputer for handling missing values in a dataset with the capability of partial fitting.

        This class extends SimpleImputer from sklearn and AbstractScikitProcessor to handle missing
        values using various strategies and allows for incremental fitting.

        Parameters
        ----------
        missing_values : any, optional
            The placeholder for the missing values. All occurrences of `missing_values` will be
            imputed. Default is np.nan.
        strategy : str, optional
            The imputation strategy. Default is 'mean'.
        verbose : int, optional
            The verbosity level. Default is 1.
        copy : bool, optional
            If True, a copy of X will be created. If False, imputation will be done in-place whenever possible.
            Default is True.
        fill_value : any, optional
            When strategy="constant", `fill_value` is used to replace all occurrences of missing_values.
            Default is 0.
        add_indicator : bool, optional
            If True, a MissingIndicator transform will stack onto the output of the imputerâs transform.
            Default is False.
        keep_empty_features : bool, optional
            If True, features that are all-NaN will be kept in the resulting dataset.
            Default is True.
        storage_path : Path or str, optional
            The path where the imputer's state will be stored. Default is None.
        """
        self._verbose = verbose
        self.statistics_ = None
        self.n_features_in_ = 0
        self.n_samples_in_ = 0
        self._storage_name = "partial_imputer.pkl"
        if storage_path is not None:
            self._storage_path = Path(storage_path, self._storage_name)
            self._storage_path.parent.mkdir(parents=True, exist_ok=True)
        else:
            self._storage_path = None
        super().__init__(missing_values=missing_values,
                         strategy=strategy,
                         copy=copy,
                         fill_value=fill_value,
                         add_indicator=add_indicator,
                         keep_empty_features=keep_empty_features)

    def partial_fit(self, X):
        """
        Incrementally fit the imputer on a batch of data.

        This method allows the imputer to be fitted in increments, which is useful for large datasets
        that do not fit into memory.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            The input data to fit.

        Returns
        -------
        self : object
            Returns self.
        """
        warnings.filterwarnings("ignore")
        n = len(X)
        if is_allnan(X):
            return self
        avg = np.nanmean(X, axis=0)

        if self.statistics_ is None:
            self.fit(X)
            self.n_samples_in_ = n
        else:
            self.statistics_ = np.nanmean([self.n_samples_in_ * self.statistics_, n * avg],
                                          axis=0) / (self.n_samples_in_ + n)
        self.n_samples_in_ += n
        warnings.filterwarnings("default")
        return self

    @classmethod
    def _get_param_names(cls):
        """Necessary for parent class.
        """
        return []


### FILE: .\src\preprocessing\scalers.py ###
import numpy as np
import pandas as pd
from typing import Union
from sklearn.preprocessing import MinMaxScaler as _MinMaxScaler
from sklearn.preprocessing import StandardScaler as _StandardScaler
from sklearn.preprocessing import RobustScaler as _RobustScaler
from sklearn.preprocessing import MaxAbsScaler as _MaxAbsScaler
from utils.IO import *
from pathlib import Path
from preprocessing import AbstractScikitProcessor

__all__ = [
    "AbstractScaler", "StandardScaler", "MinMaxScaler", "MaxAbsScaler",
    "RobustScaler"
]


class AbstractScaler(AbstractScikitProcessor):
    """
    Base class for all scalers in the MIMIC-III preprocessing pipeline.

    This class handles initialization of storage paths and optional imputers, along with
    verbosity settings.

    Parameters
    ----------
    storage_path : str or Path, optional
        The path where the scaler's state will be stored, by default None.
    imputer : object, optional
        The imputer to use for handling missing values, by default None. Called
        before fitting or transforming on each batch if passed.
    verbose : bool, optional
        If True, print verbose logs during processing, by default True.
    """
    def __init__(self, storage_path=None, imputer=None, verbose=True):
        if storage_path is not None:
            self._storage_path = Path(storage_path, self._storage_name)
            self._storage_path.parent.mkdir(parents=True, exist_ok=True)
        else:
            self._storage_path = None
        self._verbose = verbose
        self._imputer = imputer


class StandardScaler(_StandardScaler, AbstractScaler):
    """
    """

    def __init__(self,
                 storage_path=None,
                 imputer=None,
                 copy=True,
                 with_mean=True,
                 with_std=True,
                 verbose=True):
        """
        Standard Scaler for the MIMIC-III dataset.

        This scaler standardizes features by removing the mean and scaling to unit variance.

        Parameters
        ----------
        storage_path : str or Path, optional
            The path where the scaler's state will be stored, by default None.
        imputer : object, optional
            The imputer to use for handling missing values, by default None.
        copy : bool, optional
            If True, a copy of X will be created, by default True.
        with_mean : bool, optional
            If True, center the data before scaling, by default True.
        with_std : bool, optional
            If True, scale the data to unit variance, by default True.
        verbose : bool, optional
            If True, print verbose logs during processing, by default True.
        """
        self._storage_name = "standard_scaler.pkl"
        AbstractScaler.__init__(self, storage_path=storage_path, imputer=imputer, verbose=verbose)
        _StandardScaler.__init__(self, copy=copy, with_mean=with_mean, with_std=with_std)

    @classmethod
    def _get_param_names(cls):
        """
        Necessary for scikit-learn compatibility.
        """
        return []

    def transform(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
        """
        Scale the data.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            The data to be scaled.

        Returns
        -------
        np.ndarray
            The scaled data.
        """
        if hasattr(self, "_imputer") and self._imputer is not None:
            X = self._imputer.transform(X)
        return super().transform(X)

    def fit(self,
            X: Union[np.ndarray, pd.DataFrame],
            y: Union[np.ndarray, pd.DataFrame] = None,
            **fit_params):
        """
        Compute the mean and std to be used for later scaling.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            The data to compute the mean and std on.
        y : None
            Ignored.
        **fit_params : dict
            Additional fit parameters.

        Returns
        -------
        self : object
            Fitted scaler.
        """
        if hasattr(self, "_imputer") and self._imputer is not None:
            return self._imputer.transform(X)
        return super().fit(X, y, **fit_params)

    def fit_transform(self,
                      X: Union[np.ndarray, pd.DataFrame],
                      y: Union[np.ndarray, pd.DataFrame] = None,
                      **fit_params) -> np.ndarray:
        """
        Fit to data, then transform it.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            The data to fit and transform.
        y : None
            Ignored.
        **fit_params : dict
            Additional fit parameters.

        Returns
        -------
        np.ndarray
            The transformed data.
        """
        if hasattr(self, "_imputer") and self._imputer is not None:
            return self._imputer.transform(X)
        return super().fit_transform(X, y, **fit_params)


class MinMaxScaler(_MinMaxScaler, AbstractScaler):
    """
    Min-Max Scaler for the MIMIC-III dataset.

    This scaler transforms features by scaling each feature to a given range.

    Parameters
    ----------
    storage_path : str or Path, optional
        The path where the scaler's state will be stored, by default None.
    imputer : object, optional
        The imputer to use for handling missing values, by default None.
    verbose : bool, optional
        If True, print verbose logs during processing, by default True.
    feature_range : tuple (min, max), optional
        Desired range of transformed data, by default (0, 1).
    copy : bool, optional
        If True, a copy of X will be created, by default True.
    clip : bool, optional
        Set to True to clip transformed values of held-out data to provided feature range, by default False.
    """

    def __init__(self,
                 storage_path=None,
                 imputer=None,
                 verbose=True,
                 feature_range=(0, 1),
                 copy=True,
                 clip=False):
        """_summary_

        Args:
            storage_path (_type_, optional): _description_. Defaults to None.
            verbose (int, optional): _description_. Defaults to 1.
        """
        self._storage_name = "minmax_scaler.pkl"
        AbstractScaler.__init__(self, storage_path=storage_path, imputer=imputer, verbose=verbose)
        _MinMaxScaler.__init__(self, feature_range=feature_range, copy=copy, clip=clip)

    @classmethod
    def _get_param_names(cls):
        """
        Necessary for scikit-learn compatibility.
        """
        return []

    def transform(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
        """
        Scale the data.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            The data to be scaled.

        Returns
        -------
        np.ndarray
            The scaled data.
        """
        if hasattr(self, "_imputer") and self._imputer is not None:
            X = self._imputer.transform(X)
        return super().transform(X)

    def fit(self,
            X: Union[np.ndarray, pd.DataFrame],
            y: Union[np.ndarray, pd.DataFrame] = None,
            **fit_params):
        """
        Compute the volumn wise minimum and maximum to be used for later scaling.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            The data to compute the minimum and maximum on.
        y : None
            Ignored.
        **fit_params : dict
            Additional fit parameters.

        Returns
        -------
        self : object
            Fitted scaler.
        """
        if hasattr(self, "_imputer") and self._imputer is not None:
            return self._imputer.transform(X)
        return super().fit(X, y, **fit_params)

    def fit_transform(self,
                      X: Union[np.ndarray, pd.DataFrame],
                      y: Union[np.ndarray, pd.DataFrame] = None,
                      **fit_params) -> np.ndarray:
        """
        Fit to data, then transform it.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            The data to fit and transform.
        y : None
            Ignored.
        **fit_params : dict
            Additional fit parameters.

        Returns
        -------
        np.ndarray
            The transformed data.
        """
        if hasattr(self, "_imputer") and self._imputer is not None:
            return self._imputer.transform(X)
        return super().fit_transform(X, y, **fit_params)


class MaxAbsScaler(_MaxAbsScaler, AbstractScaler):
    """
    Max-Abs Scaler for the MIMIC-III dataset.

    This scaler scales each feature by its maximum absolute value, preserving the sparsity of the data.

    Parameters
    ----------
    storage_path : str or Path, optional
        The path where the scaler's state will be stored, by default None.
    imputer : object, optional
        The imputer to use for handling missing values, by default None.
    verbose : bool, optional
        If True, print verbose logs during processing, by default True.
    copy : bool, optional
        If True, a copy of X will be created, by default True.
    """

    def __init__(self, storage_path=None, imputer=None, verbose=True, copy=True):
        self._storage_name = "maxabs_scaler.pkl"
        AbstractScaler.__init__(self, storage_path=storage_path, imputer=imputer, verbose=verbose)
        _MaxAbsScaler().__init__(copy=copy)

    @classmethod
    def _get_param_names(cls):
        """
        Necessary for scikit-learn compatibility.
        """
        return []

    def transform(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
        """
        Scale the data.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            The data to be scaled.

        Returns
        -------
        np.ndarray
            The scaled data.
        """
        if hasattr(self, "_imputer") and self._imputer is not None:
            X = self._imputer.transform(X)
        return super().transform(X)

    def fit(self,
            X: Union[np.ndarray, pd.DataFrame],
            y: Union[np.ndarray, pd.DataFrame] = None,
            **fit_params):
        """
        Compute the maximum absolute value to be used for later scaling.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            The data to compute the maximum absolute value on.
        y : None
            Ignored.
        **fit_params : dict
            Additional fit parameters.

        Returns
        -------
        self : object
            Fitted scaler.
        """
        if hasattr(self, "_imputer") and self._imputer is not None:
            return self._imputer.transform(X)
        return super().fit(X, y, **fit_params)

    def fit_transform(self,
                      X: Union[np.ndarray, pd.DataFrame],
                      y: Union[np.ndarray, pd.DataFrame] = None,
                      **fit_params) -> np.ndarray:
        """
        Fit to data, then transform it.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            The data to fit and transform.
        y : None
            Ignored.
        **fit_params : dict
            Additional fit parameters.

        Returns
        -------
        np.ndarray
            The transformed data.
        """
        if hasattr(self, "_imputer") and self._imputer is not None:
            return self._imputer.transform(X)
        return super().fit_transform(X, y, **fit_params)


class RobustScaler(_RobustScaler, AbstractScaler):
    """
    Robust Scaler for the MIMIC-III dataset.

    This scaler scales features using statistics that are robust to outliers.

    Parameters
    ----------
    storage_path : str or Path, optional
        The path where the scaler's state will be stored, by default None.
    imputer : object, optional
        The imputer to use for handling missing values, by default None.
    with_centering : bool, optional
        If True, center the data before scaling, by default True.
    with_scaling : bool, optional
        If True, scale the data to the interquartile range, by default True.
    quantile_range : tuple (float, float), optional
        Quantile range used to calculate the scale, by default (25.0, 75.0).
    copy : bool, optional
        If True, a copy of X will be created, by default True.
    verbose : bool, optional
        If True, print verbose logs during processing, by default True.
    """
    def __init__(self,
                 storage_path=None,
                 imputer=None,
                 with_centering=True,
                 with_scaling=True,
                 quantile_range=(25.0, 75.0),
                 copy=True,
                 verbose=True):
        self._storage_name = "robust_scaler.pkl"
        AbstractScaler.__init__(self, storage_path=storage_path, imputer=imputer, verbose=verbose)
        _RobustScaler().__init__(with_centering=with_centering,
                         with_scaling=with_scaling,
                         quantile_range=quantile_range,
                         copy=copy)

    @classmethod
    def _get_param_names(cls):
        """
        Necessary for scikit-learn compatibility.
        """
        return []

    def transform(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
        """
        Scale the data.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            The data to be scaled.

        Returns
        -------
        np.ndarray
            The scaled data.
        """
        if hasattr(self, "_imputer") and self._imputer is not None:
            X = self._imputer.transform(X)
        return super().transform(X)

    def fit(self,
            X: Union[np.ndarray, pd.DataFrame],
            y: Union[np.ndarray, pd.DataFrame] = None,
            **fit_params):
        """
        Compute the median and quantiles to be used for later scaling.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            The data to compute the median and quantiles on.
        y : None
            Ignored.
        **fit_params : dict
            Additional fit parameters.

        Returns
        -------
        self : object
            Fitted scaler.
        """
        if hasattr(self, "_imputer") and self._imputer is not None:
            return self._imputer.transform(X)
        return super().fit(X, y, **fit_params)

    def fit_transform(self,
                      X: Union[np.ndarray, pd.DataFrame],
                      y: Union[np.ndarray, pd.DataFrame] = None,
                      **fit_params) -> np.ndarray:
        """
        Fit to data, then transform it.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            The data to fit and transform.
        y : None
            Ignored.
        **fit_params : dict
            Additional fit parameters.

        Returns
        -------
        np.ndarray
            The transformed data.
        """
        if hasattr(self, "_imputer") and self._imputer is not None:
            return self._imputer.transform(X)
        return super().fit_transform(X, y, **fit_params)


### FILE: .\src\preprocessing\__init__.py ###
import os
import numpy as np
import pickle
from utils.IO import *
from tensorflow.keras.utils import Progbar
from pathlib import Path
from abc import ABC, abstractmethod



class AbstractScikitProcessor(ABC):
    """
    Abstract base class for scikit-learn style processors.

    This class provides a template for processors that need to implement `transform`, `fit`, and `partial_fit`
    methods. It also includes methods for saving and loading the processor's state.

    Parameters
    ----------
    storage_path : Path
        The path where the processor's state will be stored.
    """

    @abstractmethod
    def __init__(self, storage_path: Path):
        """_summary_

        Args:
            storage_path (_type_): _description_
        """
        ...

    @abstractmethod
    def transform(self, X: np.ndarray):
        """
        Transform the input data once the preprocessor has been fitted.

        Parameters
        ----------
        X : np.ndarray
            The input data to transform.

        Returns
        -------
        np.ndarray
            The transformed data.
        """
        ...

    @abstractmethod
    def fit(self, X: np.ndarray):
        """
        Fit the processor to the input data.

        Parameters
        ----------
        X : np.ndarray
            The input data to fit.
        """
        ...

    @abstractmethod
    def partial_fit(self, X: np.ndarray):
        """
        Partially fit the processor to the input data.

        Parameters
        ----------
        X : np.ndarray
            The input data to partially fit.
        """
        ...

    def save(self, storage_path=None):
        """
        Save the processor's state to the storage path.

        Parameters
        ----------
        storage_path : Path, optional
            The path where the processor's state will be saved. If None, the existing storage path is used.

        Raises
        ------
        ValueError
            If no storage path is provided.
        """
        if storage_path is not None:
            self._storage_path = Path(storage_path, self._storage_name)
        if self._storage_path is None:
            raise ValueError("No storage path provided!")
        with open(self._storage_path, "wb") as save_file:
            pickle.dump(obj=self.__dict__, file=save_file, protocol=2)

    def load(self):
        """
        Load the processor's state from the storage path.

        Returns
        -------
        int
            1 if the state was successfully loaded, 0 otherwise.
        """
        if self._storage_path is not None:
            if self._storage_path.is_file():
                if os.path.getsize(self._storage_path) > 0:
                    info_io(f"Loading {Path(self._storage_path).stem} from:\n{self._storage_path}")
                    with open(self._storage_path, "rb") as load_file:
                        load_params = pickle.load(load_file)
                    for key, value in load_params.items():
                        setattr(self, key, value)

                    return 1
        return 0

    def fit_dataset(self, X):
        """
        Fit the processor to an entire dataset.

        Parameters
        ----------
        X : iterable
            The dataset to fit.

        Returns
        -------
        self
            The fitted processor.
        """
        if self._verbose:
            info_io(f"Fitting scaler to dataset of size {len(X)}")
            progbar = Progbar(len(X), unit_name='step')
        n_fitted = 0

        for frame in X:
            if hasattr(self, "_imputer") and self._imputer is not None:
                frame = self._imputer.transform(frame)
            self.partial_fit(frame)
            n_fitted += 1
            if self._verbose:
                progbar.update(n_fitted)

        self.save()

        if self._verbose:
            info_io(f"Done computing new {Path(self._storage_path).stem}.")
        return self

    def fit_reader(self, reader, save=False):
        """
        Fit the processor to a dataset read from a reader.

        Parameters
        ----------
        reader : ProcessedSetReader
            The reader to read the dataset from.
        save : bool, optional
            Whether to save the processor's state after fitting, by default False.

        Returns
        -------
        self
            The fitted processor.
        """
        if self._storage_path is None:
            self._storage_path = Path(reader.root_path, self._storage_name)
            self._storage_path.parent.mkdir(parents=True, exist_ok=True)
        if self.load():
            return self
        if self._verbose:
            info_io(
                f"Fitting {Path(self._storage_path).stem} to reader of size {len(reader.subject_ids)}"
            )
            progbar = Progbar(len(reader.subject_ids), unit_name='step')

        n_fitted = 0

        for subject_id in reader.subject_ids:
            X_subjects, _ = reader.read_sample(subject_id).values()
            for frame in X_subjects:
                if hasattr(self, "_imputer") and self._imputer is not None:
                    frame = self._imputer.transform(frame)
                self.partial_fit(frame)
            n_fitted += 1
            if self._verbose:
                progbar.update(n_fitted)

        self.save()

        if self._verbose:
            info_io(
                f"Done computing new {Path(self._storage_path).stem}.\nSaved in location {self._storage_path}!"
            )

        return self


### FILE: .\src\utils\IO.py ###
"""Utility file

This file implements functionalities used by other modules and accessible to 
the user

Todo:
    - Use a settings.json
    - fix the progressbar bullshit

"""

# Import necessary packages
import datetime
import os
import datetime
import inspect
import pdb
import sys
from pathlib import Path
from colorama import init, Fore, Style

__all__ = ["info_io", "info_io", "tests_io", "error_io", "debug_io", "warn_io", "suppress_io"]

WORKINGDIR = os.getenv("WORKINGDIR")

# Initialize Colorama
init(autoreset=True)


def _clean_path(path):
    """
    If path is inside of the directroy, make it relative to directory.
    """
    path = Path(path).relative_to(WORKINGDIR)
    if str(path).startswith("src"):
        return Path(path).relative_to("src")
    elif str(path).startswith("tests"):
        return Path(path).relative_to("tests")
    elif str(path).startswith("script"):
        return Path(path).relative_to("script")
    elif str(path).startswith("examples"):
        return Path(path).relative_to("examples")


DEBUG_OPTION = None
main_name = Path(sys.argv[0])
if "pytest" in str(main_name):
    PATH_PADDING = max([
        len(str(_clean_path(path)))
        for path in Path(WORKINGDIR).glob("**/*")
        if path.suffix == ".py"
    ])
else:
    # Doxygen may break on this
    try:
        PATH_PADDING = max([
            len(str(_clean_path(path)))
            for path in Path(WORKINGDIR).glob("**/*")
            if path.suffix == ".py" and not path.name.startswith("tests")
        ] + [len(str(main_name.relative_to(WORKINGDIR)))])
    except ValueError:
        PATH_PADDING = 0
# {path: len(str(path)) for path in Path().glob("**/*") if path.suffix == ".py"}
SRC_DIR = Path(WORKINGDIR, "src")
SCRIPT_DIR = Path(WORKINGDIR, "scripts")
TEST_DIR = Path(os.getenv("TESTS"))
EXAMPLE_DIR = Path(os.getenv("EXAMPLES"))
# Might need adjustment
HEADER_LENGTH = 5 + 19 + 3 + PATH_PADDING + 3 + 2 + 4
TAG_PADDING = 8


def _get_relative_path(file_path: Path):
    """
    Get the relative path based on the file location and predefined directories.
    """
    file_path = str(file_path)
    if "src" in file_path:
        relativa_path = SRC_DIR
        return Path(file_path).relative_to(relativa_path)
    elif "script" in file_path:
        relativa_path = SCRIPT_DIR
        return Path(file_path).relative_to(relativa_path)
    elif "tests" in file_path:
        relativa_path = TEST_DIR
        return Path(file_path).relative_to(relativa_path)
    elif "examples" in file_path:
        relativa_path = EXAMPLE_DIR
        return Path(file_path).relative_to(relativa_path)
    return file_path


def _get_line(caller):
    """
    Generates a formatted line with the caller's file path and line number.
    """
    path = str(_get_relative_path(caller.filename)) + " "
    return f"{Fore.LIGHTWHITE_EX}{str(datetime.datetime.now().strftime('%m-%d %H:%M:%S'))[:19]}{Style.RESET_ALL} : {path:-<{PATH_PADDING}} : {Fore.LIGHTCYAN_EX}L {caller.lineno:<4}{Style.RESET_ALL}"


def _print_iostring(string: str, line_header: str, flush_block: bool, collor: str):
    """
    Prints the message itself without the header.
    """
    if flush_block:
        lines = string.split('\n')
        num_lines = len(lines)
        sys.stdout.write(f"\x1b[{num_lines}A")
        sys.stdout.write('\x1b[2K')
        print(lines[0])
        for line in lines[1:]:
            sys.stdout.write('\x1b[2K')
            print(" " * HEADER_LENGTH + f"{collor}-{Style.RESET_ALL} {line}")

        sys.stdout.flush()
        return True
    elif "\n" in string:
        lines = string.split('\n')
        print(lines[0])
        for line in lines[1:]:
            print(" " * HEADER_LENGTH + f"{collor}-{Style.RESET_ALL} {line}")
        return True
    return False


def info_io(message: str,
            level: int = None,
            end: str = None,
            flush: bool = None,
            unflush: bool = None,
            flush_block: bool = False):
    """
    Prints a message with an information header, which can be formatted at different levels, or without any header formatting.
    Level 0: Wide header
    Level 1: Medium header
    Level 2: Narrow header
    Level None: No header formatting, behaves like the original info_io.

    Parameters
    ----------
    message : str
        The message to log.
    level : int, optional
        The level of the header, or None for no header formatting. Defaults to None.
    end : str, optional
        How to end the print, e.g., "\n" (newline). Defaults to None.
    flush : bool, optional
        Whether to forcibly flush the stream. Defaults to None.
    unflush : bool, optional
        If True, adds an additional newline before the message. Defaults to None.
    flush_block : bool, optional
        If True, prints the whole block at once to avoid disruptions. Defaults to False.
    """
    base_io("INFO", Fore.BLUE, message, level, end, flush, unflush, flush_block)


def tests_io(message: str,
             level: int = None,
             end: str = None,
             flush: bool = None,
             unflush: bool = None,
             flush_block: bool = False):
    """
    Prints a message with an information header, which can be formatted at different levels, or without any header formatting.
    Level 0: Wide header
    Level 1: Medium header
    Level 2: Narrow header
    Level None: No header formatting, behaves like the original info_io.

    Parameters
    ----------
    message : str
        The message to log.
    level : int, optional
        The level of the header, or None for no header formatting. Defaults to None.
    end : str, optional
        How to end the print, e.g., "\n" (newline). Defaults to None.
    flush : bool, optional
        Whether to forcibly flush the stream. Defaults to None.
    unflush : bool, optional
        If True, adds an additional newline before the message. Defaults to None.
    flush_block : bool, optional
        If True, prints the whole block at once to avoid disruptions. Defaults to False.
    """
    base_io("TEST", Fore.GREEN, message, level, end, flush, unflush, flush_block)


def debug_io(message: str,
             end: str = None,
             flush: bool = None,
             unflush: bool = None,
             flush_block: bool = False):
    """
    Prints a debug message if the DEBUG_OPTION is set to a non-zero value.

    Parameters
    ----------
    message : str
        The debug message to log.
    end : str, optional
        How to end the print, e.g., "\n" (newline). Defaults to None.
    flush : bool, optional
        Whether to forcibly flush the stream. Defaults to None.
    unflush : bool, optional
        If True, adds an additional newline before the message. Defaults to None.
    flush_block : bool, optional
        If True, prints the whole block at once to avoid disruptions. Defaults to False.
    """
    # TODO: Improve this solution (Adopted because file level declaration will initialize before the var can be changed by test fixtures)
    global DEBUG_OPTION
    if DEBUG_OPTION is None:
        DEBUG_OPTION = os.getenv("DEBUG", "0")
    if DEBUG_OPTION == "0":
        return
    base_io("DEBUG", Fore.YELLOW, message, None, end, flush, unflush, flush_block)


def warn_io(message: str,
            end: str = None,
            flush: bool = None,
            unflush: bool = None,
            flush_block: bool = False):
    """
    Prints a warning.

    Parameters
    ----------
    message : str
        The warning message to log.
    end : str, optional
        How to end the print, e.g., "\n" (newline). Defaults to None.
    flush : bool, optional
        Whether to forcibly flush the stream. Defaults to None.
    unflush : bool, optional
        If True, adds an additional newline before the message. Defaults to None.
    flush_block : bool, optional
        If True, prints the whole block at once to avoid disruptions. Defaults to False.
    """
    base_io("WARN", "\033[38;5;208m", message, None, end, flush, unflush, flush_block)


def error_io(message: str,
             exception_type: Exception,
             end: str = None,
             flush: bool = None,
             unflush: bool = None,
             flush_block: bool = False):
    """
    Prints an error message with an error header and raises the specified exception.

    Parameters
    ----------
    message : str
        The error message to log.
    exception_type : Exception
        The type of exception to raise after logging the message.
    end : str, optional
        How to end the print, e.g., "\n" (newline). Defaults to None.
    flush : bool, optional
        Whether to forcibly flush the stream. Defaults to None.
    unflush : bool, optional
        If True, adds an additional newline before the message. Defaults to None.
    flush_block : bool, optional
        If True, prints the whole block at once to avoid disruptions. Defaults to False.

    Raises
    ------
    exception_type
        The specified exception type is raised after the error message is logged.
    """
    base_io("ERROR", Fore.RED, message, None, end, flush, unflush, flush_block)
    raise exception_type


def base_io(info_tag: str,
            collor: str,
            message: str,
            level: int = None,
            end: str = None,
            flush: bool = None,
            unflush: bool = None,
            flush_block: bool = False):

    # Get caller information
    caller = inspect.getframeinfo(inspect.stack()[2][0])
    line_header = _get_line(caller)

    if level is None:
        header_message = message
    else:
        assert level in [0, 1, 2], ValueError("Header level must be one of 0, 1, 2")
        width = [60, 50, 40][level]
        header_message = '-' * 10 + ' ' + collor + f'{message}{Style.RESET_ALL} '.ljust(
            width + len({Style.RESET_ALL}), "-")
        flush_block = False  # Disable flush_block when level is specified
    collor_info_tag = f"{collor}{info_tag}{Style.RESET_ALL} "
    info_tag_padding = TAG_PADDING + len(collor) + len(Style.RESET_ALL)

    io_string = f"{collor_info_tag.ljust(info_tag_padding, '-')} {line_header} {collor}-{Style.RESET_ALL} {header_message}"

    if level in [0, 1]:
        print(" " * HEADER_LENGTH + "-")
    if unflush:
        print()
    if not _print_iostring(io_string, line_header, flush_block, collor):
        if flush:
            end = "\r"
        print(io_string, end=end, flush=flush)


#https://stackoverflow.com/questions/8391411/how-to-block-calls-to-print
class suppress_io:

    def __enter__(self):
        self._original_stdout = sys.stdout
        sys.stdout = open(os.devnull, 'w')

    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stdout.close()
        sys.stdout = self._original_stdout


### FILE: .\src\utils\notebooks.py ###
import ipywidgets as widgets

# The docker environment manipulates the python path to include our source directory
# Execute this from within the docker environ to make these import work
import keras
import matplotlib.pyplot as plt
import numpy as np
from tensorflow import math

# The docker environment manipulates the python path to include our source directory
# Execute this from within the docker environ to make these import work
import visualization

label_name = {
    "hhourly": 'energy(kWh/hh)',
    "hourly": 'energy(kWh/hh)_sum',
    "hdaily": 'energy(kWh/hh)_sum',
    "qdaily": 'energy(kWh/hh)_sum',
    "daily": 'energy_sum'
}


class Metric():
    """
    """

    def __init__(self, bin_averages):
        """"""
        self.bin_averages = bin_averages

    def cat_mse(self, y_true, y_pred):
        """
        """
        y_pred = math.argmax(y_pred, axis=1)
        y_true = math.argmax(y_true, axis=1)
        y_pred = np.array([self.bin_averages[val] for val in y_pred.numpy()])
        y_true = np.array([self.bin_averages[val] for val in y_true.numpy()])

        return keras.losses.mse(y_pred, y_true)


class History():

    def __init__(self):
        self.acc = {}
        self.loss = {}
        self.mse = {}

    def update(self, value, history):
        self.acc[f"{value}_acc"] = history.history["accuracy"]
        self.acc[f"{value}_val_acc"] = history.history["val_accuracy"]

        self.loss[f"{value}_loss"] = history.history["loss"]
        self.loss[f"{value}_val_loss"] = history.history["val_loss"]

        self.mse[f"{value}_cat_mse"] = history.history["cat_mse"]
        self.mse[f"{value}_val_cat_mse"] = history.history["val_cat_mse"]


def make_hist_plot(values, history, type):
    plt.clf()
    colors = ["darkviolet", "darkgreen", "goldenrod", "darkblue"]
    plt.figure(figsize=(10, 5))

    for value, color in zip(values, colors):
        key = f"{value}_{type}"
        plt.plot(history[key], label=key, color=color)
        val_key = f"{value}_val_{type}"
        plt.plot(history[val_key], label=val_key, color=color, linestyle="--")

    plt.grid()
    plt.legend()

### FILE: .\src\utils\__init__.py ###
"""Utility file

This file implements functionalities used by other modules and accessible to 
the user

Todo:
    - Use a settings.json
    - fix the progressbar bullshit

"""

import re
import json
import numpy as np
import pandas as pd
from typing import Dict, Union
from multipledispatch import dispatch

from utils.IO import *
from pathlib import Path


def is_numerical(df: pd.DataFrame) -> bool:
    """
    Check if a DataFrame contains only numerical data.

    Parameters
    ----------
    df : pd.DataFrame
        The DataFrame to check.

    Returns
    -------
    bool
        True if the DataFrame is numerical, False otherwise.
    """    
    # This is the worst implementation but what works works
    try:
        df.astype(float)
        return True
    except:
        pass
    if (df.dtypes == object).any() or\
       (df.dtypes == "category").any() or\
       (df.dtypes == "datetime64[ns]").any():
        return False
    return True


def to_snake_case(string):
    """
    Convert a string to snake case.

    Parameters
    ----------
    string : str
        The string to convert.

    Returns
    -------
    str
        The converted string in snake case.
    """
    # Replace spaces and hyphens with underscores
    string = re.sub(r'[\s-]+', '_', string)

    # Replace camel case and Pascal case with underscores
    string = re.sub(r'([a-z0-9])([A-Z])', r'\1_\2', string)

    # Convert the string to lowercase
    string = string.lower()

    return string


def is_colwise_numerical(df: pd.DataFrame) -> Dict[str, bool]:
    """
    Check if each column in a DataFrame is numerical.

    Parameters
    ----------
    df : pd.DataFrame
        The DataFrame to check.

    Returns
    -------
    dict
        A dictionary with column names as keys and boolean values indicating if the column is numerical.
    """
    return {col: is_numerical(df[[col]]) for col in df.columns}


def is_allnan(data: Union[pd.DataFrame, pd.Series, np.ndarray]):
    """
    Check if all elements in the data are NaN.

    Parameters
    ----------
    data : Union[pd.DataFrame, pd.Series, np.ndarray]
        The data to check.

    Returns
    -------
    bool
        True if all elements are NaN, False otherwise.
    """
    if isinstance(data, (pd.DataFrame, pd.Series)):
        return data.isna().all().all()
    elif isinstance(data, np.ndarray):
        return np.isnan(data).all()
    else:
        raise TypeError("Input must be a pandas DataFrame, Series, or numpy array.")


def update_json(json_path, items: dict):
    """
    Update a JSON file with new items.

    Parameters
    ----------
    json_path : Path
        The path to the JSON file.
    items : dict
        The items to update the JSON file with.

    Returns
    -------
    dict
        The updated JSON data.
    """
    if not json_path.parent.is_dir():
        json_path.parent.mkdir(parents=True, exist_ok=True)
    if not json_path.is_file():
        with open(json_path, 'w+') as file:
            json.dump({}, file)

    with open(json_path, 'r') as file:
        json_data = json.load(file)

    json_data.update(items)
    try:
        with open(json_path, 'w') as file:
            json.dump(json_data, file, indent=4)
    except KeyboardInterrupt:
        info_io("Finishing JSON operation before interupt.")
        with open(json_path, 'w') as file:
            json.dump(json_data, file, indent=4)

    return json_data


def load_json(json_path):
    """
    Load data from a JSON file.

    Parameters
    ----------
    json_path : Path
        The path to the JSON file.

    Returns
    -------
    dict
        The loaded JSON data.
    """
    if not json_path.is_file():
        return {}

    with open(json_path, 'r') as file:
        json_data = json.load(file)

    return json_data


def write_json(json_path, json_data):
    """
    Write data to a JSON file.

    Parameters
    ----------
    json_path : Path
        The path to the JSON file.
    json_data : dict
        The data to write to the JSON file.
    """
    try:
        with open(json_path, 'w') as file:
            json.dump(json_data, file, indent=4)
    except KeyboardInterrupt:
        info_io("Finishing JSON operation before interupt.")
        with open(json_path, 'w') as file:
            json.dump(json_data, file, indent=4)

    return


def dict_subset(dictionary: dict, keys: list):
    """
    Get a subset of a dictionary.

    Parameters
    ----------
    dictionary : dict
        The original dictionary.
    keys : list
        The keys to extract from the dictionary.

    Returns
    -------
    dict
        A dictionary containing only the specified keys.
    """
    return {k: dictionary[k] for k in keys if k in dictionary}


def is_iterable(obj):
    """
    Check if an object is iterable.

    Parameters
    ----------
    obj : object
        The object to check.

    Returns
    -------
    bool
        True if the object is iterable, False otherwise.
    """
    return hasattr(obj, '__iter__')


def make_prediction_vector(model, generator, batches=20, bin_averages=None):
    """_summary_
    """
    # TODO! fix bin averages instead
    Xs = list()
    ys = list()

    for _ in range(batches):
        X, y = generator.next()
        Xs.append(X)
        ys.append(y)

    y_true = np.concatenate(ys)
    y_pred = np.concatenate([model.predict(X, verbose=0) for X in Xs])

    if bin_averages:
        # TODO! shape mismatch betwenn prediction vector length and length of bin averages
        y_pred = np.array([
            bin_averages[int(label)]
            if label < len(bin_averages) else bin_averages[len(bin_averages) - 1]
            for label in np.argmax(y_pred, axis=1)
        ]).reshape(1, -1)

        if len(y_true.shape) > 1:
            y_true = np.argmax(y_true, axis=1)

        y_true = np.array([
            bin_averages[int(label)]
            if label < len(bin_averages) else bin_averages[len(bin_averages) - 1]
            for label in y_true
        ]).reshape(1, -1)

    # TODO! this should be y_pred y_true
    return y_pred, y_true


def count_csv_size(file_path: Path):

    def blocks(files, size=65536):
        while True:
            b = files.read(size)
            if not b:
                break
            yield b

    with open(file_path, "r", encoding="utf-8", errors='ignore') as f:
        file_length = sum(bl.count("\n") for bl in blocks(f))

    return file_length - 1


### FILE: .\tests\conftest.py ###
import shutil
import pytest
import os
import re
import datasets
import pandas as pd
from typing import Dict
from pathlib import Path
from tests.settings import *
from utils.IO import *
from datasets.readers import ExtractedSetReader, EventReader, ProcessedSetReader
from settings import *

collect_ignore = ['src/utils/IO.py']


def pytest_configure(config) -> None:
    os.environ["DEBUG"] = "0"

    if SEMITEMP_DIR.is_dir():
        shutil.rmtree(str(SEMITEMP_DIR))

    [
        datasets.load_data(chunksize=75835,
                           source_path=TEST_DATA_DEMO,
                           storage_path=SEMITEMP_DIR,
                           preprocess=True,
                           task=name) for name in TASK_NAMES
    ]


@pytest.fixture(scope="function", autouse=True)
def cleanup():
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))

    # Execution
    yield

    # Clean after execution
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))


@pytest.fixture(scope="session")
def extracted_reader() -> ExtractedSetReader:
    reader = datasets.load_data(chunksize=75835,
                                source_path=TEST_DATA_DEMO,
                                storage_path=SEMITEMP_DIR)
    return reader


@pytest.fixture(scope="session")
def subject_ids(extracted_reader: ExtractedSetReader) -> list:
    icu_history = extracted_reader.read_csv("icu_history.csv")
    subjects = icu_history["SUBJECT_ID"].astype(int).unique().tolist()
    return subjects


@pytest.fixture(scope="session")
def preprocessed_readers() -> Dict[str, ProcessedSetReader]:
    return {
        name:
            datasets.load_data(chunksize=75835,
                               source_path=TEST_DATA_DEMO,
                               storage_path=SEMITEMP_DIR,
                               preprocess=True,
                               task=name) for name in TASK_NAMES
    }


@pytest.fixture(scope="session")
def engineered_readers() -> Dict[str, ProcessedSetReader]:
    return {
        name:
            datasets.load_data(chunksize=75835,
                               source_path=TEST_DATA_DEMO,
                               storage_path=SEMITEMP_DIR,
                               engineer=True,
                               task=name) for name in TASK_NAMES
    }


@pytest.fixture(scope="session")
def discretized_readers() -> Dict[str, ProcessedSetReader]:
    return {
        name:
            datasets.load_data(chunksize=75835,
                               source_path=TEST_DATA_DEMO,
                               storage_path=SEMITEMP_DIR,
                               discretize=True,
                               task=name) for name in TASK_NAMES
    }


@pytest.fixture(scope="session")
def discretizer_listfiles() -> None:
    list_files = dict()
    for task_name in TASK_NAMES:
        # Path to discretizer sets
        test_data_dir = Path(TEST_GT_DIR, "discretized", TASK_NAME_MAPPING[task_name])
        # Listfile with truth values
        listfile = pd.read_csv(Path(test_data_dir, "listfile.csv")).set_index("stay")
        stay_name_regex = r"(\d+)_episode(\d+)_timeseries\.csv"

        listfile = listfile.reset_index()
        listfile["subject"] = listfile["stay"].apply(
            lambda x: re.search(stay_name_regex, x).group(1))
        listfile["icustay"] = listfile["stay"].apply(
            lambda x: re.search(stay_name_regex, x).group(2))
        listfile = listfile.set_index("stay")
        list_files[task_name] = listfile
    return list_files


def pytest_unconfigure(config) -> None:
    os.environ["DEBUG"] = "0"
    if SEMITEMP_DIR.is_dir():
        shutil.rmtree(str(SEMITEMP_DIR))


### FILE: .\tests\decorators.py ###
import pytest
import functools


class repeat:

    def __init__(self, times):
        self.times = times

    def __call__(self, func):

        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            for _ in range(self.times):
                func(*args, **kwargs)

        return wrapper


### FILE: .\tests\settings.py ###
import json
import os
from dotenv import load_dotenv
from pathlib import Path

load_dotenv(verbose=False)

__all__ = [
    'TEST_SETTINGS', 'SEMITEMP_DIR', 'TEMP_DIR', 'DEVTEMP_DIR', 'TEST_DATA_DIR', 'TEST_DATA_DEMO',
    'TEST_GT_DIR', 'TASK_NAMES', 'FTASK_NAMES', 'TASK_NAME_MAPPING', 'DATASET_SETTINGS',
    'DECOMP_SETTINGS', 'LOS_SETTINGS', 'PHENOT_SETTINGS', 'IHM_SETTINGS'
]

TEST_SETTINGS = json.load(Path(os.getenv("TESTS"), "etc", "test.json").open())
SEMITEMP_DIR = Path(os.getenv("WORKINGDIR"), "tests", "data", "semitemp")
TEMP_DIR = Path(os.getenv("WORKINGDIR"), "tests", "data", "temp")
DEVTEMP_DIR = Path(os.getenv("WORKINGDIR"), "tests", "data", "devtemp")
TEST_DATA_DIR = Path(os.getenv("WORKINGDIR"), "tests", "data")
TEST_DATA_DEMO = Path(TEST_DATA_DIR, "physionet.org", "files", "mimiciii-demo", "1.4")
TEST_GT_DIR = Path(TEST_DATA_DIR, "generated-benchmark")

TASK_NAMES = ["IHM", "DECOMP", "LOS", "PHENO"]
FTASK_NAMES = ["in-hospital-mortality", "decompensation", "length-of-stay", "phenotyping"]
TASK_NAME_MAPPING = dict(zip(TASK_NAMES, FTASK_NAMES))

with Path(os.getenv("CONFIG"), "datasets.json").open() as file:
    DATASET_SETTINGS = json.load(file)
    DECOMP_SETTINGS = DATASET_SETTINGS["DECOMP"]
    LOS_SETTINGS = DATASET_SETTINGS["LOS"]
    PHENOT_SETTINGS = DATASET_SETTINGS["PHENO"]
    IHM_SETTINGS = DATASET_SETTINGS["IHM"]


### FILE: .\tests\test_generators.py ###
import datasets
import pytest
from generators.tf2 import TFGenerator
from generators.pytorch import TorchGenerator
from generators.stream import RiverGenerator
from preprocessing.scalers import MinMaxScaler
import numpy as np
from utils.IO import *
from datasets.readers import ProcessedSetReader
from tests.settings import *
from preprocessing.imputers import PartialImputer


@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_tf_generator(task_name, discretized_readers):
    tests_io(f"Test case tf2 iterative generator for task: {task_name}", level=0)

    # Prepare generator inputs
    reader = discretized_readers[task_name]
    scaler = MinMaxScaler().fit_reader(reader)

    # Bining types for LOS
    for bining in ["none", "log", "custom"]:
        # Batch sizes for dimensional robustness
        for batch_size in [1, 8, 16]:
            tests_io(f"Test case batch size: {batch_size}" + \
                    (f" and bining: {bining}" if task_name == "LOS" else ""))

            # Create generator
            generator = TFGenerator(reader=reader,
                                    scaler=scaler,
                                    batch_size=batch_size,
                                    bining=bining,
                                    shuffle=True)
            assert len(generator)
            for batch in range(len(generator)):
                # Get batch
                X, y = generator.__getitem__()
                # Check batch
                assert_batch_sanity(X, y, batch_size, bining)
                tests_io(f"Successfully tested {batch + 1} batches", flush=True)
            tests_io(f"Successfully tested {batch + 1} batches")
        if task_name != "LOS":
            break


@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_torch_generator(task_name, discretized_readers):
    tests_io(f"Test case torch iterative generator for task: {task_name}", level=0)

    # Prepare generator inputs
    reader = discretized_readers[task_name]
    scaler = MinMaxScaler().fit_reader(reader)

    # Bining types for LOS
    for bining in ["none", "log", "custom"]:
        # Batch sizes for dimensional robustness
        for batch_size in [1, 8, 16]:
            tests_io(f"Test case batch size: {batch_size}" + \
                    (f" and bining: {bining}" if task_name == "LOS" else ""))
            # Create generator
            generator = TorchGenerator(reader=reader,
                                       scaler=scaler,
                                       batch_size=batch_size,
                                       bining=bining,
                                       drop_last=True,
                                       shuffle=True)
            assert len(generator)
            for batch, (X, y) in enumerate(generator):
                # Get batch
                X = X.numpy()
                y = y.numpy()
                # Check batch
                assert_batch_sanity(X, y, batch_size, bining)

                tests_io(f"Successfully tested {batch + 1} batches", flush=True)
            tests_io(f"Successfully tested {batch + 1} batches")
        if task_name != "LOS":
            break


@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_river_generator(task_name, engineered_readers):
    tests_io(f"Test case river generator for task: {task_name}", level=0)

    # Prepare generator inputs
    reader = engineered_readers[task_name]
    imputer = PartialImputer().fit_reader(reader)
    scaler = MinMaxScaler(imputer=imputer).fit_reader(reader)

    # Bining types for LOS
    for bining in ["log", "custom"]:  # ["none", "log", "custom"]:
        # No Batch sizes this is a stream
        generator = RiverGenerator(reader=reader, scaler=scaler, shuffle=True, bining=bining)
        if task_name == "LOS":
            tests_io(f"Test case with bining: {bining}")
        # Create generator
        for batch, (X, y) in enumerate(generator):
            # Get batch
            X = np.fromiter(X.values(), dtype=float)
            if task_name == "PHENO" or (task_name == "LOS" and bining != "none"):
                y = np.fromiter(y.values(), dtype=float)
            assert_sample_sanity(X, y, bining)
            tests_io(f"Successfully tested {batch + 1} batches", flush=True)
        tests_io(f"Successfully tested {batch + 1} batches")
        if task_name != "LOS":
            break


def assert_batch_sanity(X: np.ndarray, y: np.ndarray, batch_size: int, bining: str):
    # The batch might be sane but I am not
    assert not np.isnan(X).any()
    assert not np.isnan(y).any()
    assert X.shape[0] == batch_size
    assert X.shape[2] == 59
    assert X.dtype == np.float32
    assert y.dtype == np.int8
    assert y.shape[0] == batch_size
    if task_name in ["PHENO"]:
        assert y.shape[1] == 25
    elif task_name in ["DECOMP", "IHM"]:
        assert y.shape[1] == 1
    elif task_name in ["LOS"]:
        # Depending on the binning this changes
        if bining == "none":
            assert y.shape[1] == 1
        elif bining in ["log", "custom"]:
            assert y.shape[1] == 10


def assert_sample_sanity(X: np.ndarray, y: np.ndarray, bining: str):
    assert not np.isnan(X).any()
    assert not np.isnan(y).any()
    assert len(X) == 714
    if task_name == "PHENO":
        assert len(y) == 25
    elif task_name == "LOS":
        if bining == "none":
            assert len(y) == 1
        elif bining in ["log", "custom"]:
            assert len(y) == 10
    else:
        assert isinstance(y, (float, int, bool))


if __name__ == "__main__":
    # for task_name in TASK_NAMES:
    for task_name in ["LOS"]:
        """
        reader = datasets.load_data(chunksize=75836,
                                    source_path=TEST_DATA_DEMO,
                                    storage_path=SEMITEMP_DIR,
                                    discretize=True,
                                    time_step_size=1.0,
                                    start_at_zero=True,
                                    impute_strategy='previous',
                                    task=task_name)
        # test_tf_generator(task_name, {task_name: reader})
        test_torch_generator(task_name, {task_name: reader})
        """
        reader = datasets.load_data(chunksize=75836,
                                    source_path=TEST_DATA_DEMO,
                                    storage_path=SEMITEMP_DIR,
                                    engineer=True,
                                    task=task_name)
        test_river_generator(task_name, {task_name: reader})


### FILE: .\tests\__init__.py ###



### FILE: .\tests\data\mimic3benchmarks\convert_columns.py ###
import pandas as pd
from datasets.mimic_utils import upper_case_column_names
from tests.settings import *

for csv in TEST_DATA_DEMO.iterdir():
    if csv.is_dir() or csv.suffix != ".csv":
        continue

    df = pd.read_csv(csv,
                     dtype={
                         "ROW_ID": 'Int64',
                         "ICUSTAY_ID": 'Int64',
                         "HADM_ID": 'Int64',
                         "SUBJECT_ID": 'Int64',
                         "row_id": 'Int64',
                         "icustay_id": 'Int64',
                         "hadm_id": 'Int64',
                         "subject_id": 'Int64'
                     },
                     low_memory=False)
    df = upper_case_column_names(df)
    df.to_csv(csv, index=False)


### FILE: .\tests\data\mimic3benchmarks\discretize_data.py ###
import os
import sys

sys.path.append(os.getenv("WORKINGDIR"))
import pandas as pd
from pathlib import Path
from tests.settings import *
from mimic3benchmark.readers import DecompensationReader

from mimic3models.preprocessing import Discretizer
from mimic3benchmark.readers import InHospitalMortalityReader, DecompensationReader, LengthOfStayReader, PhenotypingReader

# Set the paths to the data files
processed_paths = {
    "IHM": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "in-hospital-mortality"),
    "LOS": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "length-of-stay"),
    "PHENO": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "phenotyping"),
    "DECOMP": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "decompensation")
}

discretized_paths = {
    "IHM": Path(TEST_DATA_DIR, "generated-benchmark", "discretized", "in-hospital-mortality"),
    "LOS": Path(TEST_DATA_DIR, "generated-benchmark", "discretized", "length-of-stay"),
    "PHENO": Path(TEST_DATA_DIR, "generated-benchmark", "discretized", "phenotyping"),
    "DECOMP": Path(TEST_DATA_DIR, "generated-benchmark", "discretized", "decompensation")
}

readers = {
    "IHM": InHospitalMortalityReader,
    "LOS": LengthOfStayReader,
    "PHENO": PhenotypingReader,
    "DECOMP": DecompensationReader
}

impute_strategies = ['zero', 'normal_value', 'previous', 'next']
start_times = ['zero', 'relative']

# Discritize the data from processed directory using different discretizer settings
for task in TASK_NAMES:
    list_file_path = Path(processed_paths[task], "listfile.csv")
    list_file = pd.read_csv(list_file_path)
    if task in ["IHM", "PHENO"]:
        example_indices = list_file.index
    else:
        example_indices = list_file.groupby("stay")["period_length"].idxmax().values
    discretized_paths[task].mkdir(parents=True, exist_ok=True)
    list_file.loc[example_indices].to_csv(Path(discretized_paths[task], "listfile.csv"),
                                          index=False)

    print(f"Discretizing {task} data")
    for impute_strategy in impute_strategies:
        for start_time in start_times:
            discretizer = Discretizer(timestep=1.0,
                                      store_masks=False,
                                      impute_strategy=impute_strategy,
                                      start_time=start_time)
            reader = readers[task](dataset_dir=processed_paths[task])
            for idx in example_indices:
                sample = reader.read_example(idx)
                discretized_data = discretizer.transform(sample['X'])
                discretizer_header = discretized_data[1].split(',')
                target_dir = Path(discretized_paths[task],
                                  f"imp{impute_strategy}_start{start_time}")
                target_dir.mkdir(parents=True, exist_ok=True)
                pd.DataFrame(discretized_data[0],
                             columns=discretizer_header).to_csv(Path(target_dir, sample['name']),
                                                                index=False)


### FILE: .\tests\data\mimic3benchmarks\engineer_data.py ###
import os
import sys

sys.path.append(os.getenv("WORKINGDIR"))
import pandas as pd
import numpy as np
from pathlib import Path
from tests.settings import TEST_DATA_DIR, TASK_NAMES
# This is copied into the mimic3benchmark directory once cloned
from mimic3benchmark.readers import InHospitalMortalityReader, DecompensationReader, LengthOfStayReader, PhenotypingReader
from mimic3models.in_hospital_mortality.logistic.main import read_and_extract_features as ihm_extractor
from mimic3models.phenotyping.logistic.main import read_and_extract_features as phenotyping_extractor
from mimic3models.length_of_stay.logistic.main import read_and_extract_features as los_extractor
from mimic3models.decompensation.logistic.main import read_and_extract_features as decompensation_extractor

# Set the paths to the data files
processed_paths = {
    "IHM": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "in-hospital-mortality"),
    "LOS": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "length-of-stay"),
    "PHENO": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "phenotyping"),
    "DECOMP": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "decompensation")
}

engineered_paths = {
    "IHM": Path(TEST_DATA_DIR, "generated-benchmark", "engineered", "in-hospital-mortality"),
    "LOS": Path(TEST_DATA_DIR, "generated-benchmark", "engineered", "length-of-stay"),
    "PHENO": Path(TEST_DATA_DIR, "generated-benchmark", "engineered", "phenotyping"),
    "DECOMP": Path(TEST_DATA_DIR, "generated-benchmark", "engineered", "decompensation")
}

readers = {
    "IHM": InHospitalMortalityReader,
    "LOS": LengthOfStayReader,
    "PHENO": PhenotypingReader,
    "DECOMP": DecompensationReader
}

extractors = {
    "IHM": ihm_extractor,
    "LOS": los_extractor,
    "PHENO": phenotyping_extractor,
    "DECOMP": decompensation_extractor
}

# Create the readers for each task type
ihm_reader = InHospitalMortalityReader(dataset_dir=processed_paths["IHM"],
                                       listfile=Path(processed_paths["IHM"], "listfile.csv"))
decomp_reader = DecompensationReader(dataset_dir=processed_paths["DECOMP"],
                                     listfile=Path(processed_paths["DECOMP"], "listfile.csv"))
los_reader = LengthOfStayReader(dataset_dir=processed_paths["LOS"],
                                listfile=Path(processed_paths["LOS"], "listfile.csv"))
phenotyping_reader = PhenotypingReader(dataset_dir=processed_paths["PHENO"],
                                       listfile=Path(processed_paths["PHENO"], "listfile.csv"))

for task in TASK_NAMES:
    if engineered_paths[task].exists():
        continue
    if task in ["LOS", "DECOMP"]:
        print(f"Engineering data for task: {task}. This may take up to 30 min ...")
    else:
        print(f"Engineering data for task: {task}.")
    reader = readers[task](dataset_dir=processed_paths[task],
                           listfile=Path(processed_paths[task], "listfile.csv"))
    if task == "IHM":
        (X, y, train_names) = extractors[task](reader, period="all", features="all")
    elif task == "PHENO":
        (X, y, train_names, ts) = extractors[task](reader, period="all", features="all")
    else:
        n_samples = min(100000, reader.get_number_of_examples())
        (X, y, train_names, ts) = extractors[task](reader,
                                                   period="all",
                                                   features="all",
                                                   count=n_samples)

    print(f"Done engineering data for task: {task}.")
    # 10127_episode271544_timeseries.csv is included in the original DS despite the subject being a new born infant. Minimum age was set at 18
    X_df = pd.DataFrame(np.concatenate([np.array(train_names).reshape(-1, 1), X],
                                       axis=1)).set_index(0)
    if task == "IHM":
        y_df = pd.DataFrame(np.stack([np.array(train_names), y]).T).set_index(0)
    elif task == "PHENO":
        y_df = pd.DataFrame(
            np.concatenate([np.array(train_names).reshape(-1, 1),
                            np.array(y)], axis=1)).set_index(0)
    else:
        y_df = pd.DataFrame(
            np.concatenate([np.array(train_names).reshape(-1, 1),
                            np.array(y).reshape(-1, 1)],
                           axis=1)).set_index(0)
    engineered_paths[task].mkdir(parents=True, exist_ok=True)
    X_df.to_csv(str(Path(engineered_paths[task], "X.csv")))
    y_df.to_csv(str(Path(engineered_paths[task], "y.csv")))


### FILE: .\tests\data\mimic3benchmarks\rename_files.py ###
import pdb
import re
import os
import pandas as pd
from pathlib import Path
import sys
sys.path.append(os.getenv("WORKINGDIR"))
from tests.settings import *


for subject_dir in Path(TEST_DATA_DIR, "generated-benchmark", "extracted").iterdir():
    if not subject_dir.is_dir():
        continue
    for csv in subject_dir.iterdir():
        if "episode" in csv.name and not "timeseries" in csv.name:
            df = pd.read_csv(csv)
            icustay_id = df["Icustay"].iloc[0]
            file_index = re.findall(r'\d+', csv.name).pop()
            csv.rename(Path(csv.parent, f"episode{icustay_id}.csv"))
            Path(csv.parent, f"episode{file_index}_timeseries.csv").rename(Path(csv.parent, f"episode{icustay_id}_timeseries.csv"))
            # I want to rename the files episodeX.csv and episodeX_timeseries.csv by replacing X, which in this case is a random number by the icustay


### FILE: .\tests\data\mimic3benchmarks\revert_split.py ###
import os
from pathlib import Path
import pandas as pd
import sys

sys.path.append(os.getenv("WORKINGDIR"))
from tests.settings import *

result_paths = [
    Path(TEST_DATA_DIR, "generated-benchmark", "extracted"),
    Path(TEST_DATA_DIR, "generated-benchmark", "processed", "in-hospital-mortality"),
    Path(TEST_DATA_DIR, "generated-benchmark", "processed", "length-of-stay"),
    Path(TEST_DATA_DIR, "generated-benchmark", "processed", "multitask"),
    Path(TEST_DATA_DIR, "generated-benchmark", "processed", "phenotyping"),
    Path(TEST_DATA_DIR, "generated-benchmark", "processed", "decompensation"),
]

for path in result_paths:
    for entity in path.iterdir():
        if entity.is_dir():
            for subject_entity in entity.iterdir():
                if subject_entity.is_dir():
                    for csv in subject_entity.iterdir():
                        target_path = Path(csv.parents[2], csv.parent.name, csv.name)
                        target_path.parent.mkdir(parents=True, exist_ok=True)
                        csv.rename(target_path)
                    subject_entity.rmdir()
                else:
                    target_path = Path(subject_entity.parents[1], subject_entity.name)
                    if subject_entity.name == "listfile.csv" and target_path.exists():
                        listfile_df = pd.read_csv(subject_entity)
                        listfile_df.to_csv(target_path, mode='a', index=False, header=False)
                        subject_entity.unlink()
                    else:
                        subject_entity.rename(target_path)
            entity.rmdir()


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\mimic3csv.py ###
import csv
import numpy as np
import os
import pandas as pd
from tqdm import tqdm

from mimic3benchmark.util import dataframe_from_csv


def read_patients_table(mimic3_path):
    pats = dataframe_from_csv(os.path.join(mimic3_path, 'PATIENTS.csv'))
    pats = pats[['SUBJECT_ID', 'GENDER', 'DOB', 'DOD']]
    pats.DOB = pd.to_datetime(pats.DOB)
    pats.DOD = pd.to_datetime(pats.DOD)
    return pats


def read_admissions_table(mimic3_path):
    admits = dataframe_from_csv(os.path.join(mimic3_path, 'ADMISSIONS.csv'))
    admits = admits[['SUBJECT_ID', 'HADM_ID', 'ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'ETHNICITY', 'DIAGNOSIS']]
    admits.ADMITTIME = pd.to_datetime(admits.ADMITTIME)
    admits.DISCHTIME = pd.to_datetime(admits.DISCHTIME)
    admits.DEATHTIME = pd.to_datetime(admits.DEATHTIME)
    return admits


def read_icustays_table(mimic3_path):
    stays = dataframe_from_csv(os.path.join(mimic3_path, 'ICUSTAYS.csv'))
    stays.INTIME = pd.to_datetime(stays.INTIME)
    stays.OUTTIME = pd.to_datetime(stays.OUTTIME)
    return stays


def read_icd_diagnoses_table(mimic3_path):
    codes = dataframe_from_csv(os.path.join(mimic3_path, 'D_ICD_DIAGNOSES.csv'))
    codes = codes[['ICD9_CODE', 'SHORT_TITLE', 'LONG_TITLE']]
    diagnoses = dataframe_from_csv(os.path.join(mimic3_path, 'DIAGNOSES_ICD.csv'))
    diagnoses = diagnoses.merge(codes, how='inner', left_on='ICD9_CODE', right_on='ICD9_CODE')
    diagnoses[['SUBJECT_ID', 'HADM_ID', 'SEQ_NUM']] = diagnoses[['SUBJECT_ID', 'HADM_ID', 'SEQ_NUM']].astype(int)
    return diagnoses


def read_events_table_by_row(mimic3_path, table):
    nb_rows = {'chartevents': 330712484, 'labevents': 27854056, 'outputevents': 4349219}
    reader = csv.DictReader(open(os.path.join(mimic3_path, table.upper() + '.csv'), 'r'))
    for i, row in enumerate(reader):
        if 'ICUSTAY_ID' not in row:
            row['ICUSTAY_ID'] = ''
        yield row, i, nb_rows[table.lower()]


def count_icd_codes(diagnoses, output_path=None):
    codes = diagnoses[['ICD9_CODE', 'SHORT_TITLE', 'LONG_TITLE']].drop_duplicates().set_index('ICD9_CODE')
    codes['COUNT'] = diagnoses.groupby('ICD9_CODE')['ICUSTAY_ID'].count()
    codes.COUNT = codes.COUNT.fillna(0).astype(int)
    codes = codes[codes.COUNT > 0]
    if output_path:
        codes.to_csv(output_path, index_label='ICD9_CODE')
    return codes.sort_values('COUNT', ascending=False).reset_index()


def remove_icustays_with_transfers(stays):
    stays = stays[(stays.FIRST_WARDID == stays.LAST_WARDID) & (stays.FIRST_CAREUNIT == stays.LAST_CAREUNIT)]
    return stays[['SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'LAST_CAREUNIT', 'DBSOURCE', 'INTIME', 'OUTTIME', 'LOS']]


def merge_on_subject(table1, table2):
    return table1.merge(table2, how='inner', left_on=['SUBJECT_ID'], right_on=['SUBJECT_ID'])


def merge_on_subject_admission(table1, table2):
    return table1.merge(table2, how='inner', left_on=['SUBJECT_ID', 'HADM_ID'], right_on=['SUBJECT_ID', 'HADM_ID'])


def add_age_to_icustays(stays):
    stays['AGE'] = stays.apply(lambda e: (e['INTIME'].to_pydatetime()
                                          - e['DOB'].to_pydatetime()).total_seconds() / 3600.0 / 24.0 / 365.0,
                               axis=1)
    stays.loc[stays.AGE < 0, 'AGE'] = 90
    return stays


def add_inhospital_mortality_to_icustays(stays):
    mortality = stays.DOD.notnull() & ((stays.ADMITTIME <= stays.DOD) & (stays.DISCHTIME >= stays.DOD))
    mortality = mortality | (stays.DEATHTIME.notnull() & ((stays.ADMITTIME <= stays.DEATHTIME) & (stays.DISCHTIME >= stays.DEATHTIME)))
    stays['MORTALITY'] = mortality.astype(int)
    stays['MORTALITY_INHOSPITAL'] = stays['MORTALITY']
    return stays


def add_inunit_mortality_to_icustays(stays):
    mortality = stays.DOD.notnull() & ((stays.INTIME <= stays.DOD) & (stays.OUTTIME >= stays.DOD))
    mortality = mortality | (stays.DEATHTIME.notnull() & ((stays.INTIME <= stays.DEATHTIME) & (stays.OUTTIME >= stays.DEATHTIME)))
    stays['MORTALITY_INUNIT'] = mortality.astype(int)
    return stays


def filter_admissions_on_nb_icustays(stays, min_nb_stays=1, max_nb_stays=1):
    to_keep = stays.groupby('HADM_ID').count()[['ICUSTAY_ID']].reset_index()
    to_keep = to_keep[(to_keep.ICUSTAY_ID >= min_nb_stays) & (to_keep.ICUSTAY_ID <= max_nb_stays)][['HADM_ID']]
    stays = stays.merge(to_keep, how='inner', left_on='HADM_ID', right_on='HADM_ID')
    return stays


def filter_icustays_on_age(stays, min_age=18, max_age=np.inf):
    stays = stays[(stays.AGE >= min_age) & (stays.AGE <= max_age)]
    return stays


def filter_diagnoses_on_stays(diagnoses, stays):
    return diagnoses.merge(stays[['SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID']].drop_duplicates(), how='inner',
                           left_on=['SUBJECT_ID', 'HADM_ID'], right_on=['SUBJECT_ID', 'HADM_ID'])


def break_up_stays_by_subject(stays, output_path, subjects=None):
    subjects = stays.SUBJECT_ID.unique() if subjects is None else subjects
    nb_subjects = subjects.shape[0]
    for subject_id in tqdm(subjects, total=nb_subjects, desc='Breaking up stays by subjects'):
        dn = os.path.join(output_path, str(subject_id))
        try:
            os.makedirs(dn)
        except:
            pass

        stays[stays.SUBJECT_ID == subject_id].sort_values(by='INTIME').to_csv(os.path.join(dn, 'stays.csv'),
                                                                              index=False)


def break_up_diagnoses_by_subject(diagnoses, output_path, subjects=None):
    subjects = diagnoses.SUBJECT_ID.unique() if subjects is None else subjects
    nb_subjects = subjects.shape[0]
    for subject_id in tqdm(subjects, total=nb_subjects, desc='Breaking up diagnoses by subjects'):
        dn = os.path.join(output_path, str(subject_id))
        try:
            os.makedirs(dn)
        except:
            pass

        diagnoses[diagnoses.SUBJECT_ID == subject_id].sort_values(by=['ICUSTAY_ID', 'SEQ_NUM'])\
                                                     .to_csv(os.path.join(dn, 'diagnoses.csv'), index=False)


def read_events_table_and_break_up_by_subject(mimic3_path, table, output_path,
                                              items_to_keep=None, subjects_to_keep=None):
    obs_header = ['SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'CHARTTIME', 'ITEMID', 'VALUE', 'VALUEUOM']
    if items_to_keep is not None:
        items_to_keep = set([str(s) for s in items_to_keep])
    if subjects_to_keep is not None:
        subjects_to_keep = set([str(s) for s in subjects_to_keep])

    class DataStats(object):
        def __init__(self):
            self.curr_subject_id = ''
            self.curr_obs = []

    data_stats = DataStats()

    def write_current_observations():
        dn = os.path.join(output_path, str(data_stats.curr_subject_id))
        try:
            os.makedirs(dn)
        except:
            pass
        fn = os.path.join(dn, 'events.csv')
        if not os.path.exists(fn) or not os.path.isfile(fn):
            f = open(fn, 'w')
            f.write(','.join(obs_header) + '\n')
            f.close()
        w = csv.DictWriter(open(fn, 'a'), fieldnames=obs_header, quoting=csv.QUOTE_MINIMAL)
        w.writerows(data_stats.curr_obs)
        data_stats.curr_obs = []

    nb_rows_dict = {'chartevents': 330712484, 'labevents': 27854056, 'outputevents': 4349219}
    nb_rows = nb_rows_dict[table.lower()]

    for row, row_no, _ in tqdm(read_events_table_by_row(mimic3_path, table), total=nb_rows,
                                                        desc='Processing {} table'.format(table)):

        if (subjects_to_keep is not None) and (row['SUBJECT_ID'] not in subjects_to_keep):
            continue
        if (items_to_keep is not None) and (row['ITEMID'] not in items_to_keep):
            continue

        row_out = {'SUBJECT_ID': row['SUBJECT_ID'],
                   'HADM_ID': row['HADM_ID'],
                   'ICUSTAY_ID': '' if 'ICUSTAY_ID' not in row else row['ICUSTAY_ID'],
                   'CHARTTIME': row['CHARTTIME'],
                   'ITEMID': row['ITEMID'],
                   'VALUE': row['VALUE'],
                   'VALUEUOM': row['VALUEUOM']}
        if data_stats.curr_subject_id != '' and data_stats.curr_subject_id != row['SUBJECT_ID']:
            write_current_observations()
        data_stats.curr_obs.append(row_out)
        data_stats.curr_subject_id = row['SUBJECT_ID']

    if data_stats.curr_subject_id != '':
        write_current_observations()


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\preprocessing.py ###
import numpy as np
import re

from pandas import DataFrame, Series

from mimic3benchmark.util import dataframe_from_csv

###############################
# Non-time series preprocessing
###############################

g_map = {'F': 1, 'M': 2, 'OTHER': 3, '': 0}


def transform_gender(gender_series):
    global g_map
    return {'Gender': gender_series.fillna('').apply(lambda s: g_map[s] if s in g_map else g_map['OTHER'])}


e_map = {'ASIAN': 1,
         'BLACK': 2,
         'CARIBBEAN ISLAND': 2,
         'HISPANIC': 3,
         'SOUTH AMERICAN': 3,
         'WHITE': 4,
         'MIDDLE EASTERN': 4,
         'PORTUGUESE': 4,
         'AMERICAN INDIAN': 0,
         'NATIVE HAWAIIAN': 0,
         'UNABLE TO OBTAIN': 0,
         'PATIENT DECLINED TO ANSWER': 0,
         'UNKNOWN': 0,
         'OTHER': 0,
         '': 0}


def transform_ethnicity(ethnicity_series):
    global e_map

    def aggregate_ethnicity(ethnicity_str):
        return ethnicity_str.replace(' OR ', '/').split(' - ')[0].split('/')[0]

    ethnicity_series = ethnicity_series.apply(aggregate_ethnicity)
    return {'Ethnicity': ethnicity_series.fillna('').apply(lambda s: e_map[s] if s in e_map else e_map['OTHER'])}


def assemble_episodic_data(stays, diagnoses):
    data = {'Icustay': stays.ICUSTAY_ID, 'Age': stays.AGE, 'Length of Stay': stays.LOS,
            'Mortality': stays.MORTALITY}
    data.update(transform_gender(stays.GENDER))
    data.update(transform_ethnicity(stays.ETHNICITY))
    data['Height'] = np.nan
    data['Weight'] = np.nan
    data = DataFrame(data).set_index('Icustay')
    data = data[['Ethnicity', 'Gender', 'Age', 'Height', 'Weight', 'Length of Stay', 'Mortality']]
    return data.merge(extract_diagnosis_labels(diagnoses), left_index=True, right_index=True)


diagnosis_labels = ['4019', '4280', '41401', '42731', '25000', '5849', '2724', '51881', '53081', '5990', '2720',
                    '2859', '2449', '486', '2762', '2851', '496', 'V5861', '99592', '311', '0389', '5859', '5070',
                    '40390', '3051', '412', 'V4581', '2761', '41071', '2875', '4240', 'V1582', 'V4582', 'V5867',
                    '4241', '40391', '78552', '5119', '42789', '32723', '49390', '9971', '2767', '2760', '2749',
                    '4168', '5180', '45829', '4589', '73300', '5845', '78039', '5856', '4271', '4254', '4111',
                    'V1251', '30000', '3572', '60000', '27800', '41400', '2768', '4439', '27651', 'V4501', '27652',
                    '99811', '431', '28521', '2930', '7907', 'E8798', '5789', '79902', 'V4986', 'V103', '42832',
                    'E8788', '00845', '5715', '99591', '07054', '42833', '4275', '49121', 'V1046', '2948', '70703',
                    '2809', '5712', '27801', '42732', '99812', '4139', '3004', '2639', '42822', '25060', 'V1254',
                    '42823', '28529', 'E8782', '30500', '78791', '78551', 'E8889', '78820', '34590', '2800', '99859',
                    'V667', 'E8497', '79092', '5723', '3485', '5601', '25040', '570', '71590', '2869', '2763', '5770',
                    'V5865', '99662', '28860', '36201', '56210']


def extract_diagnosis_labels(diagnoses):
    global diagnosis_labels
    diagnoses['VALUE'] = 1
    labels = diagnoses[['ICUSTAY_ID', 'ICD9_CODE', 'VALUE']].drop_duplicates()\
                      .pivot(index='ICUSTAY_ID', columns='ICD9_CODE', values='VALUE').fillna(0).astype(int)
    for l in diagnosis_labels:
        if l not in labels:
            labels[l] = 0
    labels = labels[diagnosis_labels]
    return labels.rename(dict(zip(diagnosis_labels, ['Diagnosis ' + d for d in diagnosis_labels])), axis=1)


def add_hcup_ccs_2015_groups(diagnoses, definitions):
    def_map = {}
    for dx in definitions:
        for code in definitions[dx]['codes']:
            def_map[code] = (dx, definitions[dx]['use_in_benchmark'])
    diagnoses['HCUP_CCS_2015'] = diagnoses.ICD9_CODE.apply(lambda c: def_map[c][0] if c in def_map else None)
    diagnoses['USE_IN_BENCHMARK'] = diagnoses.ICD9_CODE.apply(lambda c: int(def_map[c][1]) if c in def_map else None)
    return diagnoses


def make_phenotype_label_matrix(phenotypes, stays=None):
    phenotypes = phenotypes[['ICUSTAY_ID', 'HCUP_CCS_2015']].loc[phenotypes.USE_IN_BENCHMARK > 0].drop_duplicates()
    phenotypes['VALUE'] = 1
    phenotypes = phenotypes.pivot(index='ICUSTAY_ID', columns='HCUP_CCS_2015', values='VALUE')
    if stays is not None:
        phenotypes = phenotypes.reindex(stays.ICUSTAY_ID.sort_values())
    return phenotypes.fillna(0).astype(int).sort_index(axis=0).sort_index(axis=1)


###################################
# Time series preprocessing
###################################

def read_itemid_to_variable_map(fn, variable_column='LEVEL2'):
    var_map = dataframe_from_csv(fn, index_col=None).fillna('').astype(str)
    # var_map[variable_column] = var_map[variable_column].apply(lambda s: s.lower())
    var_map.COUNT = var_map.COUNT.astype(int)
    var_map = var_map[(var_map[variable_column] != '') & (var_map.COUNT > 0)]
    var_map = var_map[(var_map.STATUS == 'ready')]
    var_map.ITEMID = var_map.ITEMID.astype(int)
    var_map = var_map[[variable_column, 'ITEMID', 'MIMIC LABEL']].set_index('ITEMID')
    return var_map.rename({variable_column: 'VARIABLE', 'MIMIC LABEL': 'MIMIC_LABEL'}, axis=1)


def map_itemids_to_variables(events, var_map):
    return events.merge(var_map, left_on='ITEMID', right_index=True)


def read_variable_ranges(fn, variable_column='LEVEL2'):
    columns = [variable_column, 'OUTLIER LOW', 'VALID LOW', 'IMPUTE', 'VALID HIGH', 'OUTLIER HIGH']
    to_rename = dict(zip(columns, [c.replace(' ', '_') for c in columns]))
    to_rename[variable_column] = 'VARIABLE'
    var_ranges = dataframe_from_csv(fn, index_col=None)
    # var_ranges = var_ranges[variable_column].apply(lambda s: s.lower())
    var_ranges = var_ranges[columns]
    var_ranges.rename(to_rename, axis=1, inplace=True)
    var_ranges = var_ranges.drop_duplicates(subset='VARIABLE', keep='first')
    var_ranges.set_index('VARIABLE', inplace=True)
    return var_ranges.loc[var_ranges.notnull().all(axis=1)]


def remove_outliers_for_variable(events, variable, ranges):
    if variable not in ranges.index:
        return events
    idx = (events.VARIABLE == variable)
    v = events.VALUE[idx].copy()
    v.loc[v < ranges.OUTLIER_LOW[variable]] = np.nan
    v.loc[v > ranges.OUTLIER_HIGH[variable]] = np.nan
    v.loc[v < ranges.VALID_LOW[variable]] = ranges.VALID_LOW[variable]
    v.loc[v > ranges.VALID_HIGH[variable]] = ranges.VALID_HIGH[variable]
    events.loc[idx, 'VALUE'] = v
    return events


# SBP: some are strings of type SBP/DBP
def clean_sbp(df):
    v = df.VALUE.astype(str).copy()
    idx = v.apply(lambda s: '/' in s)
    v.loc[idx] = v[idx].apply(lambda s: re.match('^(\d+)/(\d+)$', s).group(1))
    return v.astype(float)


def clean_dbp(df):
    v = df.VALUE.astype(str).copy()
    idx = v.apply(lambda s: '/' in s)
    v.loc[idx] = v[idx].apply(lambda s: re.match('^(\d+)/(\d+)$', s).group(2))
    return v.astype(float)


# CRR: strings with brisk, <3 normal, delayed, or >3 abnormal
def clean_crr(df):
    v = Series(np.zeros(df.shape[0]), index=df.index)
    v[:] = np.nan

    # when df.VALUE is empty, dtype can be float and comparision with string
    # raises an exception, to fix this we change dtype to str
    df_value_str = df.VALUE.astype(str)

    v.loc[(df_value_str == 'Normal <3 secs') | (df_value_str == 'Brisk')] = 0
    v.loc[(df_value_str == 'Abnormal >3 secs') | (df_value_str == 'Delayed')] = 1
    return v


# FIO2: many 0s, some 0<x<0.2 or 1<x<20
def clean_fio2(df):
    v = df.VALUE.astype(float).copy()

    ''' The line below is the correct way of doing the cleaning, since we will not compare 'str' to 'float'.
    If we use that line it will create mismatches from the data of the paper in ~50 ICU stays.
    The next releases of the benchmark should use this line.
    '''
    # idx = df.VALUEUOM.fillna('').apply(lambda s: 'torr' not in s.lower()) & (v>1.0)

    ''' The line below was used to create the benchmark dataset that the paper used. Note this line will not work
    in python 3, since it may try to compare 'str' to 'float'.
    '''
    # idx = df.VALUEUOM.fillna('').apply(lambda s: 'torr' not in s.lower()) & (df.VALUE > 1.0)

    ''' The two following lines implement the code that was used to create the benchmark dataset that the paper used.
    This works with both python 2 and python 3.
    '''
    is_str = np.array(map(lambda x: type(x) == str, list(df.VALUE)), dtype=bool)
    idx = df.VALUEUOM.fillna('').apply(lambda s: 'torr' not in s.lower()) & (is_str | (~is_str & (v > 1.0)))

    v.loc[idx] = v[idx] / 100.
    return v


# GLUCOSE, PH: sometimes have ERROR as value
def clean_lab(df):
    v = df.VALUE.copy()
    idx = v.apply(lambda s: type(s) is str and not re.match('^(\d+(\.\d*)?|\.\d+)$', s))
    v.loc[idx] = np.nan
    return v.astype(float)


# O2SAT: small number of 0<x<=1 that should be mapped to 0-100 scale
def clean_o2sat(df):
    # change "ERROR" to NaN
    v = df.VALUE.copy()
    idx = v.apply(lambda s: type(s) is str and not re.match('^(\d+(\.\d*)?|\.\d+)$', s))
    v.loc[idx] = np.nan

    v = v.astype(float)
    idx = (v <= 1)
    v.loc[idx] = v[idx] * 100.
    return v


# Temperature: map Farenheit to Celsius, some ambiguous 50<x<80
def clean_temperature(df):
    v = df.VALUE.astype(float).copy()
    idx = df.VALUEUOM.fillna('').apply(lambda s: 'F' in s.lower()) | df.MIMIC_LABEL.apply(lambda s: 'F' in s.lower()) | (v >= 79)
    v.loc[idx] = (v[idx] - 32) * 5. / 9
    return v


# Weight: some really light/heavy adults: <50 lb, >450 lb, ambiguous oz/lb
# Children are tough for height, weight
def clean_weight(df):
    v = df.VALUE.astype(float).copy()
    # ounces
    idx = df.VALUEUOM.fillna('').apply(lambda s: 'oz' in s.lower()) | df.MIMIC_LABEL.apply(lambda s: 'oz' in s.lower())
    v.loc[idx] = v[idx] / 16.
    # pounds
    idx = idx | df.VALUEUOM.fillna('').apply(lambda s: 'lb' in s.lower()) | df.MIMIC_LABEL.apply(lambda s: 'lb' in s.lower())
    v.loc[idx] = v[idx] * 0.453592
    return v


# Height: some really short/tall adults: <2 ft, >7 ft)
# Children are tough for height, weight
def clean_height(df):
    v = df.VALUE.astype(float).copy()
    idx = df.VALUEUOM.fillna('').apply(lambda s: 'in' in s.lower()) | df.MIMIC_LABEL.apply(lambda s: 'in' in s.lower())
    v.loc[idx] = np.round(v[idx] * 2.54)
    return v


# ETCO2: haven't found yet
# Urine output: ambiguous units (raw ccs, ccs/kg/hr, 24-hr, etc.)
# Tidal volume: tried to substitute for ETCO2 but units are ambiguous
# Glascow coma scale eye opening
# Glascow coma scale motor response
# Glascow coma scale total
# Glascow coma scale verbal response
# Heart Rate
# Respiratory rate
# Mean blood pressure
clean_fns = {
    'Capillary refill rate': clean_crr,
    'Diastolic blood pressure': clean_dbp,
    'Systolic blood pressure': clean_sbp,
    'Fraction inspired oxygen': clean_fio2,
    'Oxygen saturation': clean_o2sat,
    'Glucose': clean_lab,
    'pH': clean_lab,
    'Temperature': clean_temperature,
    'Weight': clean_weight,
    'Height': clean_height
}


def clean_events(events):
    global clean_fns
    for var_name, clean_fn in clean_fns.items():
        idx = (events.VARIABLE == var_name)
        try:
            events.loc[idx, 'VALUE'] = clean_fn(events[idx])
        except Exception as e:
            import traceback
            print("Exception in clean_events:", clean_fn.__name__, e)
            print(traceback.format_exc())
            print("number of rows:", np.sum(idx))
            print("values:", events[idx])
            exit()
    return events.loc[events.VALUE.notnull()]


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\readers.py ###
import os
import numpy as np
import random


class Reader(object):
    def __init__(self, dataset_dir, listfile=None):
        self._dataset_dir = dataset_dir
        self._current_index = 0
        if listfile is None:
            listfile_path = os.path.join(dataset_dir, "listfile.csv")
        else:
            listfile_path = listfile
        with open(listfile_path, "r") as lfile:
            self._data = lfile.readlines()
        self._listfile_header = self._data[0]
        self._data = self._data[1:]

    def get_number_of_examples(self):
        return len(self._data)

    def random_shuffle(self, seed=None):
        if seed is not None:
            random.seed(seed)
        random.shuffle(self._data)

    def read_example(self, index):
        raise NotImplementedError()

    def read_next(self):
        to_read_index = self._current_index
        self._current_index += 1
        if self._current_index == self.get_number_of_examples():
            self._current_index = 0
        return self.read_example(to_read_index)


class DecompensationReader(Reader):
    def __init__(self, dataset_dir, listfile=None):
        """ Reader for decompensation prediction task.
        :param dataset_dir: Directory where timeseries files are stored.
        :param listfile:    Path to a listfile. If this parameter is left `None` then
                            `dataset_dir/listfile.csv` will be used.
        """
        Reader.__init__(self, dataset_dir, listfile)
        self._data = [line.split(',') for line in self._data]
        self._data = [(x, float(t), int(y)) for (x, t, y) in self._data]

    def _read_timeseries(self, ts_filename, time_bound):
        ret = []
        with open(os.path.join(self._dataset_dir, ts_filename), "r") as tsfile:
            header = tsfile.readline().strip().split(',')
            assert header[0] == "Hours"
            for line in tsfile:
                mas = line.strip().split(',')
                t = float(mas[0])
                if t > time_bound + 1e-6:
                    break
                ret.append(np.array(mas))
        return (np.stack(ret), header)

    def read_example(self, index):
        """ Read the example with given index.

        :param index: Index of the line of the listfile to read (counting starts from 0).
        :return: Directory with the following keys:
            X : np.array
                2D array containing all events. Each row corresponds to a moment.
                First column is the time and other columns correspond to different
                variables.
            t : float
                Length of the data in hours. Note, in general, it is not equal to the
                timestamp of last event.
            y : int (0 or 1)
                Mortality within next 24 hours.
            header : array of strings
                Names of the columns. The ordering of the columns is always the same.
            name: Name of the sample.
        """
        if index < 0 or index >= len(self._data):
            raise ValueError("Index must be from 0 (inclusive) to number of examples (exclusive).")

        name = self._data[index][0]
        t = self._data[index][1]
        y = self._data[index][2]
        (X, header) = self._read_timeseries(name, t)

        return {"X": X,
                "t": t,
                "y": y,
                "header": header,
                "name": name}


class InHospitalMortalityReader(Reader):
    def __init__(self, dataset_dir, listfile=None, period_length=48.0):
        """ Reader for in-hospital moratality prediction task.

        :param dataset_dir:   Directory where timeseries files are stored.
        :param listfile:      Path to a listfile. If this parameter is left `None` then
                              `dataset_dir/listfile.csv` will be used.
        :param period_length: Length of the period (in hours) from which the prediction is done.
        """
        Reader.__init__(self, dataset_dir, listfile)
        self._data = [line.split(',') for line in self._data]
        self._data = [(x, int(y)) for (x, y) in self._data]
        self._period_length = period_length

    def _read_timeseries(self, ts_filename):
        ret = []
        with open(os.path.join(self._dataset_dir, ts_filename), "r") as tsfile:
            header = tsfile.readline().strip().split(',')
            assert header[0] == "Hours"
            for line in tsfile:
                mas = line.strip().split(',')
                ret.append(np.array(mas))
        return (np.stack(ret), header)

    def read_example(self, index):
        """ Reads the example with given index.

        :param index: Index of the line of the listfile to read (counting starts from 0).
        :return: Dictionary with the following keys:
            X : np.array
                2D array containing all events. Each row corresponds to a moment.
                First column is the time and other columns correspond to different
                variables.
            t : float
                Length of the data in hours. Note, in general, it is not equal to the
                timestamp of last event.
            y : int (0 or 1)
                In-hospital mortality.
            header : array of strings
                Names of the columns. The ordering of the columns is always the same.
            name: Name of the sample.
        """
        if index < 0 or index >= len(self._data):
            raise ValueError("Index must be from 0 (inclusive) to number of lines (exclusive).")

        name = self._data[index][0]
        t = self._period_length
        y = self._data[index][1]
        (X, header) = self._read_timeseries(name)

        return {"X": X,
                "t": t,
                "y": y,
                "header": header,
                "name": name}


class LengthOfStayReader(Reader):
    def __init__(self, dataset_dir, listfile=None):
        """ Reader for length of stay prediction task.

        :param dataset_dir: Directory where timeseries files are stored.
        :param listfile:    Path to a listfile. If this parameter is left `None` then
                            `dataset_dir/listfile.csv` will be used.
        """
        Reader.__init__(self, dataset_dir, listfile)
        self._data = [line.split(',') for line in self._data]
        self._data = [(x, float(t), float(y)) for (x, t, y) in self._data]

    def _read_timeseries(self, ts_filename, time_bound):
        ret = []
        with open(os.path.join(self._dataset_dir, ts_filename), "r") as tsfile:
            header = tsfile.readline().strip().split(',')
            assert header[0] == "Hours"
            for line in tsfile:
                mas = line.strip().split(',')
                t = float(mas[0])
                if t > time_bound + 1e-6:
                    break
                ret.append(np.array(mas))
        return (np.stack(ret), header)

    def read_example(self, index):
        """ Reads the example with given index.

        :param index: Index of the line of the listfile to read (counting starts from 0).
        :return: Dictionary with the following keys:
            X : np.array
                2D array containing all events. Each row corresponds to a moment.
                First column is the time and other columns correspond to different
                variables.
            t : float
                Length of the data in hours. Note, in general, it is not equal to the
                timestamp of last event.
            y : float
                Remaining time in ICU.
            header : array of strings
                Names of the columns. The ordering of the columns is always the same.
            name: Name of the sample.
        """
        if index < 0 or index >= len(self._data):
            raise ValueError("Index must be from 0 (inclusive) to number of lines (exclusive).")

        name = self._data[index][0]
        t = self._data[index][1]
        y = self._data[index][2]
        (X, header) = self._read_timeseries(name, t)

        return {"X": X,
                "t": t,
                "y": y,
                "header": header,
                "name": name}


class PhenotypingReader(Reader):
    def __init__(self, dataset_dir, listfile=None):
        """ Reader for phenotype classification task.

        :param dataset_dir: Directory where timeseries files are stored.
        :param listfile:    Path to a listfile. If this parameter is left `None` then
                            `dataset_dir/listfile.csv` will be used.
        """
        Reader.__init__(self, dataset_dir, listfile)
        self._data = [line.split(',') for line in self._data]
        self._data = [(mas[0], float(mas[1]), list(map(int, mas[2:]))) for mas in self._data]

    def _read_timeseries(self, ts_filename):
        ret = []
        with open(os.path.join(self._dataset_dir, ts_filename), "r") as tsfile:
            header = tsfile.readline().strip().split(',')
            assert header[0] == "Hours"
            for line in tsfile:
                mas = line.strip().split(',')
                ret.append(np.array(mas))
        return (np.stack(ret), header)

    def read_example(self, index):
        """ Reads the example with given index.

        :param index: Index of the line of the listfile to read (counting starts from 0).
        :return: Dictionary with the following keys:
            X : np.array
                2D array containing all events. Each row corresponds to a moment.
                First column is the time and other columns correspond to different
                variables.
            t : float
                Length of the data in hours. Note, in general, it is not equal to the
                timestamp of last event.
            y : array of ints
                Phenotype labels.
            header : array of strings
                Names of the columns. The ordering of the columns is always the same.
            name: Name of the sample.
        """
        if index < 0 or index >= len(self._data):
            raise ValueError("Index must be from 0 (inclusive) to number of lines (exclusive).")

        name = self._data[index][0]
        t = self._data[index][1]
        y = self._data[index][2]
        (X, header) = self._read_timeseries(name)

        return {"X": X,
                "t": t,
                "y": y,
                "header": header,
                "name": name}


class MultitaskReader(Reader):
    def __init__(self, dataset_dir, listfile=None):
        """ Reader for multitask learning.

        :param dataset_dir: Directory where timeseries files are stored.
        :param listfile:    Path to a listfile. If this parameter is left `None` then
                            `dataset_dir/listfile.csv` will be used.
        """
        Reader.__init__(self, dataset_dir, listfile)
        self._data = [line.split(',') for line in self._data]

        def process_ihm(x):
            return list(map(int, x.split(';')))

        def process_los(x):
            x = x.split(';')
            if x[0] == '':
                return ([], [])
            return (list(map(int, x[:len(x)//2])), list(map(float, x[len(x)//2:])))

        def process_ph(x):
            return list(map(int, x.split(';')))

        def process_decomp(x):
            x = x.split(';')
            if x[0] == '':
                return ([], [])
            return (list(map(int, x[:len(x)//2])), list(map(int, x[len(x)//2:])))

        self._data = [(fname, float(t), process_ihm(ihm), process_los(los),
                       process_ph(pheno), process_decomp(decomp))
                      for fname, t, ihm, los, pheno, decomp in self._data]

    def _read_timeseries(self, ts_filename):
        ret = []
        with open(os.path.join(self._dataset_dir, ts_filename), "r") as tsfile:
            header = tsfile.readline().strip().split(',')
            assert header[0] == "Hours"
            for line in tsfile:
                mas = line.strip().split(',')
                ret.append(np.array(mas))
        return (np.stack(ret), header)

    def read_example(self, index):
        """ Reads the example with given index.

        :param index: Index of the line of the listfile to read (counting starts from 0).
        :return: Return dictionary with the following keys:
            X : np.array
                2D array containing all events. Each row corresponds to a moment.
                First column is the time and other columns correspond to different
                variables.
            t : float
                Length of the data in hours. Note, in general, it is not equal to the
                timestamp of last event.
            ihm : array
                Array of 3 integers: [pos, mask, label].
            los : array
                Array of 2 arrays: [masks, labels].
            pheno : array
                Array of 25 binary integers (phenotype labels).
            decomp : array
                Array of 2 arrays: [masks, labels].
            header : array of strings
                Names of the columns. The ordering of the columns is always the same.
            name: Name of the sample.
        """
        if index < 0 or index >= len(self._data):
            raise ValueError("Index must be from 0 (inclusive) to number of lines (exclusive).")

        name = self._data[index][0]
        (X, header) = self._read_timeseries(name)

        return {"X": X,
                "t": self._data[index][1],
                "ihm": self._data[index][2],
                "los": self._data[index][3],
                "pheno": self._data[index][4],
                "decomp": self._data[index][5],
                "header": header,
                "name": name}


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\subject.py ###
import numpy as np
import os
import pandas as pd

from mimic3benchmark.util import dataframe_from_csv


def read_stays(subject_path):
    stays = dataframe_from_csv(os.path.join(subject_path, 'stays.csv'), index_col=None)
    stays.INTIME = pd.to_datetime(stays.INTIME)
    stays.OUTTIME = pd.to_datetime(stays.OUTTIME)
    stays.DOB = pd.to_datetime(stays.DOB)
    stays.DOD = pd.to_datetime(stays.DOD)
    stays.DEATHTIME = pd.to_datetime(stays.DEATHTIME)
    stays.sort_values(by=['INTIME', 'OUTTIME'], inplace=True)
    return stays


def read_diagnoses(subject_path):
    return dataframe_from_csv(os.path.join(subject_path, 'diagnoses.csv'), index_col=None)


def read_events(subject_path, remove_null=True):
    events = dataframe_from_csv(os.path.join(subject_path, 'events.csv'), index_col=None)
    if remove_null:
        events = events[events.VALUE.notnull()]
    events.CHARTTIME = pd.to_datetime(events.CHARTTIME)
    events.HADM_ID = events.HADM_ID.fillna(value=-1).astype(int)
    events.ICUSTAY_ID = events.ICUSTAY_ID.fillna(value=-1).astype(int)
    events.VALUEUOM = events.VALUEUOM.fillna('').astype(str)
    # events.sort_values(by=['CHARTTIME', 'ITEMID', 'ICUSTAY_ID'], inplace=True)
    return events


def get_events_for_stay(events, icustayid, intime=None, outtime=None):
    idx = (events.ICUSTAY_ID == icustayid)
    if intime is not None and outtime is not None:
        idx = idx | ((events.CHARTTIME >= intime) & (events.CHARTTIME <= outtime))
    events = events[idx]
    del events['ICUSTAY_ID']
    return events


def add_hours_elpased_to_events(events, dt, remove_charttime=True):
    events = events.copy()
    events['HOURS'] = (events.CHARTTIME - dt).apply(lambda s: s / np.timedelta64(1, 's')) / 60./60
    if remove_charttime:
        del events['CHARTTIME']
    return events


def convert_events_to_timeseries(events, variable_column='VARIABLE', variables=[]):
    metadata = events[['CHARTTIME', 'ICUSTAY_ID']].sort_values(by=['CHARTTIME', 'ICUSTAY_ID'])\
                    .drop_duplicates(keep='first').set_index('CHARTTIME')
    timeseries = events[['CHARTTIME', variable_column, 'VALUE']]\
                    .sort_values(by=['CHARTTIME', variable_column, 'VALUE'], axis=0)\
                    .drop_duplicates(subset=['CHARTTIME', variable_column], keep='last')
    timeseries = timeseries.pivot(index='CHARTTIME', columns=variable_column, values='VALUE')\
                    .merge(metadata, left_index=True, right_index=True)\
                    .sort_index(axis=0).reset_index()
    for v in variables:
        if v not in timeseries:
            timeseries[v] = np.nan
    return timeseries


def get_first_valid_from_timeseries(timeseries, variable):
    if variable in timeseries:
        idx = timeseries[variable].notnull()
        if idx.any():
            loc = np.where(idx)[0][0]
            return timeseries[variable].iloc[loc]
    return np.nan


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\util.py ###
import pandas as pd


def dataframe_from_csv(path, header=0, index_col=0):
    return pd.read_csv(path, header=header, index_col=index_col)


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\__init__.py ###
import mimic3benchmark.mimic3csv
import mimic3benchmark.subject
import mimic3benchmark.preprocessing
import mimic3benchmark.util


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\evaluation\evaluate_decomp.py ###
from mimic3models.metrics import print_metrics_binary
import sklearn.utils as sk_utils
import numpy as np
import pandas as pd
import argparse
import json
import os


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('prediction', type=str)
    parser.add_argument('--test_listfile', type=str,
                        default=os.path.join(os.path.dirname(__file__), '../../data/decompensation/test/listfile.csv'))
    parser.add_argument('--n_iters', type=int, default=1000)
    parser.add_argument('--save_file', type=str, default='decomp_results.json')
    args = parser.parse_args()

    pred_df = pd.read_csv(args.prediction, index_col=False, dtype={'period_length': np.float32})
    test_df = pd.read_csv(args.test_listfile, index_col=False, dtype={'period_length': np.float32})

    df = test_df.merge(pred_df, left_on=['stay', 'period_length'], right_on=['stay', 'period_length'],
                       how='left', suffixes=['_l', '_r'])
    assert (df['prediction'].isnull().sum() == 0)
    assert (df['y_true_l'].equals(df['y_true_r']))

    metrics = [('AUC of ROC', 'auroc'),
               ('AUC of PRC', 'auprc'),
               ('min(+P, Se)', 'minpse')]

    data = np.zeros((df.shape[0], 2))
    data[:, 0] = np.array(df['prediction'])
    data[:, 1] = np.array(df['y_true_l'])

    results = dict()
    results['n_iters'] = args.n_iters
    ret = print_metrics_binary(data[:, 1], data[:, 0], verbose=0)
    for (m, k) in metrics:
        results[m] = dict()
        results[m]['value'] = ret[k]
        results[m]['runs'] = []

    for i in range(args.n_iters):
        cur_data = sk_utils.resample(data, n_samples=len(data))
        ret = print_metrics_binary(cur_data[:, 1], cur_data[:, 0], verbose=0)
        for (m, k) in metrics:
            results[m]['runs'].append(ret[k])

    for (m, k) in metrics:
        runs = results[m]['runs']
        results[m]['mean'] = np.mean(runs)
        results[m]['median'] = np.median(runs)
        results[m]['std'] = np.std(runs)
        results[m]['2.5% percentile'] = np.percentile(runs, 2.5)
        results[m]['97.5% percentile'] = np.percentile(runs, 97.5)
        del results[m]['runs']

    print("Saving the results in {} ...".format(args.save_file))
    with open(args.save_file, 'w') as f:
        json.dump(results, f)

    print(results)


if __name__ == "__main__":
        main()


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\evaluation\evaluate_ihm.py ###
from mimic3models.metrics import print_metrics_binary
import sklearn.utils as sk_utils
import numpy as np
import pandas as pd
import argparse
import json
import os


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('prediction', type=str)
    parser.add_argument('--test_listfile', type=str,
                        default=os.path.join(os.path.dirname(__file__),
                                             '../../data/in-hospital-mortality/test/listfile.csv'))
    parser.add_argument('--n_iters', type=int, default=10000)
    parser.add_argument('--save_file', type=str, default='ihm_results.json')
    args = parser.parse_args()

    pred_df = pd.read_csv(args.prediction, index_col=False)
    test_df = pd.read_csv(args.test_listfile, index_col=False)

    df = test_df.merge(pred_df, left_on='stay', right_on='stay', how='left', suffixes=['_l', '_r'])
    assert (df['prediction'].isnull().sum() == 0)
    assert (df['y_true_l'].equals(df['y_true_r']))

    metrics = [('AUC of ROC', 'auroc'),
               ('AUC of PRC', 'auprc'),
               ('min(+P, Se)', 'minpse')]

    data = np.zeros((df.shape[0], 2))
    data[:, 0] = np.array(df['prediction'])
    data[:, 1] = np.array(df['y_true_l'])

    results = dict()
    results['n_iters'] = args.n_iters
    ret = print_metrics_binary(data[:, 1], data[:, 0], verbose=0)
    for (m, k) in metrics:
        results[m] = dict()
        results[m]['value'] = ret[k]
        results[m]['runs'] = []

    for i in range(args.n_iters):
        cur_data = sk_utils.resample(data, n_samples=len(data))
        ret = print_metrics_binary(cur_data[:, 1], cur_data[:, 0], verbose=0)
        for (m, k) in metrics:
            results[m]['runs'].append(ret[k])

    for (m, k) in metrics:
        runs = results[m]['runs']
        results[m]['mean'] = np.mean(runs)
        results[m]['median'] = np.median(runs)
        results[m]['std'] = np.std(runs)
        results[m]['2.5% percentile'] = np.percentile(runs, 2.5)
        results[m]['97.5% percentile'] = np.percentile(runs, 97.5)
        del results[m]['runs']

    print("Saving the results in {} ...".format(args.save_file))
    with open(args.save_file, 'w') as f:
        json.dump(results, f)

    print(results)


if __name__ == "__main__":
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\evaluation\evaluate_los.py ###
from mimic3models.metrics import print_metrics_regression
import sklearn.utils as sk_utils
import numpy as np
import pandas as pd
import argparse
import json
import os


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('prediction', type=str)
    parser.add_argument('--test_listfile', type=str,
                        default=os.path.join(os.path.dirname(__file__), '../../data/length-of-stay/test/listfile.csv'))
    parser.add_argument('--n_iters', type=int, default=1000)
    parser.add_argument('--save_file', type=str, default='los_results.json')
    args = parser.parse_args()

    pred_df = pd.read_csv(args.prediction, index_col=False, dtype={'period_length': np.float32,
                                                                   'y_true': np.float32})
    test_df = pd.read_csv(args.test_listfile, index_col=False, dtype={'period_length': np.float32,
                                                                   'y_true': np.float32})

    df = test_df.merge(pred_df, left_on=['stay', 'period_length'], right_on=['stay', 'period_length'],
                       how='left', suffixes=['_l', '_r'])
    assert (df['prediction'].isnull().sum() == 0)
    assert (df['y_true_l'].equals(df['y_true_r']))

    metrics = [('Kappa', 'kappa'),
               ('MAD', 'mad'),
               ('MSE', 'mse'),
               ('MAPE', 'mape')]

    data = np.zeros((df.shape[0], 2))
    data[:, 0] = np.array(df['prediction'])
    data[:, 1] = np.array(df['y_true_l'])

    results = dict()
    results['n_iters'] = args.n_iters
    ret = print_metrics_regression(data[:, 1], data[:, 0], verbose=0)
    for (m, k) in metrics:
        results[m] = dict()
        results[m]['value'] = ret[k]
        results[m]['runs'] = []

    for i in range(args.n_iters):
        cur_data = sk_utils.resample(data, n_samples=len(data))
        ret = print_metrics_regression(cur_data[:, 1], cur_data[:, 0], verbose=0)
        for (m, k) in metrics:
            results[m]['runs'].append(ret[k])

    for (m, k) in metrics:
        runs = results[m]['runs']
        results[m]['mean'] = np.mean(runs)
        results[m]['median'] = np.median(runs)
        results[m]['std'] = np.std(runs)
        results[m]['2.5% percentile'] = np.percentile(runs, 2.5)
        results[m]['97.5% percentile'] = np.percentile(runs, 97.5)
        del results[m]['runs']

    print("Saving the results in {} ...".format(args.save_file))
    with open(args.save_file, 'w') as f:
        json.dump(results, f)

    print(results)


if __name__ == "__main__":
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\evaluation\evaluate_pheno.py ###
from mimic3models.metrics import print_metrics_multilabel, print_metrics_binary
import sklearn.utils as sk_utils
import numpy as np
import pandas as pd
import argparse
import json
import os


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('prediction', type=str)
    parser.add_argument('--test_listfile', type=str,
                        default=os.path.join(os.path.dirname(__file__), '../../data/phenotyping/test/listfile.csv'))
    parser.add_argument('--n_iters', type=int, default=10000)
    parser.add_argument('--save_file', type=str, default='pheno_results.json')
    args = parser.parse_args()

    pred_df = pd.read_csv(args.prediction, index_col=False, dtype={'period_length': np.float32})
    test_df = pd.read_csv(args.test_listfile, index_col=False, dtype={'period_length': np.float32})

    n_tasks = 25
    labels_cols = ["label_{}".format(i) for i in range(1, n_tasks + 1)]
    test_df.columns = list(test_df.columns[:2]) + labels_cols

    df = test_df.merge(pred_df, left_on='stay', right_on='stay', how='left', suffixes=['_l', '_r'])
    assert (df['pred_1'].isnull().sum() == 0)
    assert (df['period_length_l'].equals(df['period_length_r']))
    for i in range(1, n_tasks + 1):
        assert (df['label_{}_l'.format(i)].equals(df['label_{}_r'.format(i)]))

    metrics = [('Macro ROC AUC', 'ave_auc_macro'),
               ('Micro ROC AUC', 'ave_auc_micro'),
               ('Weighted ROC AUC', 'ave_auc_weighted')]

    data = np.zeros((df.shape[0], 50))
    for i in range(1, n_tasks + 1):
        data[:, i - 1] = df['pred_{}'.format(i)]
        data[:, 25 + i - 1] = df['label_{}_l'.format(i)]

    results = dict()
    results['n_iters'] = args.n_iters
    ret = print_metrics_multilabel(data[:, 25:], data[:, :25], verbose=0)
    for (m, k) in metrics:
        results[m] = dict()
        results[m]['value'] = ret[k]
        results[m]['runs'] = []

    for i in range(1, n_tasks + 1):
        m = 'ROC AUC of task {}'.format(i)
        results[m] = dict()
        results[m]['value'] = print_metrics_binary(data[:, 25 + i - 1], data[:, i - 1], verbose=0)['auroc']
        results[m]['runs'] = []

    for iteration in range(args.n_iters):
        cur_data = sk_utils.resample(data, n_samples=len(data))
        ret = print_metrics_multilabel(cur_data[:, 25:], cur_data[:, :25], verbose=0)
        for (m, k) in metrics:
            results[m]['runs'].append(ret[k])
        for i in range(1, n_tasks + 1):
            m = 'ROC AUC of task {}'.format(i)
            cur_auc = print_metrics_binary(cur_data[:, 25 + i - 1], cur_data[:, i - 1], verbose=0)['auroc']
            results[m]['runs'].append(cur_auc)

    reported_metrics = [m for m, k in metrics]
    reported_metrics += ['ROC AUC of task {}'.format(i) for i in range(1, n_tasks + 1)]

    for m in reported_metrics:
        runs = results[m]['runs']
        results[m]['mean'] = np.mean(runs)
        results[m]['median'] = np.median(runs)
        results[m]['std'] = np.std(runs)
        results[m]['2.5% percentile'] = np.percentile(runs, 2.5)
        results[m]['97.5% percentile'] = np.percentile(runs, 97.5)
        del results[m]['runs']

    print("Saving the results (including task specific metrics) in {} ...".format(args.save_file))
    with open(args.save_file, 'w') as f:
        json.dump(results, f)

    print("Printing the summary of results (task specific metrics are skipped) ...")
    for i in range(1, n_tasks + 1):
        m = 'ROC AUC of task {}'.format(i)
        del results[m]
    print(results)


if __name__ == "__main__":
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\evaluation\__init__.py ###


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\scripts\create_decompensation.py ###
import os
import argparse
import numpy as np
import pandas as pd
import random
random.seed(49297)
from tqdm import tqdm


def process_partition(args, partition, sample_rate=1.0, shortest_length=4.0,
                      eps=1e-6, future_time_interval=24.0):

    output_dir = os.path.join(args.output_path, partition)
    if not os.path.exists(output_dir):
        os.mkdir(output_dir)

    xty_triples = []
    patients = list(filter(str.isdigit, os.listdir(os.path.join(args.root_path, partition))))
    for patient in tqdm(patients, desc='Iterating over patients in {}'.format(partition)):
        patient_folder = os.path.join(args.root_path, partition, patient)
        patient_ts_files = list(filter(lambda x: x.find("timeseries") != -1, os.listdir(patient_folder)))
        stays_df = pd.read_csv(os.path.join(patient_folder, "stays.csv"))

        for ts_filename in patient_ts_files:
            with open(os.path.join(patient_folder, ts_filename)) as tsfile:
                lb_filename = ts_filename.replace("_timeseries", "")
                label_df = pd.read_csv(os.path.join(patient_folder, lb_filename))

                # empty label file
                if label_df.shape[0] == 0:
                    continue

                mortality = int(label_df.iloc[0]["Mortality"])

                los = 24.0 * label_df.iloc[0]['Length of Stay']  # in hours
                if pd.isnull(los):
                    print("(length of stay is missing)", patient, ts_filename)
                    continue

                stay = stays_df[stays_df.ICUSTAY_ID == label_df.iloc[0]['Icustay']]
                deathtime = pd.to_datetime(stay['DEATHTIME'].iloc[0])
                intime = pd.to_datetime(stay['INTIME'].iloc[0])
                if pd.isnull(deathtime):
                    lived_time = 1e18
                else:
                    # conversion to pydatetime is needed to avoid overflow issues when subtracting
                    lived_time = (deathtime.to_pydatetime() - intime.to_pydatetime()).total_seconds() / 3600.0

                ts_lines = tsfile.readlines()
                header = ts_lines[0]
                ts_lines = ts_lines[1:]
                event_times = [float(line.split(',')[0]) for line in ts_lines]

                ts_lines = [line for (line, t) in zip(ts_lines, event_times)
                            if -eps < t < los + eps]
                event_times = [t for t in event_times
                               if -eps < t < los + eps]

                # no measurements in ICU
                if len(ts_lines) == 0:
                    print("(no events in ICU) ", patient, ts_filename)
                    continue

                sample_times = np.arange(0.0, min(los, lived_time) + eps, sample_rate)

                sample_times = list(filter(lambda x: x > shortest_length, sample_times))

                # At least one measurement
                sample_times = list(filter(lambda x: x > event_times[0], sample_times))

                output_ts_filename = patient + "_" + ts_filename
                with open(os.path.join(output_dir, output_ts_filename), "w") as outfile:
                    outfile.write(header)
                    for line in ts_lines:
                        outfile.write(line)

                for t in sample_times:
                    if mortality == 0:
                        cur_mortality = 0
                    else:
                        cur_mortality = int(lived_time - t < future_time_interval)
                    xty_triples.append((output_ts_filename, t, cur_mortality))

    print("Number of created samples:", len(xty_triples))
    if partition == "train":
        random.shuffle(xty_triples)
    if partition == "test":
        xty_triples = sorted(xty_triples)

    with open(os.path.join(output_dir, "listfile.csv"), "w") as listfile:
        listfile.write('stay,period_length,y_true\n')
        for (x, t, y) in xty_triples:
            listfile.write('{},{:.6f},{:d}\n'.format(x, t, y))


def main():
    parser = argparse.ArgumentParser(description="Create data for decompensation prediction task.")
    parser.add_argument('root_path', type=str, help="Path to root folder containing train and test sets.")
    parser.add_argument('output_path', type=str, help="Directory where the created data should be stored.")
    args, _ = parser.parse_known_args()

    if not os.path.exists(args.output_path):
        os.makedirs(args.output_path)

    process_partition(args, "test")
    process_partition(args, "train")


if __name__ == '__main__':
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\scripts\create_in_hospital_mortality.py ###
import os
import argparse
import pandas as pd
import random
random.seed(49297)
from tqdm import tqdm


def process_partition(args, partition, eps=1e-6, n_hours=48):
    output_dir = os.path.join(args.output_path, partition)
    if not os.path.exists(output_dir):
        os.mkdir(output_dir)

    xy_pairs = []
    patients = list(filter(str.isdigit, os.listdir(os.path.join(args.root_path, partition))))
    for patient in tqdm(patients, desc='Iterating over patients in {}'.format(partition)):
        patient_folder = os.path.join(args.root_path, partition, patient)
        patient_ts_files = list(filter(lambda x: x.find("timeseries") != -1, os.listdir(patient_folder)))

        for ts_filename in patient_ts_files:
            with open(os.path.join(patient_folder, ts_filename)) as tsfile:
                lb_filename = ts_filename.replace("_timeseries", "")
                label_df = pd.read_csv(os.path.join(patient_folder, lb_filename))

                # empty label file
                if label_df.shape[0] == 0:
                    continue

                mortality = int(label_df.iloc[0]["Mortality"])
                los = 24.0 * label_df.iloc[0]['Length of Stay']  # in hours
                if pd.isnull(los):
                    print("\n\t(length of stay is missing)", patient, ts_filename)
                    continue

                if los < n_hours - eps:
                    continue

                ts_lines = tsfile.readlines()
                header = ts_lines[0]
                ts_lines = ts_lines[1:]
                event_times = [float(line.split(',')[0]) for line in ts_lines]

                ts_lines = [line for (line, t) in zip(ts_lines, event_times)
                            if -eps < t < n_hours + eps]

                # no measurements in ICU
                if len(ts_lines) == 0:
                    print("\n\t(no events in ICU) ", patient, ts_filename)
                    continue

                output_ts_filename = patient + "_" + ts_filename
                with open(os.path.join(output_dir, output_ts_filename), "w") as outfile:
                    outfile.write(header)
                    for line in ts_lines:
                        outfile.write(line)

                xy_pairs.append((output_ts_filename, mortality))

    print("Number of created samples:", len(xy_pairs))
    if partition == "train":
        random.shuffle(xy_pairs)
    if partition == "test":
        xy_pairs = sorted(xy_pairs)

    with open(os.path.join(output_dir, "listfile.csv"), "w") as listfile:
        listfile.write('stay,y_true\n')
        for (x, y) in xy_pairs:
            listfile.write('{},{:d}\n'.format(x, y))


def main():
    parser = argparse.ArgumentParser(description="Create data for in-hospital mortality prediction task.")
    parser.add_argument('root_path', type=str, help="Path to root folder containing train and test sets.")
    parser.add_argument('output_path', type=str, help="Directory where the created data should be stored.")
    args, _ = parser.parse_known_args()

    if not os.path.exists(args.output_path):
        os.makedirs(args.output_path)

    process_partition(args, "test")
    process_partition(args, "train")


if __name__ == '__main__':
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\scripts\create_length_of_stay.py ###
import os
import argparse
import numpy as np
import pandas as pd
import random
random.seed(49297)
from tqdm import tqdm


def process_partition(args, partition, sample_rate=1.0, shortest_length=4.0, eps=1e-6):
    output_dir = os.path.join(args.output_path, partition)
    if not os.path.exists(output_dir):
        os.mkdir(output_dir)

    xty_triples = []
    patients = list(filter(str.isdigit, os.listdir(os.path.join(args.root_path, partition))))
    for patient in tqdm(patients, desc='Iterating over patients in {}'.format(partition)):
        patient_folder = os.path.join(args.root_path, partition, patient)
        patient_ts_files = list(filter(lambda x: x.find("timeseries") != -1, os.listdir(patient_folder)))

        for ts_filename in patient_ts_files:
            with open(os.path.join(patient_folder, ts_filename)) as tsfile:
                lb_filename = ts_filename.replace("_timeseries", "")
                label_df = pd.read_csv(os.path.join(patient_folder, lb_filename))

                # empty label file
                if label_df.shape[0] == 0:
                    print("\n\t(empty label file)", patient, ts_filename)
                    continue

                los = 24.0 * label_df.iloc[0]['Length of Stay']  # in hours
                if pd.isnull(los):
                    print("\n\t(length of stay is missing)", patient, ts_filename)
                    continue

                ts_lines = tsfile.readlines()
                header = ts_lines[0]
                ts_lines = ts_lines[1:]
                event_times = [float(line.split(',')[0]) for line in ts_lines]

                ts_lines = [line for (line, t) in zip(ts_lines, event_times)
                            if -eps < t < los + eps]
                event_times = [t for t in event_times
                               if -eps < t < los + eps]

                # no measurements in ICU
                if len(ts_lines) == 0:
                    print("\n\t(no events in ICU) ", patient, ts_filename)
                    continue

                sample_times = np.arange(0.0, los + eps, sample_rate)

                sample_times = list(filter(lambda x: x > shortest_length, sample_times))

                # At least one measurement
                sample_times = list(filter(lambda x: x > event_times[0], sample_times))

                output_ts_filename = patient + "_" + ts_filename
                with open(os.path.join(output_dir, output_ts_filename), "w") as outfile:
                    outfile.write(header)
                    for line in ts_lines:
                        outfile.write(line)

                for t in sample_times:
                    xty_triples.append((output_ts_filename, t, los - t))

    print("Number of created samples:", len(xty_triples))
    if partition == "train":
        random.shuffle(xty_triples)
    if partition == "test":
        xty_triples = sorted(xty_triples)

    with open(os.path.join(output_dir, "listfile.csv"), "w") as listfile:
        listfile.write('stay,period_length,y_true\n')
        for (x, t, y) in xty_triples:
            listfile.write('{},{:.6f},{:.6f}\n'.format(x, t, y))


def main():
    parser = argparse.ArgumentParser(description="Create data for length of stay prediction task.")
    parser.add_argument('root_path', type=str, help="Path to root folder containing train and test sets.")
    parser.add_argument('output_path', type=str, help="Directory where the created data should be stored.")
    args, _ = parser.parse_known_args()

    if not os.path.exists(args.output_path):
        os.makedirs(args.output_path)

    process_partition(args, "test")
    process_partition(args, "train")


if __name__ == '__main__':
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\scripts\create_multitask.py ###
import os
import argparse
import numpy as np
import pandas as pd
import yaml
import random
random.seed(49297)
from tqdm import tqdm


def process_partition(args, definitions, code_to_group, id_to_group, group_to_id,
                      partition, sample_rate=1.0, shortest_length=4,
                      eps=1e-6, future_time_interval=24.0, fixed_hours=48.0):

    output_dir = os.path.join(args.output_path, partition)
    if not os.path.exists(output_dir):
        os.mkdir(output_dir)

    file_names = []
    loses = []

    ihm_masks = []
    ihm_labels = []
    ihm_positions = []

    los_masks = []
    los_labels = []

    phenotype_labels = []

    decomp_masks = []
    decomp_labels = []

    patients = list(filter(str.isdigit, os.listdir(os.path.join(args.root_path, partition))))

    for patient in tqdm(patients, desc='Iterating over patients in {}'.format(partition)):
        patient_folder = os.path.join(args.root_path, partition, patient)
        patient_ts_files = list(filter(lambda x: x.find("timeseries") != -1, os.listdir(patient_folder)))
        stays_df = pd.read_csv(os.path.join(patient_folder, "stays.csv"))

        for ts_filename in patient_ts_files:
            with open(os.path.join(patient_folder, ts_filename)) as ts_file:
                lb_filename = ts_filename.replace("_timeseries", "")
                label_df = pd.read_csv(os.path.join(patient_folder, lb_filename))

                # empty label file, skip globally
                if label_df.shape[0] == 0:
                    print("\n\t(empty label file)", patient, ts_filename)
                    continue

                # find length of stay, skip globally if it is missing
                los = 24.0 * label_df.iloc[0]['Length of Stay']  # in hours
                if pd.isnull(los):
                    print("\n\t(length of stay is missing)", patient, ts_filename)
                    continue

                # find all event in ICU, skip globally if there is no event in ICU
                ts_lines = ts_file.readlines()
                header = ts_lines[0]
                ts_lines = ts_lines[1:]
                event_times = [float(line.split(',')[0]) for line in ts_lines]
                ts_lines = [line for (line, t) in zip(ts_lines, event_times)
                            if -eps < t < los + eps]
                event_times = [t for t in event_times
                               if -eps < t < los + eps]

                if len(ts_lines) == 0:
                    print("\n\t(no events in ICU) ", patient, ts_filename)
                    continue

                # add length of stay
                loses.append(los)

                # find in hospital mortality
                mortality = int(label_df.iloc[0]["Mortality"])

                # write episode data and add file name
                output_ts_filename = patient + "_" + ts_filename
                with open(os.path.join(output_dir, output_ts_filename), "w") as outfile:
                    outfile.write(header)
                    for line in ts_lines:
                        outfile.write(line)
                file_names.append(output_ts_filename)

                # create in-hospital mortality
                ihm_label = mortality

                ihm_mask = 1
                if los < fixed_hours - eps:
                    ihm_mask = 0
                if event_times[0] > fixed_hours + eps:
                    ihm_mask = 0

                ihm_position = 47
                if ihm_mask == 0:
                    ihm_position = 0

                ihm_masks.append(ihm_mask)
                ihm_labels.append(ihm_label)
                ihm_positions.append(ihm_position)

                # create length of stay
                sample_times = np.arange(0.0, los + eps, sample_rate)
                sample_times = np.array([int(x+eps) for x in sample_times])
                cur_los_masks = map(int, (sample_times > shortest_length) & (sample_times > event_times[0]))
                cur_los_labels = los - sample_times

                los_masks.append(cur_los_masks)
                los_labels.append(cur_los_labels)

                # create phenotyping
                cur_phenotype_labels = [0 for i in range(len(id_to_group))]
                icustay = label_df['Icustay'].iloc[0]
                diagnoses_df = pd.read_csv(os.path.join(patient_folder, "diagnoses.csv"), dtype={"ICD9_CODE": str})
                diagnoses_df = diagnoses_df[diagnoses_df.ICUSTAY_ID == icustay]

                for index, row in diagnoses_df.iterrows():
                    if row['USE_IN_BENCHMARK']:
                        code = row['ICD9_CODE']
                        group = code_to_group[code]
                        group_id = group_to_id[group]
                        cur_phenotype_labels[group_id] = 1

                cur_phenotype_labels = [x for (i, x) in enumerate(cur_phenotype_labels)
                                        if definitions[id_to_group[i]]['use_in_benchmark']]
                phenotype_labels.append(cur_phenotype_labels)

                # create decompensation
                stay = stays_df[stays_df.ICUSTAY_ID == icustay]
                deathtime = pd.to_datetime(stay['DEATHTIME'].iloc[0])
                intime = pd.to_datetime(stay['INTIME'].iloc[0])
                if pd.isnull(deathtime):
                    lived_time = 1e18
                else:
                    # conversion to pydatetime is needed to avoid overflow issues when subtracting
                    lived_time = (deathtime.to_pydatetime() - intime.to_pydatetime()).total_seconds() / 3600.0

                sample_times = np.arange(0.0, min(los, lived_time) + eps, sample_rate)
                sample_times = np.array([int(x+eps) for x in sample_times])
                cur_decomp_masks = map(int, (sample_times > shortest_length) & (sample_times > event_times[0]))
                cur_decomp_labels = [(mortality & int(lived_time - t < future_time_interval))
                                     for t in sample_times]
                decomp_masks.append(cur_decomp_masks)
                decomp_labels.append(cur_decomp_labels)

    def permute(arr, p):
        return [arr[index] for index in p]

    if partition == "train":
        perm = list(range(len(file_names)))
        random.shuffle(perm)
    if partition == "test":
        perm = list(np.argsort(file_names))

    file_names = permute(file_names, perm)
    loses = permute(loses, perm)

    ihm_masks = permute(ihm_masks, perm)
    ihm_labels = permute(ihm_labels, perm)
    ihm_positions = permute(ihm_positions, perm)

    los_masks = permute(los_masks, perm)
    los_labels = permute(los_labels, perm)

    phenotype_labels = permute(phenotype_labels, perm)

    decomp_masks = permute(decomp_masks, perm)
    decomp_labels = permute(decomp_labels, perm)

    with open(os.path.join(output_dir, "listfile.csv"), "w") as listfile:
        header = ','.join(['filename', 'length of stay', 'in-hospital mortality task (pos;mask;label)',
                           'length of stay task (masks;labels)', 'phenotyping task (labels)',
                           'decompensation task (masks;labels)'])
        listfile.write(header + "\n")

        for index in range(len(file_names)):
            file_name = file_names[index]
            los = '{:.6f}'.format(loses[index])

            ihm_task = '{:d};{:d};{:d}'.format(ihm_positions[index], ihm_masks[index], ihm_labels[index])

            ls1 = ";".join(map(str, los_masks[index]))
            ls2 = ";".join(map(lambda x: '{:.6f}'.format(x), los_labels[index]))
            los_task = '{};{}'.format(ls1, ls2)

            pheno_task = ';'.join(map(str, phenotype_labels[index]))

            dec1 = ";".join(map(str, decomp_masks[index]))
            dec2 = ";".join(map(str, decomp_labels[index]))
            decomp_task = '{};{}'.format(dec1, dec2)

            listfile.write(','.join([file_name, los, ihm_task, los_task, pheno_task, decomp_task]) + "\n")


def main():
    parser = argparse.ArgumentParser(description="Create data for multitask prediction.")
    parser.add_argument('root_path', type=str, help="Path to root folder containing train and test sets.")
    parser.add_argument('output_path', type=str, help="Directory where the created data should be stored.")
    parser.add_argument('--phenotype_definitions', '-p', type=str,
                        default=os.path.join(os.path.dirname(__file__), '../resources/hcup_ccs_2015_definitions.yaml'),
                        help='YAML file with phenotype definitions.')
    args, _ = parser.parse_known_args()

    with open(args.phenotype_definitions) as definitions_file:
        definitions = yaml.safe_load(definitions_file)

    code_to_group = {}
    for group in definitions:
        codes = definitions[group]['codes']
        for code in codes:
            if code not in code_to_group:
                code_to_group[code] = group
            else:
                assert code_to_group[code] == group

    id_to_group = sorted(definitions.keys())
    group_to_id = dict((x, i) for (i, x) in enumerate(id_to_group))

    if not os.path.exists(args.output_path):
        os.makedirs(args.output_path)

    process_partition(args, definitions, code_to_group, id_to_group, group_to_id, "test")
    process_partition(args, definitions, code_to_group, id_to_group, group_to_id, "train")


if __name__ == '__main__':
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\scripts\create_phenotyping.py ###
import os
import argparse
import pandas as pd
import yaml
import random
random.seed(49297)
from tqdm import tqdm


def process_partition(args, definitions, code_to_group, id_to_group, group_to_id,
                      partition, eps=1e-6):
    output_dir = os.path.join(args.output_path, partition)
    if not os.path.exists(output_dir):
        os.mkdir(output_dir)

    xty_triples = []
    patients = list(filter(str.isdigit, os.listdir(os.path.join(args.root_path, partition))))
    for patient in tqdm(patients, desc='Iterating over patients in {}'.format(partition)):
        patient_folder = os.path.join(args.root_path, partition, patient)
        patient_ts_files = list(filter(lambda x: x.find("timeseries") != -1, os.listdir(patient_folder)))

        for ts_filename in patient_ts_files:
            with open(os.path.join(patient_folder, ts_filename)) as tsfile:
                lb_filename = ts_filename.replace("_timeseries", "")
                label_df = pd.read_csv(os.path.join(patient_folder, lb_filename))

                # empty label file
                if label_df.shape[0] == 0:
                    continue

                los = 24.0 * label_df.iloc[0]['Length of Stay']  # in hours
                if pd.isnull(los):
                    print("\n\t(length of stay is missing)", patient, ts_filename)
                    continue

                ts_lines = tsfile.readlines()
                header = ts_lines[0]
                ts_lines = ts_lines[1:]
                event_times = [float(line.split(',')[0]) for line in ts_lines]

                ts_lines = [line for (line, t) in zip(ts_lines, event_times)
                            if -eps < t < los + eps]

                # no measurements in ICU
                if len(ts_lines) == 0:
                    print("\n\t(no events in ICU) ", patient, ts_filename)
                    continue

                output_ts_filename = patient + "_" + ts_filename
                with open(os.path.join(output_dir, output_ts_filename), "w") as outfile:
                    outfile.write(header)
                    for line in ts_lines:
                        outfile.write(line)

                cur_labels = [0 for i in range(len(id_to_group))]

                icustay = label_df['Icustay'].iloc[0]
                diagnoses_df = pd.read_csv(os.path.join(patient_folder, "diagnoses.csv"),
                                           dtype={"ICD9_CODE": str})
                diagnoses_df = diagnoses_df[diagnoses_df.ICUSTAY_ID == icustay]
                for index, row in diagnoses_df.iterrows():
                    if row['USE_IN_BENCHMARK']:
                        code = row['ICD9_CODE']
                        group = code_to_group[code]
                        group_id = group_to_id[group]
                        cur_labels[group_id] = 1

                cur_labels = [x for (i, x) in enumerate(cur_labels)
                              if definitions[id_to_group[i]]['use_in_benchmark']]

                xty_triples.append((output_ts_filename, los, cur_labels))

    print("Number of created samples:", len(xty_triples))
    if partition == "train":
        random.shuffle(xty_triples)
    if partition == "train":
        xty_triples = sorted(xty_triples)

    codes_in_benchmark = [x for x in id_to_group
                          if definitions[x]['use_in_benchmark']]

    listfile_header = "stay,period_length," + ",".join(codes_in_benchmark)
    with open(os.path.join(output_dir, "listfile.csv"), "w") as listfile:
        listfile.write(listfile_header + "\n")
        for (x, t, y) in xty_triples:
            labels = ','.join(map(str, y))
            listfile.write('{},{:.6f},{}\n'.format(x, t, labels))


def main():
    parser = argparse.ArgumentParser(description="Create data for phenotype classification task.")
    parser.add_argument('root_path', type=str, help="Path to root folder containing train and test sets.")
    parser.add_argument('output_path', type=str, help="Directory where the created data should be stored.")
    parser.add_argument('--phenotype_definitions', '-p', type=str,
                        default=os.path.join(os.path.dirname(__file__), '../resources/hcup_ccs_2015_definitions.yaml'),
                        help='YAML file with phenotype definitions.')
    args, _ = parser.parse_known_args()

    with open(args.phenotype_definitions) as definitions_file:
        definitions = yaml.safe_load(definitions_file)

    code_to_group = {}
    for group in definitions:
        codes = definitions[group]['codes']
        for code in codes:
            if code not in code_to_group:
                code_to_group[code] = group
            else:
                assert code_to_group[code] == group

    id_to_group = sorted(definitions.keys())
    group_to_id = dict((x, i) for (i, x) in enumerate(id_to_group))

    if not os.path.exists(args.output_path):
        os.makedirs(args.output_path)

    process_partition(args, definitions, code_to_group, id_to_group, group_to_id, "test")
    process_partition(args, definitions, code_to_group, id_to_group, group_to_id, "train")


if __name__ == '__main__':
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\scripts\extract_episodes_from_subjects.py ###
import argparse
import os
import sys
from tqdm import tqdm

from mimic3benchmark.subject import read_stays, read_diagnoses, read_events, get_events_for_stay,\
    add_hours_elpased_to_events
from mimic3benchmark.subject import convert_events_to_timeseries, get_first_valid_from_timeseries
from mimic3benchmark.preprocessing import read_itemid_to_variable_map, map_itemids_to_variables, clean_events
from mimic3benchmark.preprocessing import assemble_episodic_data


parser = argparse.ArgumentParser(description='Extract episodes from per-subject data.')
parser.add_argument('subjects_root_path', type=str, help='Directory containing subject sub-directories.')
parser.add_argument('--variable_map_file', type=str,
                    default=os.path.join(os.path.dirname(__file__), '../resources/itemid_to_variable_map.csv'),
                    help='CSV containing ITEMID-to-VARIABLE map.')
parser.add_argument('--reference_range_file', type=str,
                    default=os.path.join(os.path.dirname(__file__), '../resources/variable_ranges.csv'),
                    help='CSV containing reference ranges for VARIABLEs.')
args, _ = parser.parse_known_args()

var_map = read_itemid_to_variable_map(args.variable_map_file)
variables = var_map.VARIABLE.unique()

for subject_dir in tqdm(os.listdir(args.subjects_root_path), desc='Iterating over subjects'):
    dn = os.path.join(args.subjects_root_path, subject_dir)
    try:
        subject_id = int(subject_dir)
        if not os.path.isdir(dn):
            raise Exception
    except:
        continue

    try:
        # reading tables of this subject
        stays = read_stays(os.path.join(args.subjects_root_path, subject_dir))
        diagnoses = read_diagnoses(os.path.join(args.subjects_root_path, subject_dir))
        events = read_events(os.path.join(args.subjects_root_path, subject_dir))
    except:
        sys.stderr.write('Error reading from disk for subject: {}\n'.format(subject_id))
        continue

    episodic_data = assemble_episodic_data(stays, diagnoses)

    # cleaning and converting to time series
    events = map_itemids_to_variables(events, var_map)
    events = clean_events(events)
    if events.shape[0] == 0:
        # no valid events for this subject
        continue
    timeseries = convert_events_to_timeseries(events, variables=variables)

    # extracting separate episodes
    for i in range(stays.shape[0]):
        stay_id = stays.ICUSTAY_ID.iloc[i]
        intime = stays.INTIME.iloc[i]
        outtime = stays.OUTTIME.iloc[i]

        episode = get_events_for_stay(timeseries, stay_id, intime, outtime)
        if episode.shape[0] == 0:
            # no data for this episode
            continue

        episode = add_hours_elpased_to_events(episode, intime).set_index('HOURS').sort_index(axis=0)
        if stay_id in episodic_data.index:
            episodic_data.loc[stay_id, 'Weight'] = get_first_valid_from_timeseries(episode, 'Weight')
            episodic_data.loc[stay_id, 'Height'] = get_first_valid_from_timeseries(episode, 'Height')
        episodic_data.loc[episodic_data.index == stay_id].to_csv(os.path.join(args.subjects_root_path, subject_dir,
                                                                              'episode{}.csv'.format(i+1)),
                                                                 index_label='Icustay')
        columns = list(episode.columns)
        columns_sorted = sorted(columns, key=(lambda x: "" if x == "Hours" else x))
        episode = episode[columns_sorted]
        episode.to_csv(os.path.join(args.subjects_root_path, subject_dir, 'episode{}_timeseries.csv'.format(i+1)),
                       index_label='Hours')


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\scripts\extract_subjects.py ###
import argparse
import yaml

from mimic3benchmark.mimic3csv import *
from mimic3benchmark.preprocessing import add_hcup_ccs_2015_groups, make_phenotype_label_matrix
from mimic3benchmark.util import dataframe_from_csv

parser = argparse.ArgumentParser(description='Extract per-subject data from MIMIC-III CSV files.')
parser.add_argument('mimic3_path', type=str, help='Directory containing MIMIC-III CSV files.')
parser.add_argument('output_path', type=str, help='Directory where per-subject data should be written.')
parser.add_argument('--event_tables', '-e', type=str, nargs='+', help='Tables from which to read events.',
                    default=['CHARTEVENTS', 'LABEVENTS', 'OUTPUTEVENTS'])
parser.add_argument('--phenotype_definitions', '-p', type=str,
                    default=os.path.join(os.path.dirname(__file__), '../resources/hcup_ccs_2015_definitions.yaml'),
                    help='YAML file with phenotype definitions.')
parser.add_argument('--itemids_file', '-i', type=str, help='CSV containing list of ITEMIDs to keep.')
parser.add_argument('--verbose', '-v', dest='verbose', action='store_true', help='Verbosity in output')
parser.add_argument('--quiet', '-q', dest='verbose', action='store_false', help='Suspend printing of details')
parser.set_defaults(verbose=True)
parser.add_argument('--test', action='store_true', help='TEST MODE: process only 1000 subjects, 1000000 events.')
args, _ = parser.parse_known_args()

try:
    os.makedirs(args.output_path)
except:
    pass

patients = read_patients_table(args.mimic3_path)
admits = read_admissions_table(args.mimic3_path)
stays = read_icustays_table(args.mimic3_path)
if args.verbose:
    print('START:\n\tICUSTAY_IDs: {}\n\tHADM_IDs: {}\n\tSUBJECT_IDs: {}'.format(stays.ICUSTAY_ID.unique().shape[0],
          stays.HADM_ID.unique().shape[0], stays.SUBJECT_ID.unique().shape[0]))

stays = remove_icustays_with_transfers(stays)
if args.verbose:
    print('REMOVE ICU TRANSFERS:\n\tICUSTAY_IDs: {}\n\tHADM_IDs: {}\n\tSUBJECT_IDs: {}'.format(stays.ICUSTAY_ID.unique().shape[0],
          stays.HADM_ID.unique().shape[0], stays.SUBJECT_ID.unique().shape[0]))

stays = merge_on_subject_admission(stays, admits)
stays = merge_on_subject(stays, patients)
stays = filter_admissions_on_nb_icustays(stays)
if args.verbose:
    print('REMOVE MULTIPLE STAYS PER ADMIT:\n\tICUSTAY_IDs: {}\n\tHADM_IDs: {}\n\tSUBJECT_IDs: {}'.format(stays.ICUSTAY_ID.unique().shape[0],
          stays.HADM_ID.unique().shape[0], stays.SUBJECT_ID.unique().shape[0]))

stays = add_age_to_icustays(stays)
stays = add_inunit_mortality_to_icustays(stays)
stays = add_inhospital_mortality_to_icustays(stays)
stays = filter_icustays_on_age(stays)
if args.verbose:
    print('REMOVE PATIENTS AGE < 18:\n\tICUSTAY_IDs: {}\n\tHADM_IDs: {}\n\tSUBJECT_IDs: {}'.format(stays.ICUSTAY_ID.unique().shape[0],
          stays.HADM_ID.unique().shape[0], stays.SUBJECT_ID.unique().shape[0]))

stays.to_csv(os.path.join(args.output_path, 'all_stays.csv'), index=False)
diagnoses = read_icd_diagnoses_table(args.mimic3_path)
diagnoses = filter_diagnoses_on_stays(diagnoses, stays)
diagnoses.to_csv(os.path.join(args.output_path, 'all_diagnoses.csv'), index=False)
count_icd_codes(diagnoses, output_path=os.path.join(args.output_path, 'diagnosis_counts.csv'))

phenotypes = add_hcup_ccs_2015_groups(diagnoses, yaml.safe_load(open(args.phenotype_definitions, 'r')))

make_phenotype_label_matrix(phenotypes, stays).to_csv(os.path.join(args.output_path, 'phenotype_labels.csv'),
                                                      index=False, quoting=csv.QUOTE_NONNUMERIC)

if args.test:
    pat_idx = np.random.choice(patients.shape[0], size=1000)
    patients = patients.iloc[pat_idx]
    stays = stays.merge(patients[['SUBJECT_ID']], left_on='SUBJECT_ID', right_on='SUBJECT_ID')
    args.event_tables = [args.event_tables[0]]
    print('Using only', stays.shape[0], 'stays and only', args.event_tables[0], 'table')

subjects = stays.SUBJECT_ID.unique()
break_up_stays_by_subject(stays, args.output_path, subjects=subjects)
break_up_diagnoses_by_subject(phenotypes, args.output_path, subjects=subjects)
items_to_keep = set(
    [int(itemid) for itemid in dataframe_from_csv(args.itemids_file)['ITEMID'].unique()]) if args.itemids_file else None
for table in args.event_tables:
    read_events_table_and_break_up_by_subject(args.mimic3_path, table, args.output_path, items_to_keep=items_to_keep,
                                              subjects_to_keep=subjects)


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\scripts\split_train_and_test.py ###
import os
import shutil
import argparse


def move_to_partition(args, patients, partition):
    if not os.path.exists(os.path.join(args.subjects_root_path, partition)):
        os.mkdir(os.path.join(args.subjects_root_path, partition))
    for patient in patients:
        src = os.path.join(args.subjects_root_path, patient)
        dest = os.path.join(args.subjects_root_path, partition, patient)
        shutil.move(src, dest)


def main():
    parser = argparse.ArgumentParser(description='Split data into train and test sets.')
    parser.add_argument('subjects_root_path', type=str, help='Directory containing subject sub-directories.')
    args, _ = parser.parse_known_args()

    test_set = set()
    with open(os.path.join(os.path.dirname(__file__), '../resources/testset.csv'), "r") as test_set_file:
        for line in test_set_file:
            x, y = line.split(',')
            if int(y) == 1:
                test_set.add(x)

    folders = os.listdir(args.subjects_root_path)
    folders = list((filter(str.isdigit, folders)))
    train_patients = [x for x in folders if x not in test_set]
    test_patients = [x for x in folders if x in test_set]

    assert len(set(train_patients) & set(test_patients)) == 0

    move_to_partition(args, train_patients, "train")
    move_to_partition(args, test_patients, "test")


if __name__ == '__main__':
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\scripts\validate_events.py ###
import os
import argparse
import pandas as pd
from tqdm import tqdm


def is_subject_folder(x):
    return str.isdigit(x)


def main():

    n_events = 0                   # total number of events
    empty_hadm = 0                 # HADM_ID is empty in events.csv. We exclude such events.
    no_hadm_in_stay = 0            # HADM_ID does not appear in stays.csv. We exclude such events.
    no_icustay = 0                 # ICUSTAY_ID is empty in events.csv. We try to fix such events.
    recovered = 0                  # empty ICUSTAY_IDs are recovered according to stays.csv files (given HADM_ID)
    could_not_recover = 0          # empty ICUSTAY_IDs that are not recovered. This should be zero.
    icustay_missing_in_stays = 0   # ICUSTAY_ID does not appear in stays.csv. We exclude such events.

    parser = argparse.ArgumentParser()
    parser.add_argument('subjects_root_path', type=str,
                        help='Directory containing subject subdirectories.')
    args = parser.parse_args()
    print(args)

    subdirectories = os.listdir(args.subjects_root_path)
    subjects = list(filter(is_subject_folder, subdirectories))

    for subject in tqdm(subjects, desc='Iterating over subjects'):
        stays_df = pd.read_csv(os.path.join(args.subjects_root_path, subject, 'stays.csv'), index_col=False,
                               dtype={'HADM_ID': str, "ICUSTAY_ID": str})
        stays_df.columns = stays_df.columns.str.upper()

        # assert that there is no row with empty ICUSTAY_ID or HADM_ID
        assert(not stays_df['ICUSTAY_ID'].isnull().any())
        assert(not stays_df['HADM_ID'].isnull().any())

        # assert there are no repetitions of ICUSTAY_ID or HADM_ID
        # since admissions with multiple ICU stays were excluded
        assert(len(stays_df['ICUSTAY_ID'].unique()) == len(stays_df['ICUSTAY_ID']))
        assert(len(stays_df['HADM_ID'].unique()) == len(stays_df['HADM_ID']))

        events_df = pd.read_csv(os.path.join(args.subjects_root_path, subject, 'events.csv'), index_col=False,
                                dtype={'HADM_ID': str, "ICUSTAY_ID": str})
        events_df.columns = events_df.columns.str.upper()
        n_events += events_df.shape[0]

        # we drop all events for them HADM_ID is empty
        # TODO: maybe we can recover HADM_ID by looking at ICUSTAY_ID
        empty_hadm += events_df['HADM_ID'].isnull().sum()
        events_df = events_df.dropna(subset=['HADM_ID'])

        merged_df = events_df.merge(stays_df, left_on=['HADM_ID'], right_on=['HADM_ID'],
                                    how='left', suffixes=['', '_r'], indicator=True)

        # we drop all events for which HADM_ID is not listed in stays.csv
        # since there is no way to know the targets of that stay (for example mortality)
        no_hadm_in_stay += (merged_df['_merge'] == 'left_only').sum()
        merged_df = merged_df[merged_df['_merge'] == 'both']

        # if ICUSTAY_ID is empty in stays.csv, we try to recover it
        # we exclude all events for which we could not recover ICUSTAY_ID
        cur_no_icustay = merged_df['ICUSTAY_ID'].isnull().sum()
        no_icustay += cur_no_icustay
        merged_df.loc[:, 'ICUSTAY_ID'] = merged_df['ICUSTAY_ID'].fillna(merged_df['ICUSTAY_ID_r'])
        recovered += cur_no_icustay - merged_df['ICUSTAY_ID'].isnull().sum()
        could_not_recover += merged_df['ICUSTAY_ID'].isnull().sum()
        merged_df = merged_df.dropna(subset=['ICUSTAY_ID'])

        # now we take a look at the case when ICUSTAY_ID is present in events.csv, but not in stays.csv
        # this mean that ICUSTAY_ID in events.csv is not the same as that of stays.csv for the same HADM_ID
        # we drop all such events
        icustay_missing_in_stays += (merged_df['ICUSTAY_ID'] != merged_df['ICUSTAY_ID_r']).sum()
        merged_df = merged_df[(merged_df['ICUSTAY_ID'] == merged_df['ICUSTAY_ID_r'])]

        to_write = merged_df[['SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'CHARTTIME', 'ITEMID', 'VALUE', 'VALUEUOM']]
        to_write.to_csv(os.path.join(args.subjects_root_path, subject, 'events.csv'), index=False)

    assert(could_not_recover == 0)
    print('n_events: {}'.format(n_events))
    print('empty_hadm: {}'.format(empty_hadm))
    print('no_hadm_in_stay: {}'.format(no_hadm_in_stay))
    print('no_icustay: {}'.format(no_icustay))
    print('recovered: {}'.format(recovered))
    print('could_not_recover: {}'.format(could_not_recover))
    print('icustay_missing_in_stays: {}'.format(icustay_missing_in_stays))


if __name__ == "__main__":
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\scripts\__init__.py ###


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\tests\compare_dir_with_existing.py ###
from tqdm import tqdm

import os
import argparse
import pandas as pd


def formatter(x):
    try:
        x = float(x)
        return '{:.1f}'.format(x)
    except:
        return x


def get_all_csv_files(dir):
    csv_files = []
    for subdir, _, files in tqdm(os.walk(dir), desc='Getting all csv files'):
        for f in files:
            extension = f.split('.')[-1]
            if extension == 'csv':
                csv_files.append(os.path.join(os.path.relpath(subdir, dir),
                                              f))
    return set(csv_files)


def load_df(path):
    df = pd.read_csv(path, index_col=False)

    if 'AGE' in df.columns:
        df = df.drop('AGE', axis=1)
    if 'Age' in df.columns:
        df = df.drop('Age', axis=1)

    # convert all numbers to floats with fixed precision
    for col in df.columns:
        df[col] = df[col].apply(formatter)

    # sort by the first column that has unique values
    for col in df.columns:
        if len(df[col].unique()) == len(df):
            df = df.sort_values(by=col).reset_index(drop=True)
            break

    return df


def main():
    parser = argparse.ArgumentParser(description='Recursively checks whether all csv files are the same')
    parser.add_argument('--old_directory', type=str, required=True)
    parser.add_argument('--new_directory', type=str, required=True)
    args = parser.parse_args()
    print(args)

    # check that old and new directories contain the same csv files
    old_csv_files = get_all_csv_files(args.old_directory)
    new_csv_files = get_all_csv_files(args.new_directory)

    for s in old_csv_files:
        if s not in new_csv_files:
            print(f'{s} is missing in the new directory')

    for s in new_csv_files:
        if s not in old_csv_files:
            print(f'{s} appears in the new directroy but not in the old one')

    # iterate over all old csv files
    for csv_path in tqdm(old_csv_files, desc='checking csv files'):
        old_full_path = os.path.join(args.old_directory, csv_path)
        new_full_path = os.path.join(args.new_directory, csv_path)

        old_df = load_df(old_full_path)
        new_df = load_df(new_full_path)

        if not new_df.equals(old_df):
            print(f'differences found for {csv_path}')


if __name__ == "__main__":
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\tests\compare_hashes.py ###
import argparse
import pickle


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--old_pkl', type=str, required=True)
    parser.add_argument('--new_pkl', type=str, required=True)
    args = parser.parse_args()

    with open(args.old_pkl, 'rb') as f:
        old = dict(pickle.load(f))

    with open(args.new_pkl, 'rb') as f:
        new = dict(pickle.load(f))

    old_keys = set(old.keys())
    new_keys = set(new.keys())

    for s in old_keys:
        if s not in new_keys:
            print(f'{s} is missing')

    for s in new_keys:
        if s not in old_keys:
            print(f'{s} is extra')

    for s in old_keys:
        if old[s] != new[s]:
            print(f'Mismatch found for {s}')

    print('Finished')


if __name__ == '__main__':
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\tests\compare_listfiles.py ###
import argparse


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--old_listfile', type=str, required=True)
    parser.add_argument('--new_listfile', type=str, required=True)
    parser.add_argument('--sort', dest='sort', action='store_true')
    parser.set_defaults(sort=False)
    args = parser.parse_args()

    with open(args.old_listfile, 'r') as f:
        old_listfile = f.readlines()
    if args.sort:
        old_listfile = sorted(old_listfile)

    with open(args.new_listfile, 'r') as f:
        new_listfile = f.readlines()
    if args.sort:
        new_listfile = sorted(new_listfile)

    assert len(old_listfile) == len(new_listfile)

    for (old, new) in zip(old_listfile, new_listfile):
        if old.strip() != new.strip():
            print('Mismatch found:')
            print('\told:', old)
            print('\tnew:', new)


if __name__ == '__main__':
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\tests\hash_tables.py ###
from tqdm import tqdm
import hashlib
import os
import argparse
import pandas as pd


def formatter(x):
    try:
        x = float(x)
        return '{:.1f}'.format(x)
    except:
        pass

    try:
        x = pd.to_datetime(x)
        return str(x)
    except:
        return x


def main():
    parser = argparse.ArgumentParser(description='Recursively produces hashes for all tables inside this directory')
    parser.add_argument('--directory', '-d', type=str, required=True, help='The directory to hash.')
    parser.add_argument('--output_file', '-o', type=str, default='hashes.csv')
    args = parser.parse_args()
    print(args)

    # count the directories
    total = 0
    for subdir, dirs, files in tqdm(os.walk(args.directory), desc='Counting directories'):
        total += len(files)

    # change directory to args.directory
    initial_dir = os.getcwd()
    os.chdir(args.directory)

    # iterate over all subdirectories
    hashes = []
    pbar = tqdm(total=total, desc='Iterating over files')
    for subdir, dirs, files in os.walk('.'):
        for file in files:
            pbar.update(1)
            # skip files that are not csv
            extension = file.split('.')[-1]
            if extension != 'csv':
                continue

            full_path = os.path.join(subdir, file)
            df = pd.read_csv(full_path, index_col=False)

            # convert all numbers to floats with fixed precision
            for col in df.columns:
                df[col] = df[col].apply(formatter)

            # sort by the first column that has unique values
            for col in df.columns:
                if len(df[col].unique()) == len(df):
                    df = df.sort_values(by=col).reset_index(drop=True)
                    break

            # convert the data frame to string and hash it
            df_str = df.to_string().encode()
            hashcode = hashlib.md5(df_str).hexdigest()
            hashes.append((full_path, hashcode))
    pbar.close()

    # go to the initial directory and save the results
    os.chdir(initial_dir)
    hashes = sorted(hashes)
    with open(args.output_file, 'w') as f:
        for s, h in hashes:
            f.write(f'{s},{h}\n')


if __name__ == "__main__":
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3benchmark\tests\__init__.py ###


### FILE: .\tests\data\mimic3benchmarks\mimic3models\common_utils.py ###
import numpy as np
import os
import json
import random

from mimic3models.feature_extractor import extract_features


def convert_to_dict(data, header, channel_info):
    """ convert data from readers output in to array of arrays format """
    ret = [[] for i in range(data.shape[1] - 1)]
    for i in range(1, data.shape[1]):
        ret[i-1] = [(t, x) for (t, x) in zip(data[:, 0], data[:, i]) if x != ""]
        channel = header[i]
        if len(channel_info[channel]['possible_values']) != 0:
            ret[i-1] = list(map(lambda x: (x[0], channel_info[channel]['values'][x[1]]), ret[i-1]))
        ret[i-1] = list(map(lambda x: (float(x[0]), float(x[1])), ret[i-1]))
    return ret


def extract_features_from_rawdata(chunk, header, period, features):
    with open(os.path.join(os.path.dirname(__file__), "resources/channel_info.json")) as channel_info_file:
        channel_info = json.loads(channel_info_file.read())
    data = [convert_to_dict(X, header, channel_info) for X in chunk]
    return extract_features(data, period, features)


def read_chunk(reader, chunk_size):
    data = {}
    for i in range(chunk_size):
        ret = reader.read_next()
        for k, v in ret.items():
            if k not in data:
                data[k] = []
            data[k].append(v)
    data["header"] = data["header"][0]
    return data


def sort_and_shuffle(data, batch_size):
    """ Sort data by the length and then make batches and shuffle them.
        data is tuple (X1, X2, ..., Xn) all of them have the same length.
        Usually data = (X, y).
    """
    assert len(data) >= 2
    data = list(zip(*data))

    random.shuffle(data)

    old_size = len(data)
    rem = old_size % batch_size
    head = data[:old_size - rem]
    tail = data[old_size - rem:]
    data = []

    head.sort(key=(lambda x: x[0].shape[0]))

    mas = [head[i: i+batch_size] for i in range(0, len(head), batch_size)]
    random.shuffle(mas)

    for x in mas:
        data += x
    data += tail

    data = list(zip(*data))
    return data


def add_common_arguments(parser):
    """ Add all the parameters which are common across the tasks
    """
    parser.add_argument('--network', type=str, required=True)
    parser.add_argument('--dim', type=int, default=256,
                        help='number of hidden units')
    parser.add_argument('--depth', type=int, default=1,
                        help='number of bi-LSTMs')
    parser.add_argument('--epochs', type=int, default=100,
                        help='number of chunks to train')
    parser.add_argument('--load_state', type=str, default="",
                        help='state file path')
    parser.add_argument('--mode', type=str, default="train",
                        help='mode: train or test')
    parser.add_argument('--batch_size', type=int, default=64)
    parser.add_argument('--l2', type=float, default=0, help='L2 regularization')
    parser.add_argument('--l1', type=float, default=0, help='L1 regularization')
    parser.add_argument('--save_every', type=int, default=1,
                        help='save state every x epoch')
    parser.add_argument('--prefix', type=str, default="",
                        help='optional prefix of network name')
    parser.add_argument('--dropout', type=float, default=0.0)
    parser.add_argument('--rec_dropout', type=float, default=0.0,
                        help="dropout rate for recurrent connections")
    parser.add_argument('--batch_norm', type=bool, default=False,
                        help='batch normalization')
    parser.add_argument('--timestep', type=float, default=1.0,
                        help="fixed timestep used in the dataset")
    parser.add_argument('--imputation', type=str, default='previous')
    parser.add_argument('--small_part', dest='small_part', action='store_true')
    parser.add_argument('--whole_data', dest='small_part', action='store_false')
    parser.add_argument('--optimizer', type=str, default='adam')
    parser.add_argument('--lr', type=float, default=0.001, help='learning rate')
    parser.add_argument('--beta_1', type=float, default=0.9,
                        help='beta_1 param for Adam optimizer')
    parser.add_argument('--verbose', type=int, default=2)
    parser.add_argument('--size_coef', type=float, default=4.0)
    parser.add_argument('--normalizer_state', type=str, default=None,
                        help='Path to a state file of a normalizer. Leave none if you want to '
                             'use one of the provided ones.')
    parser.set_defaults(small_part=False)


class DeepSupervisionDataLoader:
    r"""
    Data loader for decompensation and length of stay task.
    Reads all the data for one patient at once.

    Parameters
    ----------
    dataset_dir : str
        Directory where timeseries files are stored.
    listfile : str
        Path to a listfile. If this parameter is left `None` then
        `dataset_dir/listfile.csv` will be used.
    """
    def __init__(self, dataset_dir, listfile=None, small_part=False):

        self._dataset_dir = dataset_dir
        if listfile is None:
            listfile_path = os.path.join(dataset_dir, "listfile.csv")
        else:
            listfile_path = listfile
        with open(listfile_path, "r") as lfile:
            self._data = lfile.readlines()[1:]  # skip the header

        self._data = [line.split(',') for line in self._data]
        self._data = [(x, float(t), y) for (x, t, y) in self._data]
        self._data = sorted(self._data)

        mas = {"X": [],
               "ts": [],
               "ys": [],
               "name": []}
        i = 0
        while i < len(self._data):
            j = i
            cur_stay = self._data[i][0]
            cur_ts = []
            cur_labels = []
            while j < len(self._data) and self._data[j][0] == cur_stay:
                cur_ts.append(self._data[j][1])
                cur_labels.append(self._data[j][2])
                j += 1

            cur_X, header = self._read_timeseries(cur_stay)
            mas["X"].append(cur_X)
            mas["ts"].append(cur_ts)
            mas["ys"].append(cur_labels)
            mas["name"].append(cur_stay)

            i = j
            if small_part and len(mas["name"]) == 256:
                break

        self._data = mas

    def _read_timeseries(self, ts_filename):
        ret = []
        with open(os.path.join(self._dataset_dir, ts_filename), "r") as tsfile:
            header = tsfile.readline().strip().split(',')
            assert header[0] == "Hours"
            for line in tsfile:
                mas = line.strip().split(',')
                ret.append(np.array(mas))
        return (np.stack(ret), header)


def create_directory(directory):
    if not os.path.exists(directory):
        os.makedirs(directory)


def pad_zeros(arr, min_length=None):
    """
    `arr` is an array of `np.array`s

    The function appends zeros to every `np.array` in `arr`
    to equalize their first axis lenghts.
    """
    dtype = arr[0].dtype
    max_len = max([x.shape[0] for x in arr])
    ret = [np.concatenate([x, np.zeros((max_len - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)
           for x in arr]
    if (min_length is not None) and ret[0].shape[0] < min_length:
        ret = [np.concatenate([x, np.zeros((min_length - x.shape[0],) + x.shape[1:], dtype=dtype)], axis=0)
               for x in ret]
    return np.array(ret)


### FILE: .\tests\data\mimic3benchmarks\mimic3models\create_normalizer_state.py ###
from mimic3benchmark.readers import InHospitalMortalityReader
from mimic3benchmark.readers import DecompensationReader
from mimic3benchmark.readers import LengthOfStayReader
from mimic3benchmark.readers import PhenotypingReader
from mimic3benchmark.readers import MultitaskReader
from mimic3models.preprocessing import Discretizer, Normalizer

import os
import argparse


def main():
    parser = argparse.ArgumentParser(description='Script for creating a normalizer state - a file which stores the '
                                                 'means and standard deviations of columns of the output of a '
                                                 'discretizer, which are later used to standardize the input of '
                                                 'neural models.')
    parser.add_argument('--task', type=str, required=True,
                        choices=['ihm', 'decomp', 'los', 'pheno', 'multi'])
    parser.add_argument('--timestep', type=float, default=1.0,
                        help="Rate of the re-sampling to discretize time-series.")
    parser.add_argument('--impute_strategy', type=str, default='previous',
                        choices=['zero', 'next', 'previous', 'normal_value'],
                        help='Strategy for imputing missing values.')
    parser.add_argument('--start_time', type=str, choices=['zero', 'relative'],
                        help='Specifies the start time of discretization. Zero means to use the beginning of '
                             'the ICU stay. Relative means to use the time of the first ICU event')
    parser.add_argument('--store_masks', dest='store_masks', action='store_true',
                        help='Store masks that specify observed/imputed values.')
    parser.add_argument('--no-masks', dest='store_masks', action='store_false',
                        help='Do not store that specify specifying observed/imputed values.')
    parser.add_argument('--n_samples', type=int, default=-1, help='How many samples to use to estimates means and '
                        'standard deviations. Set -1 to use all training samples.')
    parser.add_argument('--output_dir', type=str, help='Directory where the output file will be saved.',
                        default='.')
    parser.add_argument('--data', type=str, required=True, help='Path to the task data.')
    parser.set_defaults(store_masks=True)

    args = parser.parse_args()
    print(args)

    # create the reader
    reader = None
    dataset_dir = os.path.join(args.data, 'train')
    if args.task == 'ihm':
        reader = InHospitalMortalityReader(dataset_dir=dataset_dir, period_length=48.0)
    if args.task == 'decomp':
        reader = DecompensationReader(dataset_dir=dataset_dir)
    if args.task == 'los':
        reader = LengthOfStayReader(dataset_dir=dataset_dir)
    if args.task == 'pheno':
        reader = PhenotypingReader(dataset_dir=dataset_dir)
    if args.task == 'multi':
        reader = MultitaskReader(dataset_dir=dataset_dir)

    # create the discretizer
    discretizer = Discretizer(timestep=args.timestep,
                              store_masks=args.store_masks,
                              impute_strategy=args.impute_strategy,
                              start_time=args.start_time)
    discretizer_header = reader.read_example(0)['header']
    continuous_channels = [i for (i, x) in enumerate(discretizer_header) if x.find("->") == -1]

    # create the normalizer
    normalizer = Normalizer(fields=continuous_channels)

    # read all examples and store the state of the normalizer
    n_samples = args.n_samples
    if n_samples == -1:
        n_samples = reader.get_number_of_examples()

    for i in range(n_samples):
        if i % 1000 == 0:
            print('Processed {} / {} samples'.format(i, n_samples), end='\r')
        ret = reader.read_example(i)
        data, new_header = discretizer.transform(ret['X'], end=ret['t'])
        normalizer._feed_data(data)
    print('\n')

    # all dashes (-) were colons(:)
    file_name = '{}_ts-{:.2f}_impute-{}_start-{}_masks-{}_n-{}.normalizer'.format(
        args.task, args.timestep, args.impute_strategy, args.start_time, args.store_masks, n_samples)
    file_name = os.path.join(args.output_dir, file_name)
    print('Saving the state in {} ...'.format(file_name))
    normalizer._save_params(file_name)


if __name__ == '__main__':
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3models\feature_extractor.py ###
import numpy as np
from scipy.stats import skew

all_functions = [min, max, np.mean, np.std, skew, len]

functions_map = {
    "all": all_functions,
    "len": [len],
    "all_but_len": all_functions[:-1]
}

periods_map = {
    "all": (0, 0, 1, 0),
    "first4days": (0, 0, 0, 4 * 24),
    "first8days": (0, 0, 0, 8 * 24),
    "last12hours": (1, -12, 1, 0),
    "first25percent": (2, 25),
    "first50percent": (2, 50)
}

sub_periods = [(2, 100), (2, 10), (2, 25), (2, 50),
               (3, 10), (3, 25), (3, 50)]


def get_range(begin, end, period):
    # first p %
    if period[0] == 2:
        return (begin, begin + (end - begin) * period[1] / 100.0)
    # last p %
    if period[0] == 3:
        return (end - (end - begin) * period[1] / 100.0, end)

    if period[0] == 0:
        L = begin + period[1]
    else:
        L = end + period[1]

    if period[2] == 0:
        R = begin + period[3]
    else:
        R = end + period[3]

    return (L, R)


def calculate(channel_data, period, sub_period, functions):
    if len(channel_data) == 0:
        return np.full((len(functions, )), np.nan)

    L = channel_data[0][0]
    R = channel_data[-1][0]
    L, R = get_range(L, R, period)
    L, R = get_range(L, R, sub_period)

    data = [x for (t, x) in channel_data
            if L - 1e-6 < t < R + 1e-6]

    if len(data) == 0:
        return np.full((len(functions, )), np.nan)
    return np.array([fn(data) for fn in functions], dtype=np.float32)


def extract_features_single_episode(data_raw, period, functions):
    global sub_periods
    extracted_features = [np.concatenate([calculate(data_raw[i], period, sub_period, functions)
                                          for sub_period in sub_periods],
                                         axis=0)
                          for i in range(len(data_raw))]
    return np.concatenate(extracted_features, axis=0)


def extract_features(data_raw, period, features):
    period = periods_map[period]
    functions = functions_map[features]
    return np.array([extract_features_single_episode(x, period, functions)
                     for x in data_raw])


### FILE: .\tests\data\mimic3benchmarks\mimic3models\keras_utils.py ###
import numpy as np
from mimic3models import metrics

import keras
import keras.backend as K

if K.backend() == 'tensorflow':
    import tensorflow as tf

from keras.layers import Layer


# ===================== METRICS ===================== #


class DecompensationMetrics(keras.callbacks.Callback):
    def __init__(self, train_data_gen, val_data_gen, deep_supervision,
                 batch_size=32, early_stopping=True, verbose=2):
        super(DecompensationMetrics, self).__init__()
        self.train_data_gen = train_data_gen
        self.val_data_gen = val_data_gen
        self.deep_supervision = deep_supervision
        self.batch_size = batch_size
        self.early_stopping = early_stopping
        self.verbose = verbose
        self.train_history = []
        self.val_history = []

    def calc_metrics(self, data_gen, history, dataset, logs):
        y_true = []
        predictions = []
        for i in range(data_gen.steps):
            if self.verbose == 1:
                print("\tdone {}/{}".format(i, data_gen.steps), end='\r')
            (x, y) = next(data_gen)
            pred = self.model.predict(x, batch_size=self.batch_size)
            if self.deep_supervision:
                for m, t, p in zip(x[1].flatten(), y.flatten(), pred.flatten()):
                    if np.equal(m, 1):
                        y_true.append(t)
                        predictions.append(p)
            else:
                y_true += list(y.flatten())
                predictions += list(pred.flatten())
        print('\n')
        predictions = np.array(predictions)
        predictions = np.stack([1 - predictions, predictions], axis=1)
        ret = metrics.print_metrics_binary(y_true, predictions)
        for k, v in ret.items():
            logs[dataset + '_' + k] = v
        history.append(ret)

    def on_epoch_end(self, epoch, logs={}):
        print("\n==>predicting on train")
        self.calc_metrics(self.train_data_gen, self.train_history, 'train', logs)
        print("\n==>predicting on validation")
        self.calc_metrics(self.val_data_gen, self.val_history, 'val', logs)

        if self.early_stopping:
            max_auc = np.max([x["auroc"] for x in self.val_history])
            cur_auc = self.val_history[-1]["auroc"]
            if max_auc > 0.88 and cur_auc < 0.86:
                self.model.stop_training = True


class InHospitalMortalityMetrics(keras.callbacks.Callback):
    def __init__(self, train_data, val_data, target_repl, batch_size=32, early_stopping=True, verbose=2):
        super(InHospitalMortalityMetrics, self).__init__()
        self.train_data = train_data
        self.val_data = val_data
        self.target_repl = target_repl
        self.batch_size = batch_size
        self.early_stopping = early_stopping
        self.verbose = verbose
        self.train_history = []
        self.val_history = []

    def calc_metrics(self, data, history, dataset, logs):
        y_true = []
        predictions = []
        B = self.batch_size
        for i in range(0, len(data[0]), B):
            if self.verbose == 1:
                print("\tdone {}/{}".format(i, len(data[0])), end='\r')
            if self.target_repl:
                (x, y, y_repl) = (data[0][i:i + B], data[1][0][i:i + B], data[1][1][i:i + B])
            else:
                (x, y) = (data[0][i:i + B], data[1][i:i + B])
            outputs = self.model.predict(x, batch_size=B)
            if self.target_repl:
                predictions += list(np.array(outputs[0]).flatten())
            else:
                predictions += list(np.array(outputs).flatten())
            y_true += list(np.array(y).flatten())
        print('\n')
        predictions = np.array(predictions)
        predictions = np.stack([1 - predictions, predictions], axis=1)
        ret = metrics.print_metrics_binary(y_true, predictions)
        for k, v in ret.items():
            logs[dataset + '_' + k] = v
        history.append(ret)

    def on_epoch_end(self, epoch, logs={}):
        print("\n==>predicting on train")
        self.calc_metrics(self.train_data, self.train_history, 'train', logs)
        print("\n==>predicting on validation")
        self.calc_metrics(self.val_data, self.val_history, 'val', logs)

        if self.early_stopping:
            max_auc = np.max([x["auroc"] for x in self.val_history])
            cur_auc = self.val_history[-1]["auroc"]
            if max_auc > 0.85 and cur_auc < 0.83:
                self.model.stop_training = True


class PhenotypingMetrics(keras.callbacks.Callback):
    def __init__(self, train_data_gen, val_data_gen, batch_size=32,
                 early_stopping=True, verbose=2):
        super(PhenotypingMetrics, self).__init__()
        self.train_data_gen = train_data_gen
        self.val_data_gen = val_data_gen
        self.batch_size = batch_size
        self.early_stopping = early_stopping
        self.verbose = verbose
        self.train_history = []
        self.val_history = []

    def calc_metrics(self, data_gen, history, dataset, logs):
        y_true = []
        predictions = []
        for i in range(data_gen.steps):
            if self.verbose == 1:
                print("\tdone {}/{}".format(i, data_gen.steps), end='\r')
            (x, y) = next(data_gen)
            outputs = self.model.predict(x, batch_size=self.batch_size)
            if data_gen.target_repl:
                y_true += list(y[0])
                predictions += list(outputs[0])
            else:
                y_true += list(y)
                predictions += list(outputs)
        print('\n')
        predictions = np.array(predictions)
        ret = metrics.print_metrics_multilabel(y_true, predictions)
        for k, v in ret.items():
            logs[dataset + '_' + k] = v
        history.append(ret)

    def on_epoch_end(self, epoch, logs={}):
        print("\n==>predicting on train")
        self.calc_metrics(self.train_data_gen, self.train_history, 'train', logs)
        print("\n==>predicting on validation")
        self.calc_metrics(self.val_data_gen, self.val_history, 'val', logs)

        if self.early_stopping:
            max_auc = np.max([x["ave_auc_macro"] for x in self.val_history])
            cur_auc = self.val_history[-1]["ave_auc_macro"]
            if max_auc > 0.75 and cur_auc < 0.73:
                self.model.stop_training = True


class LengthOfStayMetrics(keras.callbacks.Callback):
    def __init__(self, train_data_gen, val_data_gen, partition, batch_size=32,
                 early_stopping=True, verbose=2):
        super(LengthOfStayMetrics, self).__init__()
        self.train_data_gen = train_data_gen
        self.val_data_gen = val_data_gen
        self.batch_size = batch_size
        self.partition = partition
        self.early_stopping = early_stopping
        self.verbose = verbose
        self.train_history = []
        self.val_history = []

    def calc_metrics(self, data_gen, history, dataset, logs):
        y_true = []
        predictions = []
        for i in range(data_gen.steps):
            if self.verbose == 1:
                print("\tdone {}/{}".format(i, data_gen.steps), end='\r')
            (x, y_processed, y) = data_gen.next(return_y_true=True)
            pred = self.model.predict(x, batch_size=self.batch_size)
            if isinstance(x, list) and len(x) == 2:  # deep supervision
                if pred.shape[-1] == 1:  # regression
                    pred_flatten = pred.flatten()
                else:  # classification
                    pred_flatten = pred.reshape((-1, 10))
                for m, t, p in zip(x[1].flatten(), y.flatten(), pred_flatten):
                    if np.equal(m, 1):
                        y_true.append(t)
                        predictions.append(p)
            else:
                if pred.shape[-1] == 1:
                    y_true += list(y.flatten())
                    predictions += list(pred.flatten())
                else:
                    y_true += list(y)
                    predictions += list(pred)
        print('\n')
        if self.partition == 'log':
            predictions = [metrics.get_estimate_log(x, 10) for x in predictions]
            ret = metrics.print_metrics_log_bins(y_true, predictions)
        if self.partition == 'custom':
            predictions = [metrics.get_estimate_custom(x, 10) for x in predictions]
            ret = metrics.print_metrics_custom_bins(y_true, predictions)
        if self.partition == 'none':
            ret = metrics.print_metrics_regression(y_true, predictions)
        for k, v in ret.items():
            logs[dataset + '_' + k] = v
        history.append(ret)

    def on_epoch_end(self, epoch, logs={}):
        print("\n==>predicting on train")
        self.calc_metrics(self.train_data_gen, self.train_history, 'train', logs)
        print("\n==>predicting on validation")
        self.calc_metrics(self.val_data_gen, self.val_history, 'val', logs)

        if self.early_stopping:
            max_kappa = np.max([x["kappa"] for x in self.val_history])
            cur_kappa = self.val_history[-1]["kappa"]
            max_train_kappa = np.max([x["kappa"] for x in self.train_history])
            if max_kappa > 0.38 and cur_kappa < 0.35 and max_train_kappa > 0.47:
                self.model.stop_training = True


class MultitaskMetrics(keras.callbacks.Callback):
    def __init__(self, train_data_gen, val_data_gen, partition,
                 batch_size=32, early_stopping=True, verbose=2):
        super(MultitaskMetrics, self).__init__()
        self.train_data_gen = train_data_gen
        self.val_data_gen = val_data_gen
        self.batch_size = batch_size
        self.partition = partition
        self.early_stopping = early_stopping
        self.verbose = verbose
        self.train_history = []
        self.val_history = []

    def calc_metrics(self, data_gen, history, dataset, logs):
        ihm_y_true = []
        decomp_y_true = []
        los_y_true = []
        pheno_y_true = []

        ihm_pred = []
        decomp_pred = []
        los_pred = []
        pheno_pred = []

        for i in range(data_gen.steps):
            if self.verbose == 1:
                print("\tdone {}/{}".format(i, data_gen.steps), end='\r')
            (X, y, los_y_reg) = data_gen.next(return_y_true=True)
            outputs = self.model.predict(X, batch_size=self.batch_size)

            ihm_M = X[1]
            decomp_M = X[2]
            los_M = X[3]

            if not data_gen.target_repl:  # no target replication
                (ihm_p, decomp_p, los_p, pheno_p) = outputs
                (ihm_t, decomp_t, los_t, pheno_t) = y
            else:  # target replication
                (ihm_p, _, decomp_p, los_p, pheno_p, _) = outputs
                (ihm_t, _, decomp_t, los_t, pheno_t, _) = y

            los_t = los_y_reg  # real value not the label

            # ihm
            for (m, t, p) in zip(ihm_M.flatten(), ihm_t.flatten(), ihm_p.flatten()):
                if np.equal(m, 1):
                    ihm_y_true.append(t)
                    ihm_pred.append(p)

            # decomp
            for (m, t, p) in zip(decomp_M.flatten(), decomp_t.flatten(), decomp_p.flatten()):
                if np.equal(m, 1):
                    decomp_y_true.append(t)
                    decomp_pred.append(p)

            # los
            if los_p.shape[-1] == 1:  # regression
                for (m, t, p) in zip(los_M.flatten(), los_t.flatten(), los_p.flatten()):
                    if np.equal(m, 1):
                        los_y_true.append(t)
                        los_pred.append(p)
            else:  # classification
                for (m, t, p) in zip(los_M.flatten(), los_t.flatten(), los_p.reshape((-1, 10))):
                    if np.equal(m, 1):
                        los_y_true.append(t)
                        los_pred.append(p)

            # pheno
            for (t, p) in zip(pheno_t.reshape((-1, 25)), pheno_p.reshape((-1, 25))):
                pheno_y_true.append(t)
                pheno_pred.append(p)
        print('\n')

        # ihm
        print("\n ================= 48h mortality ================")
        ihm_pred = np.array(ihm_pred)
        ihm_pred = np.stack([1 - ihm_pred, ihm_pred], axis=1)
        ret = metrics.print_metrics_binary(ihm_y_true, ihm_pred)
        for k, v in ret.items():
            logs[dataset + '_ihm_' + k] = v

        # decomp
        print("\n ================ decompensation ================")
        decomp_pred = np.array(decomp_pred)
        decomp_pred = np.stack([1 - decomp_pred, decomp_pred], axis=1)
        ret = metrics.print_metrics_binary(decomp_y_true, decomp_pred)
        for k, v in ret.items():
            logs[dataset + '_decomp_' + k] = v

        # los
        print("\n ================ length of stay ================")
        if self.partition == 'log':
            los_pred = [metrics.get_estimate_log(x, 10) for x in los_pred]
            ret = metrics.print_metrics_log_bins(los_y_true, los_pred)
        if self.partition == 'custom':
            los_pred = [metrics.get_estimate_custom(x, 10) for x in los_pred]
            ret = metrics.print_metrics_custom_bins(los_y_true, los_pred)
        if self.partition == 'none':
            ret = metrics.print_metrics_regression(los_y_true, los_pred)
        for k, v in ret.items():
            logs[dataset + '_los_' + k] = v

        # pheno
        print("\n =================== phenotype ==================")
        pheno_pred = np.array(pheno_pred)
        ret = metrics.print_metrics_multilabel(pheno_y_true, pheno_pred)
        for k, v in ret.items():
            logs[dataset + '_pheno_' + k] = v

        history.append(logs)

    def on_epoch_end(self, epoch, logs={}):
        print("\n==>predicting on train")
        self.calc_metrics(self.train_data_gen, self.train_history, 'train', logs)
        print("\n==>predicting on validation")
        self.calc_metrics(self.val_data_gen, self.val_history, 'val', logs)

        if self.early_stopping:
            ihm_max_auc = np.max([x["val_ihm_auroc"] for x in self.val_history])
            ihm_cur_auc = self.val_history[-1]["val_ihm_auroc"]
            pheno_max_auc = np.max([x["val_pheno_ave_auc_macro"] for x in self.val_history])
            pheno_cur_auc = self.val_history[-1]["val_pheno_ave_auc_macro"]
            if (pheno_max_auc > 0.75 and pheno_cur_auc < 0.73) and (ihm_max_auc > 0.85 and ihm_cur_auc < 0.83):
                self.model.stop_training = True


# ===================== LAYERS ===================== #


def softmax(x, axis, mask=None):
    if mask is None:
        mask = K.constant(True)
    mask = K.cast(mask, K.floatx())
    if K.ndim(x) is K.ndim(mask) + 1:
        mask = K.expand_dims(mask)

    m = K.max(x, axis=axis, keepdims=True)
    e = K.exp(x - m) * mask
    s = K.sum(e, axis=axis, keepdims=True)
    s += K.cast(K.cast(s < K.epsilon(), K.floatx()) * K.epsilon(), K.floatx())
    return e / s


def _collect_attention(x, a, mask):
    """
    x is (B, T, D)
    a is (B, T, 1) or (B, T)
    mask is (B, T)
    """
    if K.ndim(a) == 2:
        a = K.expand_dims(a)
    a = softmax(a, axis=1, mask=mask)  # (B, T, 1)
    return K.sum(x * a, axis=1)  # (B, D)


class CollectAttetion(Layer):
    """ Collect attention on 3D tensor with softmax and summation
        Masking is disabled after this layer
    """

    def __init__(self, **kwargs):
        self.supports_masking = True
        super(CollectAttetion, self).__init__(**kwargs)

    def call(self, inputs, mask=None):
        x = inputs[0]
        a = inputs[1]
        # mask has 2 components, both are the same
        return _collect_attention(x, a, mask[0])

    def compute_output_shape(self, input_shape):
        return input_shape[0][0], input_shape[0][2]

    def compute_mask(self, input, input_mask=None):
        return None


class Slice(Layer):
    """ Slice 3D tensor by taking x[:, :, indices]
    """

    def __init__(self, indices, **kwargs):
        self.supports_masking = True
        self.indices = indices
        super(Slice, self).__init__(**kwargs)

    def call(self, x, mask=None):
        if K.backend() == 'tensorflow':
            xt = tf.transpose(x, perm=(2, 0, 1))
            gt = tf.gather(xt, self.indices)
            return tf.transpose(gt, perm=(1, 2, 0))
        return x[:, :, self.indices]

    def compute_output_shape(self, input_shape):
        return (input_shape[0], input_shape[1], len(self.indices))

    def compute_mask(self, input, input_mask=None):
        return input_mask

    def get_config(self):
        return {'indices': self.indices}


class GetTimestep(Layer):
    """ Takes 3D tensor and returns x[:, pos, :]
    """

    def __init__(self, pos=-1, **kwargs):
        self.pos = pos
        self.supports_masking = True
        super(GetTimestep, self).__init__(**kwargs)

    def call(self, x, mask=None):
        return x[:, self.pos, :]

    def compute_output_shape(self, input_shape):
        return (input_shape[0], input_shape[2])

    def compute_mask(self, input, input_mask=None):
        return None

    def get_config(self):
        return {'pos': self.pos}


LastTimestep = GetTimestep


class ExtendMask(Layer):
    """ Inputs:      [X, M]
        Output:      X
        Output_mask: M
    """

    def __init__(self, add_epsilon=False, **kwargs):
        self.supports_masking = True
        self.add_epsilon = add_epsilon
        super(ExtendMask, self).__init__(**kwargs)

    def call(self, x, mask=None):
        return x[0]

    def compute_output_shape(self, input_shape):
        return input_shape[0]

    def compute_mask(self, input, input_mask=None):
        if self.add_epsilon:
            return input[1] + K.epsilon()
        return input[1]

    def get_config(self):
        return {'add_epsilon': self.add_epsilon}


### FILE: .\tests\data\mimic3benchmarks\mimic3models\metrics.py ###
import numpy as np
from sklearn import metrics


# for decompensation, in-hospital mortality

def print_metrics_binary(y_true, predictions, verbose=1):
    predictions = np.array(predictions)
    if len(predictions.shape) == 1:
        predictions = np.stack([1 - predictions, predictions]).transpose((1, 0))

    cf = metrics.confusion_matrix(y_true, predictions.argmax(axis=1))
    if verbose:
        print("confusion matrix:")
        print(cf)
    cf = cf.astype(np.float32)

    acc = (cf[0][0] + cf[1][1]) / np.sum(cf)
    prec0 = cf[0][0] / (cf[0][0] + cf[1][0])
    prec1 = cf[1][1] / (cf[1][1] + cf[0][1])
    rec0 = cf[0][0] / (cf[0][0] + cf[0][1])
    rec1 = cf[1][1] / (cf[1][1] + cf[1][0])
    auroc = metrics.roc_auc_score(y_true, predictions[:, 1])

    (precisions, recalls, thresholds) = metrics.precision_recall_curve(y_true, predictions[:, 1])
    auprc = metrics.auc(recalls, precisions)
    minpse = np.max([min(x, y) for (x, y) in zip(precisions, recalls)])

    if verbose:
        print("accuracy = {}".format(acc))
        print("precision class 0 = {}".format(prec0))
        print("precision class 1 = {}".format(prec1))
        print("recall class 0 = {}".format(rec0))
        print("recall class 1 = {}".format(rec1))
        print("AUC of ROC = {}".format(auroc))
        print("AUC of PRC = {}".format(auprc))
        print("min(+P, Se) = {}".format(minpse))

    return {"acc": acc,
            "prec0": prec0,
            "prec1": prec1,
            "rec0": rec0,
            "rec1": rec1,
            "auroc": auroc,
            "auprc": auprc,
            "minpse": minpse}


# for phenotyping

def print_metrics_multilabel(y_true, predictions, verbose=1):
    y_true = np.array(y_true)
    predictions = np.array(predictions)

    auc_scores = metrics.roc_auc_score(y_true, predictions, average=None)
    ave_auc_micro = metrics.roc_auc_score(y_true, predictions,
                                          average="micro")
    ave_auc_macro = metrics.roc_auc_score(y_true, predictions,
                                          average="macro")
    ave_auc_weighted = metrics.roc_auc_score(y_true, predictions,
                                             average="weighted")

    if verbose:
        print("ROC AUC scores for labels:", auc_scores)
        print("ave_auc_micro = {}".format(ave_auc_micro))
        print("ave_auc_macro = {}".format(ave_auc_macro))
        print("ave_auc_weighted = {}".format(ave_auc_weighted))

    return {"auc_scores": auc_scores,
            "ave_auc_micro": ave_auc_micro,
            "ave_auc_macro": ave_auc_macro,
            "ave_auc_weighted": ave_auc_weighted}


# for length of stay

def mean_absolute_percentage_error(y_true, y_pred):
    return np.mean(np.abs((y_true - y_pred) / (y_true + 0.1))) * 100


def print_metrics_regression(y_true, predictions, verbose=1):
    predictions = np.array(predictions)
    predictions = np.maximum(predictions, 0).flatten()
    y_true = np.array(y_true)

    y_true_bins = [get_bin_custom(x, CustomBins.nbins) for x in y_true]
    prediction_bins = [get_bin_custom(x, CustomBins.nbins) for x in predictions]
    cf = metrics.confusion_matrix(y_true_bins, prediction_bins)
    if verbose:
        print("Custom bins confusion matrix:")
        print(cf)

    kappa = metrics.cohen_kappa_score(y_true_bins, prediction_bins,
                                      weights='linear')
    mad = metrics.mean_absolute_error(y_true, predictions)
    mse = metrics.mean_squared_error(y_true, predictions)
    mape = mean_absolute_percentage_error(y_true, predictions)

    if verbose:
        print("Mean absolute deviation (MAD) = {}".format(mad))
        print("Mean squared error (MSE) = {}".format(mse))
        print("Mean absolute percentage error (MAPE) = {}".format(mape))
        print("Cohen kappa score = {}".format(kappa))

    return {"mad": mad,
            "mse": mse,
            "mape": mape,
            "kappa": kappa}


class LogBins:
    nbins = 10
    means = [0.611848, 2.587614, 6.977417, 16.465430, 37.053745,
             81.816438, 182.303159, 393.334856, 810.964040, 1715.702848]


def get_bin_log(x, nbins, one_hot=False):
    binid = int(np.log(x + 1) / 8.0 * nbins)
    if binid < 0:
        binid = 0
    if binid >= nbins:
        binid = nbins - 1

    if one_hot:
        ret = np.zeros((LogBins.nbins,))
        ret[binid] = 1
        return ret
    return binid


def get_estimate_log(prediction, nbins):
    bin_id = np.argmax(prediction)
    return LogBins.means[bin_id]


def print_metrics_log_bins(y_true, predictions, verbose=1):
    y_true_bins = [get_bin_log(x, LogBins.nbins) for x in y_true]
    prediction_bins = [get_bin_log(x, LogBins.nbins) for x in predictions]
    cf = metrics.confusion_matrix(y_true_bins, prediction_bins)
    if verbose:
        print("LogBins confusion matrix:")
        print(cf)
    return print_metrics_regression(y_true, predictions, verbose)


class CustomBins:
    inf = 1e18
    bins = [(-inf, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 14), (14, +inf)]
    nbins = len(bins)
    means = [11.450379, 35.070846, 59.206531, 83.382723, 107.487817,
             131.579534, 155.643957, 179.660558, 254.306624, 585.325890]


def get_bin_custom(x, nbins, one_hot=False):
    for i in range(nbins):
        a = CustomBins.bins[i][0] * 24.0
        b = CustomBins.bins[i][1] * 24.0
        if a <= x < b:
            if one_hot:
                ret = np.zeros((CustomBins.nbins,))
                ret[i] = 1
                return ret
            return i
    return None


def get_estimate_custom(prediction, nbins):
    bin_id = np.argmax(prediction)
    assert 0 <= bin_id < nbins
    return CustomBins.means[bin_id]


def print_metrics_custom_bins(y_true, predictions, verbose=1):
    return print_metrics_regression(y_true, predictions, verbose)


### FILE: .\tests\data\mimic3benchmarks\mimic3models\parse_utils.py ###
import re


def parse_task(log):
    if re.search('ihm_C', log):
        return 'multitask'
    if re.search('partition', log):
        return 'los'
    if re.search('deep_supervision', log):
        return 'decomp'
    if re.search('ave_auc_micro', log):
        return 'pheno'
    if re.search('AUC of ROC', log):
        return'ihm'
    return None


def get_loss(log, loss_name):
    """ Options for loss_name: 'loss', 'ihm_loss', 'decomp_loss', 'pheno_loss', 'los_loss'
    """
    train = re.findall('[^_]{}: ([0-9.]+)'.format(loss_name), log)
    train = map(float, train)
    val = re.findall('val_{}: ([0-9.]+)'.format(loss_name), log)
    val = map(float, val)
    if len(train) > len(val):
        assert len(train) - 1 == len(val)
        train = train[:-1]
    return train, val


def parse_metrics(log, metric):
    ret = re.findall('{} = (.*)\n'.format(metric), log)
    ret = map(float, ret)
    if len(ret) % 2 == 1:
        ret = ret[:-1]
    return ret[::2], ret[1::2]


def parse_network(log):
    ret = re.search("network='([^']*)'", log)
    return ret.group(1)


def parse_load_state(log):
    ret = re.search("load_state='([^']*)'", log)
    return ret.group(1)


def parse_prefix(log):
    ret = re.search("prefix='([^']*)'", log)
    return ret.group(1)


def parse_dim(log):
    ret = re.search("dim=([0-9]*)", log)
    return int(ret.group(1))


def parse_size_coef(log):
    ret = re.search('size_coef=([\.0-9]*)', log)
    return ret.group(1)


def parse_depth(log):
    ret = re.search('depth=([0-9]*)', log)
    return int(ret.group(1))


def parse_ihm_C(log):
    ret = re.search('ihm_C=([\.0-9]*)', log)
    if ret:
        return float(ret.group(1))
    return None


def parse_decomp_C(log):
    ret = re.search('decomp_C=([\.0-9]*)', log)
    if ret:
        return float(ret.group(1))
    return None


def parse_los_C(log):
    ret = re.search('los_C=([\.0-9]*)', log)
    if ret:
        return float(ret.group(1))
    return None


def parse_pheno_C(log):
    ret = re.search('pheno_C=([\.0-9]*)', log)
    if ret:
        return float(ret.group(1))
    return None


def parse_dropout(log):
    ret = re.search('dropout=([\.0-9]*)', log)
    return float(ret.group(1))


def parse_timestep(log):
    ret = re.search('timestep=([\.0-9]*)', log)
    return float(ret.group(1))


def parse_partition(log):
    ret = re.search("partition='([^']*)'", log)
    if ret:
        return ret.group(1)
    return None


def parse_deep_supervision(log):
    ret = re.search('deep_supervision=(True|False)', log)
    if ret:
        return ret.group(1) == 'True'
    return False


def parse_target_repl_coef(log):
    ret = re.search('target_repl_coef=([\.0-9]*)', log)
    if ret:
        return float(ret.group(1))
    return None


def parse_epoch(state):
    ret = re.search('.*(chunk|epoch)([0-9]*).*', state)
    return int(ret.group(2))


def parse_batch_size(log):
    ret = re.search('batch_size=([0-9]*)', log)
    return int(ret.group(1))


def parse_state(log, epoch):
    lines = log.split('\n')
    for line in lines:
        res = re.search('.*saving model to (.*(chunk|epoch)([0-9]+).*)', line)
        if (res is not None):
            if epoch == 0:
                return res.group(1).strip()
            epoch -= 1
    raise Exception("State file is not found")


def parse_last_state(log):
    lines = log.split('\n')
    ret = None
    for line in lines:
        res = re.search('.*saving model to (.*(chunk|epoch)([0-9]+).*)', line)
        if (res is not None):
            ret = res.group(1).strip()
    return ret


### FILE: .\tests\data\mimic3benchmarks\mimic3models\preprocessing.py ###
import numpy as np
import platform
import pickle
import json
import os


class Discretizer:
    def __init__(self, timestep=0.8, store_masks=True, impute_strategy='zero', start_time='zero',
                 config_path=os.path.join(os.path.dirname(__file__), 'resources/discretizer_config.json')):

        with open(config_path) as f:
            config = json.load(f)
            self._id_to_channel = config['id_to_channel']
            self._channel_to_id = dict(zip(self._id_to_channel, range(len(self._id_to_channel))))
            self._is_categorical_channel = config['is_categorical_channel']
            self._possible_values = config['possible_values']
            self._normal_values = config['normal_values']

        self._header = ["Hours"] + self._id_to_channel
        self._timestep = timestep
        self._store_masks = store_masks
        self._start_time = start_time
        self._impute_strategy = impute_strategy

        # for statistics
        self._done_count = 0
        self._empty_bins_sum = 0
        self._unused_data_sum = 0

    def transform(self, X, header=None, end=None):
        if header is None:
            header = self._header
        assert header[0] == "Hours"
        eps = 1e-6

        N_channels = len(self._id_to_channel)
        ts = [float(row[0]) for row in X]
        for i in range(len(ts) - 1):
            assert ts[i] < ts[i+1] + eps

        if self._start_time == 'relative':
            first_time = ts[0]
        elif self._start_time == 'zero':
            first_time = 0
        else:
            raise ValueError("start_time is invalid")

        if end is None:
            max_hours = max(ts) - first_time
        else:
            max_hours = end - first_time

        N_bins = int(max_hours / self._timestep + 1.0 - eps)

        cur_len = 0
        begin_pos = [0 for i in range(N_channels)]
        end_pos = [0 for i in range(N_channels)]
        for i in range(N_channels):
            channel = self._id_to_channel[i]
            begin_pos[i] = cur_len
            if self._is_categorical_channel[channel]:
                end_pos[i] = begin_pos[i] + len(self._possible_values[channel])
            else:
                end_pos[i] = begin_pos[i] + 1
            cur_len = end_pos[i]

        data = np.zeros(shape=(N_bins, cur_len), dtype=float)
        mask = np.zeros(shape=(N_bins, N_channels), dtype=int)
        original_value = [["" for j in range(N_channels)] for i in range(N_bins)]
        total_data = 0
        unused_data = 0

        def write(data, bin_id, channel, value, begin_pos):
            channel_id = self._channel_to_id[channel]
            if self._is_categorical_channel[channel]:
                category_id = self._possible_values[channel].index(value)
                N_values = len(self._possible_values[channel])
                one_hot = np.zeros((N_values,))
                one_hot[category_id] = 1
                for pos in range(N_values):
                    data[bin_id, begin_pos[channel_id] + pos] = one_hot[pos]
            else:
                data[bin_id, begin_pos[channel_id]] = float(value)

        for row in X:
            t = float(row[0]) - first_time
            if t > max_hours + eps:
                continue
            bin_id = int(t / self._timestep - eps)
            assert 0 <= bin_id < N_bins

            for j in range(1, len(row)):
                if row[j] == "":
                    continue
                channel = header[j]
                channel_id = self._channel_to_id[channel]

                total_data += 1
                if mask[bin_id][channel_id] == 1:
                    unused_data += 1
                mask[bin_id][channel_id] = 1

                write(data, bin_id, channel, row[j], begin_pos)
                original_value[bin_id][channel_id] = row[j]

        # impute missing values

        if self._impute_strategy not in ['zero', 'normal_value', 'previous', 'next']:
            raise ValueError("impute strategy is invalid")

        if self._impute_strategy in ['normal_value', 'previous']:
            prev_values = [[] for i in range(len(self._id_to_channel))]
            for bin_id in range(N_bins):
                for channel in self._id_to_channel:
                    channel_id = self._channel_to_id[channel]
                    if mask[bin_id][channel_id] == 1:
                        prev_values[channel_id].append(original_value[bin_id][channel_id])
                        continue
                    if self._impute_strategy == 'normal_value':
                        imputed_value = self._normal_values[channel]
                    if self._impute_strategy == 'previous':
                        if len(prev_values[channel_id]) == 0:
                            imputed_value = self._normal_values[channel]
                        else:
                            imputed_value = prev_values[channel_id][-1]
                    write(data, bin_id, channel, imputed_value, begin_pos)

        if self._impute_strategy == 'next':
            prev_values = [[] for i in range(len(self._id_to_channel))]
            for bin_id in range(N_bins-1, -1, -1):
                for channel in self._id_to_channel:
                    channel_id = self._channel_to_id[channel]
                    if mask[bin_id][channel_id] == 1:
                        prev_values[channel_id].append(original_value[bin_id][channel_id])
                        continue
                    if len(prev_values[channel_id]) == 0:
                        imputed_value = self._normal_values[channel]
                    else:
                        imputed_value = prev_values[channel_id][-1]
                    write(data, bin_id, channel, imputed_value, begin_pos)

        empty_bins = np.sum([1 - min(1, np.sum(mask[i, :])) for i in range(N_bins)])
        self._done_count += 1
        self._empty_bins_sum += empty_bins / (N_bins + eps)
        self._unused_data_sum += unused_data / (total_data + eps)

        if self._store_masks:
            data = np.hstack([data, mask.astype(np.float32)])

        # create new header
        new_header = []
        for channel in self._id_to_channel:
            if self._is_categorical_channel[channel]:
                values = self._possible_values[channel]
                for value in values:
                    new_header.append(channel + "->" + value)
            else:
                new_header.append(channel)

        if self._store_masks:
            for i in range(len(self._id_to_channel)):
                channel = self._id_to_channel[i]
                new_header.append("mask->" + channel)

        new_header = ",".join(new_header)

        return (data, new_header)

    def print_statistics(self):
        print("statistics of discretizer:")
        print("\tconverted {} examples".format(self._done_count))
        print("\taverage unused data = {:.2f} percent".format(100.0 * self._unused_data_sum / self._done_count))
        print("\taverage empty  bins = {:.2f} percent".format(100.0 * self._empty_bins_sum / self._done_count))


class Normalizer:
    def __init__(self, fields=None):
        self._means = None
        self._stds = None
        self._fields = None
        if fields is not None:
            self._fields = [col for col in fields]

        self._sum_x = None
        self._sum_sq_x = None
        self._count = 0

    def _feed_data(self, x):
        x = np.array(x)
        self._count += x.shape[0]
        if self._sum_x is None:
            self._sum_x = np.sum(x, axis=0)
            self._sum_sq_x = np.sum(x**2, axis=0)
        else:
            self._sum_x += np.sum(x, axis=0)
            self._sum_sq_x += np.sum(x**2, axis=0)

    def _save_params(self, save_file_path):
        eps = 1e-7
        with open(save_file_path, "wb") as save_file:
            N = self._count
            self._means = 1.0 / N * self._sum_x
            self._stds = np.sqrt(1.0/(N - 1) * (self._sum_sq_x - 2.0 * self._sum_x * self._means + N * self._means**2))
            self._stds[self._stds < eps] = eps
            pickle.dump(obj={'means': self._means,
                             'stds': self._stds},
                        file=save_file,
                        protocol=2)

    def load_params(self, load_file_path):
        with open(load_file_path, "rb") as load_file:
            if platform.python_version()[0] == '2':
                dct = pickle.load(load_file)
            else:
                dct = pickle.load(load_file, encoding='latin1')
            self._means = dct['means']
            self._stds = dct['stds']

    def transform(self, X):
        if self._fields is None:
            fields = range(X.shape[1])
        else:
            fields = self._fields
        ret = 1.0 * X
        for col in fields:
            ret[:, col] = (X[:, col] - self._means[col]) / self._stds[col]
        return ret


### FILE: .\tests\data\mimic3benchmarks\mimic3models\rename_log.py ###
import os
import re
import argparse


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('log', type=str, nargs='+')
    args = parser.parse_args()

    if not isinstance(args.log, list):
        args.log = [args.log]

    for log in args.log:
        if log.find("renamed") != -1:
            print("{} is already renamed by hand, skipping...".format(log))
            continue
        if os.path.isdir(log):
            print("{} is a directory, skipping...".format(log))
            continue
        with open(log, 'r') as logfile:
            text = logfile.read()
            ret = re.search("==> model.final_name: (.*)\n", text)
            if ret is None:
                print("No model.final_name in log file: {}. Skipping...".format(log))
                continue
            name = ret.group(1)

        dirname = os.path.dirname(log)
        new_path = os.path.join(dirname, "{}.log".format(name))
        os.rename(log, new_path)

if __name__ == '__main__':
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3models\rerun.py ###
import argparse
from mimic3models import parse_utils
import json
import numpy as np


def check_decreasing(a, k, eps):
    if k >= len(a):
        return False
    pos = len(a) - 1
    for i in range(k):
        if a[pos] > a[pos - 1] + eps:
            return False
        pos -= 1
    return True


def process_single(filename, verbose, select):
    if verbose:
        print("Processing log file: {}".format(filename))

    with open(filename, 'r') as fin:
        log = fin.read()
    task = parse_utils.parse_task(log)

    if task is None:
        print("Task is not detected: {}".format(filename))
        return None

    if verbose:
        print("\ttask = {}".format(task))

    if task == 'multitask' or task == 'pheno':
        metric = 'ave_auc_macro'
    elif task == 'ihm' or task == 'decomp':
        metric = 'AUC of ROC'
    elif task == 'los':
        metric = 'Cohen kappa score'
    else:
        assert False

    train_metrics, val_metrics = parse_utils.parse_metrics(log, metric)
    if len(train_metrics) == 0:
        print("Less than one epoch: {}".format(filename))
        return None
    last_train = train_metrics[-1]
    last_val = val_metrics[-1]

    if verbose:
        print("\tlast train = {}, last val = {}".format(last_train, last_val))

    rerun = True
    if task == 'ihm':
        if last_val < 0.83 and last_train > 0.88:
            rerun = False
        if last_val < 0.84 and last_train > 0.89:
            rerun = False
        if last_val < 0.85 and last_train > 0.9:
            rerun = False
    elif task == 'decomp':
        if last_val < 0.85 and last_train > 0.89:
            rerun = False
        if last_val < 0.87 and last_train > 0.9:
            rerun = False
        if last_val < 0.88 and last_train > 0.92:
            rerun = False
    elif task == 'pheno' or task == 'multitask':
        if last_val < 0.75 and last_train > 0.77:
            rerun = False
        if last_val < 0.76 and last_train > 0.79:
            rerun = False
    elif task == 'los':
        if last_val < 0.35 and last_train > 0.42:
            rerun = False
        if last_val < 0.38 and last_train > 0.44:
            rerun = False
    else:
        assert False

    # check if val_metrics is decreasing
    if task in ['ihm', 'decomp', 'pheno', 'multitask']:
        n_decreases = 3
    else:  # 'los'
        n_decreases = 5

    if check_decreasing(val_metrics, n_decreases, 0.001):
        rerun = False

    # check if maximum value for validation was very early
    if task in ['ihm', 'decomp', 'pheno', 'multitask']:
        tol = 0.01
    else:  # 'los'
        tol = 0.03
    val_max = max(val_metrics)
    val_max_pos = np.argmax(val_metrics)
    if len(val_metrics) - val_max_pos >= 8 and val_max - last_val > tol:
        rerun = False

    if not select:
        rerun = True

    if verbose:
        print("\trerun = {}".format(rerun))

    if not rerun:
        return None

    # need to rerun
    last_state = parse_utils.parse_last_state(log)
    if last_state is None:
        print("Last state is not parsed: {}".format(filename))
        return None

    n_epochs = parse_utils.parse_epoch(last_state)

    if verbose:
        print("\tlast state = {}".format(last_state))

    network = parse_utils.parse_network(log)

    prefix = parse_utils.parse_prefix(log)
    if prefix == '':
        prefix = 'r2'
    elif not str.isdigit(prefix[-1]):
        prefix += '2'
    else:
        prefix = prefix[:-1] + str(int(prefix[-1]) + 1)

    dim = parse_utils.parse_dim(log)
    size_coef = parse_utils.parse_size_coef(log)
    depth = parse_utils.parse_depth(log)

    ihm_C = parse_utils.parse_ihm_C(log)
    decomp_C = parse_utils.parse_decomp_C(log)
    los_C = parse_utils.parse_los_C(log)
    pheno_C = parse_utils.parse_pheno_C(log)

    dropout = parse_utils.parse_dropout(log)
    partition = parse_utils.parse_partition(log)
    deep_supervision = parse_utils.parse_deep_supervision(log)
    target_repl_coef = parse_utils.parse_target_repl_coef(log)

    batch_size = parse_utils.parse_batch_size(log)

    command = "python -u main.py --network {} --prefix {} --dim {}"\
              " --depth {} --epochs 100 --batch_size {} --timestep 1.0"\
              " --load_state {}".format(network, prefix, dim, depth,  batch_size, last_state)

    if network.find('channel') != -1:
        command += ' --size_coef {}'.format(size_coef)

    if ihm_C:
        command += ' --ihm_C {}'.format(ihm_C)

    if decomp_C:
        command += ' --decomp_C {}'.format(decomp_C)

    if los_C:
        command += ' --los_C {}'.format(los_C)

    if pheno_C:
        command += ' --pheno_C {}'.format(pheno_C)

    if dropout > 0.0:
        command += ' --dropout {}'.format(dropout)

    if partition:
        command += ' --partition {}'.format(partition)

    if deep_supervision:
        command += ' --deep_supervision'

    if (target_repl_coef is not None) and target_repl_coef > 0.0:
        command += ' --target_repl_coef {}'.format(target_repl_coef)

    return {"command": command,
            "train_max": np.max(train_metrics),
            "train_max_pos": np.argmax(train_metrics),
            "val_max": np.max(val_metrics),
            "val_max_pos": np.argmax(val_metrics),
            "last_train": last_train,
            "last_val": last_val,
            "n_epochs": n_epochs,
            "filename": filename}


def main():
    argparser = argparse.ArgumentParser()
    argparser.add_argument('logs', type=str, nargs='+')
    argparser.add_argument('--verbose', type=int, default=0)
    argparser.add_argument('--select', dest='select', action='store_true')
    argparser.add_argument('--no-select', dest='select', action='store_false')
    argparser.set_defaults(select=True)
    args = argparser.parse_args()

    if not isinstance(args.logs, list):
        args.logs = [args.logs]

    rerun = []
    for log in args.logs:
        if log.find(".log") == -1:  # not a log file or is a not renamed log file
            continue
        ret = process_single(log, args.verbose, args.select)
        if ret:
            rerun += [ret]
    rerun = sorted(rerun, key=lambda x: x["last_val"], reverse=True)

    print("Need to rerun {} / {} models".format(len(rerun), len(args.logs)))
    print("Saving the results in rerun_output.json")
    with open("rerun_output.json", 'w') as fout:
        json.dump(rerun, fout)

    print("Saving commands in rerun_commands.sh")
    with open("rerun.sh", 'w') as fout:
        for a in rerun:
            fout.write(a['command'] + '\n')

    print("Saving filenames in rerun_filenames.txt")
    with open("rerun_filenames.txt", 'w') as fout:
        for a in rerun:
            fout.write(a['filename'] + '\n')


if __name__ == '__main__':
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3models\split_train_val.py ###
import shutil
import argparse
import os


def main():
    parser = argparse.ArgumentParser(description="Split train data into train and validation sets.")
    parser.add_argument('dataset_dir', type=str, help='Path to the directory which contains the dataset')
    args, _ = parser.parse_known_args()

    val_patients = set()
    with open(os.path.join(os.path.dirname(__file__), 'resources/valset.csv'), 'r') as valset_file:
        for line in valset_file:
            x, y = line.split(',')
            if int(y) == 1:
                val_patients.add(x)

    with open(os.path.join(args.dataset_dir, 'train/listfile.csv')) as listfile:
        lines = listfile.readlines()
        header = lines[0]
        lines = lines[1:]

    train_lines = [x for x in lines if x[:x.find("_")] not in val_patients]
    val_lines = [x for x in lines if x[:x.find("_")] in val_patients]
    assert len(train_lines) + len(val_lines) == len(lines)

    with open(os.path.join(args.dataset_dir, 'train_listfile.csv'), 'w') as train_listfile:
        train_listfile.write(header)
        for line in train_lines:
            train_listfile.write(line)

    with open(os.path.join(args.dataset_dir, 'val_listfile.csv'), 'w') as val_listfile:
        val_listfile.write(header)
        for line in val_lines:
            val_listfile.write(line)

    shutil.copy(os.path.join(args.dataset_dir, 'test/listfile.csv'),
                os.path.join(args.dataset_dir, 'test_listfile.csv'))


if __name__ == '__main__':
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3models\__init__.py ###


### FILE: .\tests\data\mimic3benchmarks\mimic3models\decompensation\main.py ###
import numpy as np
import argparse
import os
import imp
import re

from mimic3models.decompensation import utils
from mimic3benchmark.readers import DecompensationReader

from mimic3models.preprocessing import Discretizer, Normalizer
from mimic3models import metrics
from mimic3models import keras_utils
from mimic3models import common_utils

from keras.callbacks import ModelCheckpoint, CSVLogger


parser = argparse.ArgumentParser()
common_utils.add_common_arguments(parser)
parser.add_argument('--deep_supervision', dest='deep_supervision', action='store_true')
parser.add_argument('--data', type=str, help='Path to the data of decompensation task',
                    default=os.path.join(os.path.dirname(__file__), '../../data/decompensation/'))
parser.add_argument('--output_dir', type=str, help='Directory relative which all output files are stored',
                    default='.')
parser.set_defaults(deep_supervision=False)
args = parser.parse_args()
print(args)

if args.small_part:
    args.save_every = 2**30

# Build readers, discretizers, normalizers
if args.deep_supervision:
    train_data_loader = common_utils.DeepSupervisionDataLoader(dataset_dir=os.path.join(args.data, 'train'),
                                                               listfile=os.path.join(args.data, 'train_listfile.csv'),
                                                               small_part=args.small_part)
    val_data_loader = common_utils.DeepSupervisionDataLoader(dataset_dir=os.path.join(args.data, 'train'),
                                                             listfile=os.path.join(args.data, 'val_listfile.csv'),
                                                             small_part=args.small_part)
else:
    train_reader = DecompensationReader(dataset_dir=os.path.join(args.data, 'train'),
                                        listfile=os.path.join(args.data, 'train_listfile.csv'))
    val_reader = DecompensationReader(dataset_dir=os.path.join(args.data, 'train'),
                                      listfile=os.path.join(args.data, 'val_listfile.csv'))

discretizer = Discretizer(timestep=args.timestep,
                          store_masks=True,
                          impute_strategy='previous',
                          start_time='zero')

if args.deep_supervision:
    discretizer_header = discretizer.transform(train_data_loader._data["X"][0])[1].split(',')
else:
    discretizer_header = discretizer.transform(train_reader.read_example(0)["X"])[1].split(',')
cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find("->") == -1]

normalizer = Normalizer(fields=cont_channels)  # choose here which columns to standardize
normalizer_state = args.normalizer_state
if normalizer_state is None:
    normalizer_state = 'decomp_ts{}.input_str-previous.n1e5.start_time-zero.normalizer'.format(args.timestep)
    normalizer_state = os.path.join(os.path.dirname(__file__), normalizer_state)
normalizer.load_params(normalizer_state)

args_dict = dict(args._get_kwargs())
args_dict['header'] = discretizer_header
args_dict['task'] = 'decomp'


# Build the model
print("==> using model {}".format(args.network))
model_module = imp.load_source(os.path.basename(args.network), args.network)
model = model_module.Network(**args_dict)
suffix = "{}.bs{}{}{}.ts{}".format("" if not args.deep_supervision else ".dsup",
                                   args.batch_size,
                                   ".L1{}".format(args.l1) if args.l1 > 0 else "",
                                   ".L2{}".format(args.l2) if args.l2 > 0 else "",
                                   args.timestep)
model.final_name = args.prefix + model.say_name() + suffix
print("==> model.final_name:", model.final_name)


# Compile the model
print("==> compiling the model")
optimizer_config = {'class_name': args.optimizer,
                    'config': {'lr': args.lr,
                               'beta_1': args.beta_1}}

# NOTE: one can use binary_crossentropy even for (B, T, C) shape.
#       It will calculate binary_crossentropies for each class
#       and then take the mean over axis=-1. Tre results is (B, T).
model.compile(optimizer=optimizer_config,
              loss='binary_crossentropy')
model.summary()

# Load model weights
n_trained_chunks = 0
if args.load_state != "":
    model.load_weights(args.load_state)
    n_trained_chunks = int(re.match(".*chunk([0-9]+).*", args.load_state).group(1))

# Load data and prepare generators
if args.deep_supervision:
    train_data_gen = utils.BatchGenDeepSupervision(train_data_loader, discretizer,
                                                   normalizer, args.batch_size, shuffle=True)
    val_data_gen = utils.BatchGenDeepSupervision(val_data_loader, discretizer,
                                                 normalizer, args.batch_size, shuffle=False)
else:
    # Set number of batches in one epoch
    train_nbatches = 2000
    val_nbatches = 1000
    if args.small_part:
        train_nbatches = 40
        val_nbatches = 40
    train_data_gen = utils.BatchGen(train_reader, discretizer,
                                    normalizer, args.batch_size, train_nbatches, True)
    val_data_gen = utils.BatchGen(val_reader, discretizer,
                                  normalizer, args.batch_size, val_nbatches, False)

if args.mode == 'train':

    # Prepare training
    path = os.path.join(args.output_dir, 'keras_states/' + model.final_name + '.chunk{epoch}.test{val_loss}.state')

    metrics_callback = keras_utils.DecompensationMetrics(train_data_gen=train_data_gen,
                                                         val_data_gen=val_data_gen,
                                                         deep_supervision=args.deep_supervision,
                                                         batch_size=args.batch_size,
                                                         verbose=args.verbose)
    # make sure save directory exists
    dirname = os.path.dirname(path)
    if not os.path.exists(dirname):
        os.makedirs(dirname)
    saver = ModelCheckpoint(path, verbose=1, period=args.save_every)

    keras_logs = os.path.join(args.output_dir, 'keras_logs')
    if not os.path.exists(keras_logs):
        os.makedirs(keras_logs)
    csv_logger = CSVLogger(os.path.join(keras_logs, model.final_name + '.csv'),
                           append=True, separator=';')

    print("==> training")
    model.fit_generator(generator=train_data_gen,
                        steps_per_epoch=train_data_gen.steps,
                        validation_data=val_data_gen,
                        validation_steps=val_data_gen.steps,
                        epochs=n_trained_chunks + args.epochs,
                        initial_epoch=n_trained_chunks,
                        callbacks=[metrics_callback, saver, csv_logger],
                        verbose=args.verbose)

elif args.mode == 'test':

    # ensure that the code uses test_reader
    del train_data_gen
    del val_data_gen

    names = []
    ts = []
    labels = []
    predictions = []

    if args.deep_supervision:
        del train_data_loader
        del val_data_loader
        test_data_loader = common_utils.DeepSupervisionDataLoader(dataset_dir=os.path.join(args.data, 'test'),
                                                                  listfile=os.path.join(args.data, 'test_listfile.csv'),
                                                                  small_part=args.small_part)
        test_data_gen = utils.BatchGenDeepSupervision(test_data_loader, discretizer,
                                                      normalizer, args.batch_size,
                                                      shuffle=False, return_names=True)

        for i in range(test_data_gen.steps):
            print("\tdone {}/{}".format(i, test_data_gen.steps), end='\r')
            ret = next(test_data_gen)
            (x, y) = ret["data"]
            cur_names = np.array(ret["names"]).repeat(x[0].shape[1], axis=-1)
            cur_ts = ret["ts"]
            for single_ts in cur_ts:
                ts += single_ts

            pred = model.predict(x, batch_size=args.batch_size)
            for m, t, p, name in zip(x[1].flatten(), y.flatten(), pred.flatten(), cur_names.flatten()):
                if np.equal(m, 1):
                    labels.append(t)
                    predictions.append(p)
                    names.append(name)
        print('\n')
    else:
        del train_reader
        del val_reader
        test_reader = DecompensationReader(dataset_dir=os.path.join(args.data, 'test'),
                                           listfile=os.path.join(args.data, 'test_listfile.csv'))

        test_data_gen = utils.BatchGen(test_reader, discretizer,
                                       normalizer, args.batch_size,
                                       None, shuffle=False, return_names=True)  # put steps = None for a full test

        for i in range(test_data_gen.steps):
            print("predicting {} / {}".format(i, test_data_gen.steps), end='\r')
            ret = next(test_data_gen)
            x, y = ret["data"]
            cur_names = ret["names"]
            cur_ts = ret["ts"]

            x = np.array(x)
            pred = model.predict_on_batch(x)[:, 0]
            predictions += list(pred)
            labels += list(y)
            names += list(cur_names)
            ts += list(cur_ts)

    metrics.print_metrics_binary(labels, predictions)
    path = os.path.join(args.output_dir, 'test_predictions', os.path.basename(args.load_state)) + '.csv'
    utils.save_results(names, ts, predictions, labels, path)

else:
    raise ValueError("Wrong value for args.mode")


### FILE: .\tests\data\mimic3benchmarks\mimic3models\decompensation\utils.py ###
from mimic3models import common_utils
import threading
import os
import numpy as np
import random


def preprocess_chunk(data, ts, discretizer, normalizer=None):
    data = [discretizer.transform(X, end=t)[0] for (X, t) in zip(data, ts)]
    if normalizer is not None:
        data = [normalizer.transform(X) for X in data]
    return data


class BatchGen(object):

    def __init__(self, reader, discretizer, normalizer,
                 batch_size, steps, shuffle, return_names=False):
        self.reader = reader
        self.discretizer = discretizer
        self.normalizer = normalizer
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.return_names = return_names

        if steps is None:
            self.n_examples = reader.get_number_of_examples()
            self.steps = (self.n_examples + batch_size - 1) // batch_size
        else:
            self.n_examples = steps * batch_size
            self.steps = steps

        self.chunk_size = min(1024, self.steps) * batch_size
        self.lock = threading.Lock()
        self.generator = self._generator()

    def _generator(self):
        B = self.batch_size
        while True:
            if self.shuffle:
                self.reader.random_shuffle()
            remaining = self.n_examples
            while remaining > 0:
                current_size = min(self.chunk_size, remaining)
                remaining -= current_size

                ret = common_utils.read_chunk(self.reader, current_size)
                Xs = ret["X"]
                ts = ret["t"]
                ys = ret["y"]
                names = ret["name"]

                Xs = preprocess_chunk(Xs, ts, self.discretizer, self.normalizer)
                (Xs, ys, ts, names) = common_utils.sort_and_shuffle([Xs, ys, ts, names], B)

                for i in range(0, current_size, B):
                    X = common_utils.pad_zeros(Xs[i:i + B])
                    y = np.array(ys[i:i + B])
                    batch_names = names[i:i+B]
                    batch_ts = ts[i:i+B]
                    batch_data = (X, y)
                    if not self.return_names:
                        yield batch_data
                    else:
                        yield {"data": batch_data, "names": batch_names, "ts": batch_ts}

    def __iter__(self):
        return self.generator

    def next(self):
        with self.lock:
            return next(self.generator)

    def __next__(self):
        return self.next()


class BatchGenDeepSupervision(object):

    def __init__(self, dataloader, discretizer, normalizer,
                 batch_size, shuffle, return_names=False):
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.return_names = return_names

        self._load_per_patient_data(dataloader, discretizer, normalizer)

        self.steps = (len(self.data[1]) + batch_size - 1) // batch_size
        self.lock = threading.Lock()
        self.generator = self._generator()

    def _load_per_patient_data(self, dataloader, discretizer, normalizer):
        timestep = discretizer._timestep

        def get_bin(t):
            eps = 1e-6
            return int(t / timestep - eps)

        N = len(dataloader._data["X"])
        Xs = []
        ts = []
        masks = []
        ys = []
        names = []

        for i in range(N):
            X = dataloader._data["X"][i]
            cur_ts = dataloader._data["ts"][i]
            cur_ys = dataloader._data["ys"][i]
            name = dataloader._data["name"][i]

            cur_ys = [int(x) for x in cur_ys]

            T = max(cur_ts)
            nsteps = get_bin(T) + 1
            mask = [0] * nsteps
            y = [0] * nsteps

            for pos, z in zip(cur_ts, cur_ys):
                mask[get_bin(pos)] = 1
                y[get_bin(pos)] = z

            X = discretizer.transform(X, end=T)[0]
            if normalizer is not None:
                X = normalizer.transform(X)

            Xs.append(X)
            masks.append(np.array(mask))
            ys.append(np.array(y))
            names.append(name)
            ts.append(cur_ts)

            assert np.sum(mask) > 0
            assert len(X) == len(mask) and len(X) == len(y)

        self.data = [[Xs, masks], ys]
        self.names = names
        self.ts = ts

    def _generator(self):
        B = self.batch_size
        while True:
            if self.shuffle:
                N = len(self.data[1])
                order = list(range(N))
                random.shuffle(order)
                tmp_data = [[[None]*N, [None]*N], [None]*N]
                tmp_names = [None] * N
                tmp_ts = [None] * N
                for i in range(N):
                    tmp_data[0][0][i] = self.data[0][0][order[i]]
                    tmp_data[0][1][i] = self.data[0][1][order[i]]
                    tmp_data[1][i] = self.data[1][order[i]]
                    tmp_names[i] = self.names[order[i]]
                    tmp_ts[i] = self.ts[order[i]]
                self.data = tmp_data
                self.names = tmp_names
                self.ts = tmp_ts
            else:
                # sort entirely
                Xs = self.data[0][0]
                masks = self.data[0][1]
                ys = self.data[1]
                (Xs, masks, ys, self.names, self.ts) = common_utils.sort_and_shuffle([Xs, masks, ys,
                                                                                      self.names, self.ts], B)
                self.data = [[Xs, masks], ys]

            for i in range(0, len(self.data[1]), B):
                X = self.data[0][0][i:i + B]
                mask = self.data[0][1][i:i + B]
                y = self.data[1][i:i + B]
                names = self.names[i:i + B]
                ts = self.ts[i:i + B]

                X = common_utils.pad_zeros(X)  # (B, T, D)
                mask = common_utils.pad_zeros(mask)  # (B, T)
                y = common_utils.pad_zeros(y)
                y = np.expand_dims(y, axis=-1)  # (B, T, 1)
                batch_data = ([X, mask], y)
                if not self.return_names:
                    yield batch_data
                else:
                    yield {"data": batch_data, "names": names, "ts": ts}

    def __iter__(self):
        return self.generator

    def next(self):
        with self.lock:
            return next(self.generator)

    def __next__(self):
        return self.next()


def save_results(names, ts, pred, y_true, path):
    common_utils.create_directory(os.path.dirname(path))
    with open(path, 'w') as f:
        f.write("stay,period_length,prediction,y_true\n")
        for (name, t, x, y) in zip(names, ts, pred, y_true):
            f.write("{},{:.6f},{:.6f},{}\n".format(name, t, x, y))


### FILE: .\tests\data\mimic3benchmarks\mimic3models\decompensation\__init__.py ###


### FILE: .\tests\data\mimic3benchmarks\mimic3models\decompensation\logistic\main.py ###
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from mimic3benchmark.readers import DecompensationReader
from mimic3models import common_utils
from mimic3models.metrics import print_metrics_binary
from mimic3models.decompensation.utils import save_results

import os
import numpy as np
import argparse
import json


def read_and_extract_features(reader, count, period, features):
    read_chunk_size = 1000
    Xs = []
    ys = []
    names = []
    ts = []
    for i in range(0, count, read_chunk_size):
        j = min(count, i + read_chunk_size)
        ret = common_utils.read_chunk(reader, j - i)
        X = common_utils.extract_features_from_rawdata(ret['X'], ret['header'], period, features)
        Xs.append(X)
        ys += ret['y']
        names += ret['name']
        ts += ret['t']
    Xs = np.concatenate(Xs, axis=0)
    return (Xs, ys, names, ts)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--period', type=str, default='all', help='specifies which period extract features from',
                        choices=['first4days', 'first8days', 'last12hours', 'first25percent', 'first50percent', 'all'])
    parser.add_argument('--features', type=str, default='all', help='specifies what features to extract',
                        choices=['all', 'len', 'all_but_len'])
    parser.add_argument('--grid-search', dest='grid_search', action='store_true')
    parser.add_argument('--no-grid-search', dest='grid_search', action='store_false')
    parser.set_defaults(grid_search=False)
    parser.add_argument('--data', type=str, help='Path to the data of decompensation task',
                        default=os.path.join(os.path.dirname(__file__), '../../../data/decompensation/'))
    parser.add_argument('--output_dir', type=str, help='Directory relative which all output files are stored',
                        default='.')
    args = parser.parse_args()
    print(args)

    if args.grid_search:
        penalties = ['l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l1', 'l1', 'l1', 'l1', 'l1']
        coefs = [1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001, 1.0, 0.1, 0.01, 0.001, 0.0001]
    else:
        penalties = ['l2']
        coefs = [0.001]

    train_reader = DecompensationReader(dataset_dir=os.path.join(args.data, 'train'),
                                        listfile=os.path.join(args.data, 'train_listfile.csv'))

    val_reader = DecompensationReader(dataset_dir=os.path.join(args.data, 'train'),
                                      listfile=os.path.join(args.data, 'val_listfile.csv'))

    test_reader = DecompensationReader(dataset_dir=os.path.join(args.data, 'test'),
                                       listfile=os.path.join(args.data, 'test_listfile.csv'))

    print('Reading data and extracting features ...')
    n_train = min(100000, train_reader.get_number_of_examples())
    n_val = min(100000, val_reader.get_number_of_examples())

    (train_X, train_y, train_names, train_ts) = read_and_extract_features(
        train_reader, n_train, args.period, args.features)

    (val_X, val_y, val_names, val_ts) = read_and_extract_features(
        val_reader, n_val, args.period, args.features)

    (test_X, test_y, test_names, test_ts) = read_and_extract_features(
        test_reader, test_reader.get_number_of_examples(), args.period, args.features)

    print('Imputing missing values ...')
    imputer = SimpleImputer(missing_values=np.nan, strategy='mean', copy=True)
    imputer.fit(train_X)
    train_X = np.array(imputer.transform(train_X), dtype=np.float32)
    val_X = np.array(imputer.transform(val_X), dtype=np.float32)
    test_X = np.array(imputer.transform(test_X), dtype=np.float32)

    print('Normalizing the data to have zero mean and unit variance ...')
    scaler = StandardScaler()
    scaler.fit(train_X)
    train_X = scaler.transform(train_X)
    val_X = scaler.transform(val_X)
    test_X = scaler.transform(test_X)

    result_dir = os.path.join(args.output_dir, 'results')
    common_utils.create_directory(result_dir)

    for (penalty, C) in zip(penalties, coefs):
        file_name = '{}.{}.{}.C{}'.format(args.period, args.features, penalty, C)

        logreg = LogisticRegression(penalty=penalty, C=C, random_state=42)
        logreg.fit(train_X, train_y)

        with open(os.path.join(result_dir, 'train_{}.json'.format(file_name)), "w") as res_file:
            ret = print_metrics_binary(train_y, logreg.predict_proba(train_X))
            ret = {k: float(v) for k, v in ret.items()}
            json.dump(ret, res_file)

        with open(os.path.join(result_dir, 'val_{}.json'.format(file_name)), 'w') as res_file:
            ret = print_metrics_binary(val_y, logreg.predict_proba(val_X))
            ret = {k: float(v) for k, v in ret.items()}
            json.dump(ret, res_file)

        prediction = logreg.predict_proba(test_X)[:, 1]

        with open(os.path.join(result_dir, 'test_{}.json'.format(file_name)), 'w') as res_file:
            ret = print_metrics_binary(test_y, prediction)
            ret = {k: float(v) for k, v in ret.items()}
            json.dump(ret, res_file)

        save_results(test_names, test_ts, prediction, test_y,
                     os.path.join(args.output_dir, 'predictions', file_name + '.csv'))


if __name__ == '__main__':
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3models\decompensation\logistic\__init__.py ###


### FILE: .\tests\data\mimic3benchmarks\mimic3models\in_hospital_mortality\main.py ###
import numpy as np
import argparse
import os
import imp
import re

from mimic3models.in_hospital_mortality import utils
from mimic3benchmark.readers import InHospitalMortalityReader

from mimic3models.preprocessing import Discretizer, Normalizer
from mimic3models import metrics
from mimic3models import keras_utils
from mimic3models import common_utils

from keras.callbacks import ModelCheckpoint, CSVLogger

parser = argparse.ArgumentParser()
common_utils.add_common_arguments(parser)
parser.add_argument('--target_repl_coef', type=float, default=0.0)
parser.add_argument('--data', type=str, help='Path to the data of in-hospital mortality task',
                    default=os.path.join(os.path.dirname(__file__), '../../data/in-hospital-mortality/'))
parser.add_argument('--output_dir', type=str, help='Directory relative which all output files are stored',
                    default='.')
args = parser.parse_args()
print(args)

if args.small_part:
    args.save_every = 2**30

target_repl = (args.target_repl_coef > 0.0 and args.mode == 'train')

# Build readers, discretizers, normalizers
train_reader = InHospitalMortalityReader(dataset_dir=os.path.join(args.data, 'train'),
                                         listfile=os.path.join(args.data, 'train_listfile.csv'),
                                         period_length=48.0)

val_reader = InHospitalMortalityReader(dataset_dir=os.path.join(args.data, 'train'),
                                       listfile=os.path.join(args.data, 'val_listfile.csv'),
                                       period_length=48.0)

discretizer = Discretizer(timestep=float(args.timestep),
                          store_masks=True,
                          impute_strategy='previous',
                          start_time='zero')

discretizer_header = discretizer.transform(train_reader.read_example(0)["X"])[1].split(',')
cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find("->") == -1]

normalizer = Normalizer(fields=cont_channels)  # choose here which columns to standardize
normalizer_state = args.normalizer_state
if normalizer_state is None:
    normalizer_state = 'ihm_ts{}.input_str-{}.start_time-zero.normalizer'.format(args.timestep, args.imputation)
    normalizer_state = os.path.join(os.path.dirname(__file__), normalizer_state)
normalizer.load_params(normalizer_state)

args_dict = dict(args._get_kwargs())
args_dict['header'] = discretizer_header
args_dict['task'] = 'ihm'
args_dict['target_repl'] = target_repl

# Build the model
print("==> using model {}".format(args.network))
model_module = imp.load_source(os.path.basename(args.network), args.network)
model = model_module.Network(**args_dict)
suffix = ".bs{}{}{}.ts{}{}".format(args.batch_size,
                                   ".L1{}".format(args.l1) if args.l1 > 0 else "",
                                   ".L2{}".format(args.l2) if args.l2 > 0 else "",
                                   args.timestep,
                                   ".trc{}".format(args.target_repl_coef) if args.target_repl_coef > 0 else "")
model.final_name = args.prefix + model.say_name() + suffix
print("==> model.final_name:", model.final_name)


# Compile the model
print("==> compiling the model")
optimizer_config = {'class_name': args.optimizer,
                    'config': {'lr': args.lr,
                               'beta_1': args.beta_1}}

# NOTE: one can use binary_crossentropy even for (B, T, C) shape.
#       It will calculate binary_crossentropies for each class
#       and then take the mean over axis=-1. Tre results is (B, T).
if target_repl:
    loss = ['binary_crossentropy'] * 2
    loss_weights = [1 - args.target_repl_coef, args.target_repl_coef]
else:
    loss = 'binary_crossentropy'
    loss_weights = None

model.compile(optimizer=optimizer_config,
              loss=loss,
              loss_weights=loss_weights)
model.summary()

# Load model weights
n_trained_chunks = 0
if args.load_state != "":
    model.load_weights(args.load_state)
    n_trained_chunks = int(re.match(".*epoch([0-9]+).*", args.load_state).group(1))


# Read data
train_raw = utils.load_data(train_reader, discretizer, normalizer, args.small_part)
val_raw = utils.load_data(val_reader, discretizer, normalizer, args.small_part)

if target_repl:
    T = train_raw[0][0].shape[0]

    def extend_labels(data):
        data = list(data)
        labels = np.array(data[1])  # (B,)
        data[1] = [labels, None]
        data[1][1] = np.expand_dims(labels, axis=-1).repeat(T, axis=1)  # (B, T)
        data[1][1] = np.expand_dims(data[1][1], axis=-1)  # (B, T, 1)
        return data

    train_raw = extend_labels(train_raw)
    val_raw = extend_labels(val_raw)

if args.mode == 'train':

    # Prepare training
    path = os.path.join(args.output_dir, 'keras_states/' + model.final_name + '.epoch{epoch}.test{val_loss}.state')

    metrics_callback = keras_utils.InHospitalMortalityMetrics(train_data=train_raw,
                                                              val_data=val_raw,
                                                              target_repl=(args.target_repl_coef > 0),
                                                              batch_size=args.batch_size,
                                                              verbose=args.verbose)
    # make sure save directory exists
    dirname = os.path.dirname(path)
    if not os.path.exists(dirname):
        os.makedirs(dirname)
    saver = ModelCheckpoint(path, verbose=1, period=args.save_every)

    keras_logs = os.path.join(args.output_dir, 'keras_logs')
    if not os.path.exists(keras_logs):
        os.makedirs(keras_logs)
    csv_logger = CSVLogger(os.path.join(keras_logs, model.final_name + '.csv'),
                           append=True, separator=';')

    print("==> training")
    model.fit(x=train_raw[0],
              y=train_raw[1],
              validation_data=val_raw,
              epochs=n_trained_chunks + args.epochs,
              initial_epoch=n_trained_chunks,
              callbacks=[metrics_callback, saver, csv_logger],
              shuffle=True,
              verbose=args.verbose,
              batch_size=args.batch_size)

elif args.mode == 'test':

    # ensure that the code uses test_reader
    del train_reader
    del val_reader
    del train_raw
    del val_raw

    test_reader = InHospitalMortalityReader(dataset_dir=os.path.join(args.data, 'test'),
                                            listfile=os.path.join(args.data, 'test_listfile.csv'),
                                            period_length=48.0)
    ret = utils.load_data(test_reader, discretizer, normalizer, args.small_part,
                          return_names=True)

    data = ret["data"][0]
    labels = ret["data"][1]
    names = ret["names"]

    predictions = model.predict(data, batch_size=args.batch_size, verbose=1)
    predictions = np.array(predictions)[:, 0]
    metrics.print_metrics_binary(labels, predictions)

    path = os.path.join(args.output_dir, "test_predictions", os.path.basename(args.load_state)) + ".csv"
    utils.save_results(names, predictions, labels, path)

else:
    raise ValueError("Wrong value for args.mode")


### FILE: .\tests\data\mimic3benchmarks\mimic3models\in_hospital_mortality\utils.py ###
from mimic3models import common_utils
import numpy as np
import os


def load_data(reader, discretizer, normalizer, small_part=False, return_names=False):
    N = reader.get_number_of_examples()
    if small_part:
        N = 1000
    ret = common_utils.read_chunk(reader, N)
    data = ret["X"]
    ts = ret["t"]
    labels = ret["y"]
    names = ret["name"]
    data = [discretizer.transform(X, end=t)[0] for (X, t) in zip(data, ts)]
    if normalizer is not None:
        data = [normalizer.transform(X) for X in data]
    whole_data = (np.array(data), labels)
    if not return_names:
        return whole_data
    return {"data": whole_data, "names": names}


def save_results(names, pred, y_true, path):
    common_utils.create_directory(os.path.dirname(path))
    with open(path, 'w') as f:
        f.write("stay,prediction,y_true\n")
        for (name, x, y) in zip(names, pred, y_true):
            f.write("{},{:.6f},{}\n".format(name, x, y))


### FILE: .\tests\data\mimic3benchmarks\mimic3models\in_hospital_mortality\__init__.py ###


### FILE: .\tests\data\mimic3benchmarks\mimic3models\in_hospital_mortality\logistic\main.py ###
from mimic3benchmark.readers import InHospitalMortalityReader
from mimic3models import common_utils
from mimic3models.metrics import print_metrics_binary
from mimic3models.in_hospital_mortality.utils import save_results
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression

import os
import numpy as np
import argparse
import json


def read_and_extract_features(reader, period, features):
    ret = common_utils.read_chunk(reader, reader.get_number_of_examples())
    # ret = common_utils.read_chunk(reader, 100)
    X = common_utils.extract_features_from_rawdata(ret['X'], ret['header'], period, features)
    return (X, ret['y'], ret['name'])


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--C', type=float, default=1.0, help='inverse of L1 / L2 regularization')
    parser.add_argument('--l1', dest='l2', action='store_false')
    parser.add_argument('--l2', dest='l2', action='store_true')
    parser.set_defaults(l2=True)
    parser.add_argument('--period', type=str, default='all', help='specifies which period extract features from',
                        choices=['first4days', 'first8days', 'last12hours', 'first25percent', 'first50percent', 'all'])
    parser.add_argument('--features', type=str, default='all', help='specifies what features to extract',
                        choices=['all', 'len', 'all_but_len'])
    parser.add_argument('--data', type=str, help='Path to the data of in-hospital mortality task',
                        default=os.path.join(os.path.dirname(__file__), '../../../data/in-hospital-mortality/'))
    parser.add_argument('--output_dir', type=str, help='Directory relative which all output files are stored',
                        default='.')
    args = parser.parse_args()
    print(args)

    train_reader = InHospitalMortalityReader(dataset_dir=os.path.join(args.data, 'train'),
                                             listfile=os.path.join(args.data, 'train_listfile.csv'),
                                             period_length=48.0)

    val_reader = InHospitalMortalityReader(dataset_dir=os.path.join(args.data, 'train'),
                                           listfile=os.path.join(args.data, 'val_listfile.csv'),
                                           period_length=48.0)

    test_reader = InHospitalMortalityReader(dataset_dir=os.path.join(args.data, 'test'),
                                            listfile=os.path.join(args.data, 'test_listfile.csv'),
                                            period_length=48.0)

    print('Reading data and extracting features ...')
    (train_X, train_y, train_names) = read_and_extract_features(train_reader, args.period, args.features)
    (val_X, val_y, val_names) = read_and_extract_features(val_reader, args.period, args.features)
    (test_X, test_y, test_names) = read_and_extract_features(test_reader, args.period, args.features)
    print('  train data shape = {}'.format(train_X.shape))
    print('  validation data shape = {}'.format(val_X.shape))
    print('  test data shape = {}'.format(test_X.shape))

    print('Imputing missing values ...')
    imputer = SimpleImputer(missing_values=np.nan, strategy='mean', copy=True)
    imputer.fit(train_X)
    train_X = np.array(imputer.transform(train_X), dtype=np.float32)
    val_X = np.array(imputer.transform(val_X), dtype=np.float32)
    test_X = np.array(imputer.transform(test_X), dtype=np.float32)

    print('Normalizing the data to have zero mean and unit variance ...')
    scaler = StandardScaler()
    scaler.fit(train_X)
    train_X = scaler.transform(train_X)
    val_X = scaler.transform(val_X)
    test_X = scaler.transform(test_X)

    penalty = ('l2' if args.l2 else 'l1')
    file_name = '{}.{}.{}.C{}'.format(args.period, args.features, penalty, args.C)

    logreg = LogisticRegression(penalty=penalty, C=args.C, random_state=42)
    logreg.fit(train_X, train_y)

    result_dir = os.path.join(args.output_dir, 'results')
    common_utils.create_directory(result_dir)

    with open(os.path.join(result_dir, 'train_{}.json'.format(file_name)), 'w') as res_file:
        ret = print_metrics_binary(train_y, logreg.predict_proba(train_X))
        ret = {k : float(v) for k, v in ret.items()}
        json.dump(ret, res_file)

    with open(os.path.join(result_dir, 'val_{}.json'.format(file_name)), 'w') as res_file:
        ret = print_metrics_binary(val_y, logreg.predict_proba(val_X))
        ret = {k: float(v) for k, v in ret.items()}
        json.dump(ret, res_file)

    prediction = logreg.predict_proba(test_X)[:, 1]

    with open(os.path.join(result_dir, 'test_{}.json'.format(file_name)), 'w') as res_file:
        ret = print_metrics_binary(test_y, prediction)
        ret = {k: float(v) for k, v in ret.items()}
        json.dump(ret, res_file)

    save_results(test_names, prediction, test_y,
                 os.path.join(args.output_dir, 'predictions', file_name + '.csv'))


if __name__ == '__main__':
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3models\in_hospital_mortality\logistic\__init__.py ###


### FILE: .\tests\data\mimic3benchmarks\mimic3models\keras_models\channel_wise_lstms.py ###
from keras.models import Model
from keras.layers import Input, Dense, LSTM, Masking, Dropout
from keras.layers.wrappers import Bidirectional, TimeDistributed
from mimic3models.keras_utils import Slice, LastTimestep
from keras.layers.merge import Concatenate
from mimic3models.keras_utils import ExtendMask


class Network(Model):

    def __init__(self, dim, batch_norm, dropout, rec_dropout, header, task,
                 target_repl=False, deep_supervision=False, num_classes=1,
                 depth=1, input_dim=76, size_coef=4, **kwargs):

        self.dim = dim
        self.batch_norm = batch_norm
        self.dropout = dropout
        self.rec_dropout = rec_dropout
        self.depth = depth
        self.size_coef = size_coef

        if task in ['decomp', 'ihm', 'ph']:
            final_activation = 'sigmoid'
        elif task in ['los']:
            if num_classes == 1:
                final_activation = 'relu'
            else:
                final_activation = 'softmax'
        else:
            raise ValueError("Wrong value for task")

        print("==> not used params in network class:", kwargs.keys())

        # Parse channels
        channel_names = set()
        for ch in header:
            if ch.find("mask->") != -1:
                continue
            pos = ch.find("->")
            if pos != -1:
                channel_names.add(ch[:pos])
            else:
                channel_names.add(ch)
        channel_names = sorted(list(channel_names))
        print("==> found {} channels: {}".format(len(channel_names), channel_names))

        channels = []  # each channel is a list of columns
        for ch in channel_names:
            indices = range(len(header))
            indices = list(filter(lambda i: header[i].find(ch) != -1, indices))
            channels.append(indices)

        # Input layers and masking
        X = Input(shape=(None, input_dim), name='X')
        inputs = [X]
        mX = Masking()(X)

        if deep_supervision:
            M = Input(shape=(None,), name='M')
            inputs.append(M)

        # Configurations
        is_bidirectional = True
        if deep_supervision:
            is_bidirectional = False

        # Preprocess each channel
        cX = []
        for ch in channels:
            cX.append(Slice(ch)(mX))
        pX = []  # LSTM processed version of cX
        for x in cX:
            p = x
            for i in range(depth):
                num_units = dim
                if is_bidirectional:
                    num_units = num_units // 2

                lstm = LSTM(units=num_units,
                            activation='tanh',
                            return_sequences=True,
                            dropout=dropout,
                            recurrent_dropout=rec_dropout)

                if is_bidirectional:
                    p = Bidirectional(lstm)(p)
                else:
                    p = lstm(p)
            pX.append(p)

        # Concatenate processed channels
        Z = Concatenate(axis=2)(pX)

        # Main part of the network
        for i in range(depth-1):
            num_units = int(size_coef*dim)
            if is_bidirectional:
                num_units = num_units // 2

            lstm = LSTM(units=num_units,
                        activation='tanh',
                        return_sequences=True,
                        dropout=dropout,
                        recurrent_dropout=rec_dropout)

            if is_bidirectional:
                Z = Bidirectional(lstm)(Z)
            else:
                Z = lstm(Z)

        # Output module of the network
        return_sequences = (target_repl or deep_supervision)
        L = LSTM(units=int(size_coef*dim),
                 activation='tanh',
                 return_sequences=return_sequences,
                 dropout=dropout,
                 recurrent_dropout=rec_dropout)(Z)

        if dropout > 0:
            L = Dropout(dropout)(L)

        if target_repl:
            y = TimeDistributed(Dense(num_classes, activation=final_activation),
                                name='seq')(L)
            y_last = LastTimestep(name='single')(y)
            outputs = [y_last, y]
        elif deep_supervision:
            y = TimeDistributed(Dense(num_classes, activation=final_activation))(L)
            y = ExtendMask()([y, M])  # this way we extend mask of y to M
            outputs = [y]
        else:
            y = Dense(num_classes, activation=final_activation)(L)
            outputs = [y]

        super(Network, self).__init__(inputs=inputs, outputs=outputs)

    def say_name(self):
        return "{}.n{}.szc{}{}{}{}.dep{}".format('k_channel_wise_lstms',
                                                 self.dim,
                                                 self.size_coef,
                                                 ".bn" if self.batch_norm else "",
                                                 ".d{}".format(self.dropout) if self.dropout > 0 else "",
                                                 ".rd{}".format(self.rec_dropout) if self.rec_dropout > 0 else "",
                                                 self.depth)


### FILE: .\tests\data\mimic3benchmarks\mimic3models\keras_models\lstm.py ###
from keras.models import Model
from keras.layers import Input, Dense, LSTM, Masking, Dropout
from keras.layers.wrappers import Bidirectional, TimeDistributed
from mimic3models.keras_utils import LastTimestep
from mimic3models.keras_utils import ExtendMask


class Network(Model):

    def __init__(self, dim, batch_norm, dropout, rec_dropout, task,
                 target_repl=False, deep_supervision=False, num_classes=1,
                 depth=1, input_dim=76, **kwargs):

        print("==> not used params in network class:", kwargs.keys())

        self.dim = dim
        self.batch_norm = batch_norm
        self.dropout = dropout
        self.rec_dropout = rec_dropout
        self.depth = depth

        if task in ['decomp', 'ihm', 'ph']:
            final_activation = 'sigmoid'
        elif task in ['los']:
            if num_classes == 1:
                final_activation = 'relu'
            else:
                final_activation = 'softmax'
        else:
            raise ValueError("Wrong value for task")

        # Input layers and masking
        X = Input(shape=(None, input_dim), name='X')
        inputs = [X]
        mX = Masking()(X)

        if deep_supervision:
            M = Input(shape=(None,), name='M')
            inputs.append(M)

        # Configurations
        is_bidirectional = True
        if deep_supervision:
            is_bidirectional = False

        # Main part of the network
        for i in range(depth - 1):
            num_units = dim
            if is_bidirectional:
                num_units = num_units // 2

            lstm = LSTM(units=num_units,
                        activation='tanh',
                        return_sequences=True,
                        recurrent_dropout=rec_dropout,
                        dropout=dropout)

            if is_bidirectional:
                mX = Bidirectional(lstm)(mX)
            else:
                mX = lstm(mX)

        # Output module of the network
        return_sequences = (target_repl or deep_supervision)
        L = LSTM(units=dim,
                 activation='tanh',
                 return_sequences=return_sequences,
                 dropout=dropout,
                 recurrent_dropout=rec_dropout)(mX)

        if dropout > 0:
            L = Dropout(dropout)(L)

        if target_repl:
            y = TimeDistributed(Dense(num_classes, activation=final_activation),
                                name='seq')(L)
            y_last = LastTimestep(name='single')(y)
            outputs = [y_last, y]
        elif deep_supervision:
            y = TimeDistributed(Dense(num_classes, activation=final_activation))(L)
            y = ExtendMask()([y, M])  # this way we extend mask of y to M
            outputs = [y]
        else:
            y = Dense(num_classes, activation=final_activation)(L)
            outputs = [y]

        super(Network, self).__init__(inputs=inputs, outputs=outputs)

    def say_name(self):
        return "{}.n{}{}{}{}.dep{}".format('k_lstm',
                                           self.dim,
                                           ".bn" if self.batch_norm else "",
                                           ".d{}".format(self.dropout) if self.dropout > 0 else "",
                                           ".rd{}".format(self.rec_dropout) if self.rec_dropout > 0 else "",
                                           self.depth)


### FILE: .\tests\data\mimic3benchmarks\mimic3models\keras_models\multitask_channel_wise_lstms.py ###
from keras.models import Model
from keras.layers import Input, Dense, LSTM, Masking, Dropout
from keras.layers.wrappers import TimeDistributed
from mimic3models.keras_utils import Slice, GetTimestep, LastTimestep, ExtendMask
from keras.layers.merge import Concatenate, Multiply


class Network(Model):
    def __init__(self, dim, batch_norm, dropout, rec_dropout, header,
                 partition, ihm_pos, target_repl=False, depth=1, input_dim=76,
                 size_coef=4, **kwargs):

        print("==> not used params in network class:", kwargs.keys())

        self.dim = dim
        self.batch_norm = batch_norm
        self.dropout = dropout
        self.rec_dropout = rec_dropout
        self.depth = depth
        self.size_coef = size_coef

        # Parse channels
        channel_names = set()
        for ch in header:
            if ch.find("mask->") != -1:
                continue
            pos = ch.find("->")
            if pos != -1:
                channel_names.add(ch[:pos])
            else:
                channel_names.add(ch)
        channel_names = sorted(list(channel_names))
        print("==> found {} channels: {}".format(len(channel_names), channel_names))

        channels = []  # each channel is a list of columns
        for ch in channel_names:
            indices = range(len(header))
            indices = list(filter(lambda i: header[i].find(ch) != -1, indices))
            channels.append(indices)

        # Input layers and masking
        X = Input(shape=(None, input_dim), name='X')
        mX = Masking()(X)

        # Masks
        ihm_M = Input(shape=(1,), name='ihm_M')
        decomp_M = Input(shape=(None,), name='decomp_M')
        los_M = Input(shape=(None,), name='los_M')

        inputs = [X, ihm_M, decomp_M, los_M]

        # Preprocess each channel
        cX = []
        for ch in channels:
            cX.append(Slice(ch)(mX))
        pX = []  # LSTM processed version of cX
        for x in cX:
            p = x
            for i in range(depth):
                p = LSTM(units=dim,
                         activation='tanh',
                         return_sequences=True,
                         dropout=dropout,
                         recurrent_dropout=rec_dropout)(p)
            pX.append(p)

        # Concatenate processed channels
        Z = Concatenate(axis=2)(pX)

        # Main part of the network
        for i in range(depth):
            Z = LSTM(units=int(size_coef*dim),
                     activation='tanh',
                     return_sequences=True,
                     dropout=dropout,
                     recurrent_dropout=rec_dropout)(Z)
        L = Z

        if dropout > 0:
            L = Dropout(dropout)(L)

        # Output modules
        outputs = []

        # ihm output

        # NOTE: masking for ihm prediction works this way:
        #   if ihm_M = 1 then we will calculate an error term
        #   if ihm_M = 0, our prediction will be 0 and as the label
        #   will also be 0 then error_term will be 0.
        if target_repl:
            ihm_seq = TimeDistributed(Dense(1, activation='sigmoid'), name='ihm_seq')(L)
            ihm_y = GetTimestep(ihm_pos)(ihm_seq)
            ihm_y = Multiply(name='ihm_single')([ihm_y, ihm_M])
            outputs += [ihm_y, ihm_seq]
        else:
            ihm_seq = TimeDistributed(Dense(1, activation='sigmoid'))(L)
            ihm_y = GetTimestep(ihm_pos)(ihm_seq)
            ihm_y = Multiply(name='ihm')([ihm_y, ihm_M])
            outputs += [ihm_y]

        # decomp output
        decomp_y = TimeDistributed(Dense(1, activation='sigmoid'))(L)
        decomp_y = ExtendMask(name='decomp', add_epsilon=True)([decomp_y, decomp_M])
        outputs += [decomp_y]

        # los output
        if partition == 'none':
            los_y = TimeDistributed(Dense(1, activation='relu'))(L)
        else:
            los_y = TimeDistributed(Dense(10, activation='softmax'))(L)
        los_y = ExtendMask(name='los', add_epsilon=True)([los_y, los_M])
        outputs += [los_y]

        # pheno output
        if target_repl:
            pheno_seq = TimeDistributed(Dense(25, activation='sigmoid'), name='pheno_seq')(L)
            pheno_y = LastTimestep(name='pheno_single')(pheno_seq)
            outputs += [pheno_y, pheno_seq]
        else:
            pheno_seq = TimeDistributed(Dense(25, activation='sigmoid'))(L)
            pheno_y = LastTimestep(name='pheno')(pheno_seq)
            outputs += [pheno_y]

        super(Network, self).__init__(inputs=inputs, outputs=outputs)

    def say_name(self):
        return "{}.n{}.szc{}{}{}{}.dep{}".format('k_channel_wise_lstms',
                                                 self.dim,
                                                 self.size_coef,
                                                 ".bn" if self.batch_norm else "",
                                                 ".d{}".format(self.dropout) if self.dropout > 0 else "",
                                                 ".rd{}".format(self.rec_dropout) if self.rec_dropout > 0 else "",
                                                 self.depth)


### FILE: .\tests\data\mimic3benchmarks\mimic3models\keras_models\multitask_lstm.py ###
from keras.models import Model
from keras.layers import Input, Dense, LSTM, Masking, Dropout
from keras.layers.wrappers import TimeDistributed
from mimic3models.keras_utils import ExtendMask, GetTimestep, LastTimestep
from keras.layers.merge import Multiply


class Network(Model):
    def __init__(self, dim, batch_norm, dropout, rec_dropout, partition,
                 ihm_pos, target_repl=False, depth=1, input_dim=76, **kwargs):

        print("==> not used params in network class:", kwargs.keys())

        self.dim = dim
        self.batch_norm = batch_norm
        self.dropout = dropout
        self.rec_dropout = rec_dropout
        self.depth = depth

        # Input layers and masking
        X = Input(shape=(None, input_dim), name='X')
        mX = Masking()(X)

        # Masks
        ihm_M = Input(shape=(1,), name='ihm_M')
        decomp_M = Input(shape=(None,), name='decomp_M')
        los_M = Input(shape=(None,), name='los_M')

        inputs = [X, ihm_M, decomp_M, los_M]

        # Main part of the network
        for i in range(depth):
            mX = LSTM(units=dim,
                      activation='tanh',
                      return_sequences=True,
                      recurrent_dropout=rec_dropout,
                      dropout=dropout)(mX)
        L = mX

        if dropout > 0:
            L = Dropout(dropout)(L)

        # Output modules
        outputs = []

        # ihm output

        # NOTE: masking for ihm prediction works this way:
        #   if ihm_M = 1 then we will calculate an error term
        #   if ihm_M = 0, our prediction will be 0 and as the label
        #   will also be 0 then error_term will be 0.
        if target_repl > 0:
            ihm_seq = TimeDistributed(Dense(1, activation='sigmoid'), name='ihm_seq')(L)
            ihm_y = GetTimestep(ihm_pos)(ihm_seq)
            ihm_y = Multiply(name='ihm_single')([ihm_y, ihm_M])
            outputs += [ihm_y, ihm_seq]
        else:
            ihm_seq = TimeDistributed(Dense(1, activation='sigmoid'))(L)
            ihm_y = GetTimestep(ihm_pos)(ihm_seq)
            ihm_y = Multiply(name='ihm')([ihm_y, ihm_M])
            outputs += [ihm_y]

        # decomp output
        decomp_y = TimeDistributed(Dense(1, activation='sigmoid'))(L)
        decomp_y = ExtendMask(name='decomp', add_epsilon=True)([decomp_y, decomp_M])
        outputs += [decomp_y]

        # los output
        if partition == 'none':
            los_y = TimeDistributed(Dense(1, activation='relu'))(L)
        else:
            los_y = TimeDistributed(Dense(10, activation='softmax'))(L)
        los_y = ExtendMask(name='los', add_epsilon=True)([los_y, los_M])
        outputs += [los_y]

        # pheno output
        if target_repl:
            pheno_seq = TimeDistributed(Dense(25, activation='sigmoid'), name='pheno_seq')(L)
            pheno_y = LastTimestep(name='pheno_single')(pheno_seq)
            outputs += [pheno_y, pheno_seq]
        else:
            pheno_seq = TimeDistributed(Dense(25, activation='sigmoid'))(L)
            pheno_y = LastTimestep(name='pheno')(pheno_seq)
            outputs += [pheno_y]

        super(Network, self).__init__(inputs=inputs, outputs=outputs)

    def say_name(self):
        return "{}.n{}{}{}{}.dep{}".format('k_lstm',
                                           self.dim,
                                           ".bn" if self.batch_norm else "",
                                           ".d{}".format(self.dropout) if self.dropout > 0 else "",
                                           ".rd{}".format(self.rec_dropout) if self.rec_dropout > 0 else "",
                                           self.depth)


### FILE: .\tests\data\mimic3benchmarks\mimic3models\keras_models\__init__.py ###


### FILE: .\tests\data\mimic3benchmarks\mimic3models\length_of_stay\main.py ###
import numpy as np
import argparse
import os
import imp
import re

from mimic3models.length_of_stay import utils
from mimic3benchmark.readers import LengthOfStayReader

from mimic3models.preprocessing import Discretizer, Normalizer
from mimic3models import metrics
from mimic3models import keras_utils
from mimic3models import common_utils

from keras.callbacks import ModelCheckpoint, CSVLogger


parser = argparse.ArgumentParser()
common_utils.add_common_arguments(parser)
parser.add_argument('--deep_supervision', dest='deep_supervision', action='store_true')
parser.set_defaults(deep_supervision=False)
parser.add_argument('--partition', type=str, default='custom',
                    help="log, custom, none")
parser.add_argument('--data', type=str, help='Path to the data of length-of-stay task',
                    default=os.path.join(os.path.dirname(__file__), '../../data/length-of-stay/'))
parser.add_argument('--output_dir', type=str, help='Directory relative which all output files are stored',
                    default='.')
args = parser.parse_args()
print(args)

if args.small_part:
    args.save_every = 2**30

# Build readers, discretizers, normalizers
if args.deep_supervision:
    train_data_loader = common_utils.DeepSupervisionDataLoader(dataset_dir=os.path.join(args.data, 'train'),
                                                               listfile=os.path.join(args.data, 'train_listfile.csv'),
                                                               small_part=args.small_part)
    val_data_loader = common_utils.DeepSupervisionDataLoader(dataset_dir=os.path.join(args.data, 'train'),
                                                             listfile=os.path.join(args.data, 'val_listfile.csv'),
                                                             small_part=args.small_part)
else:
    train_reader = LengthOfStayReader(dataset_dir=os.path.join(args.data, 'train'),
                                      listfile=os.path.join(args.data, 'train_listfile.csv'))
    val_reader = LengthOfStayReader(dataset_dir=os.path.join(args.data, 'train'),
                                    listfile=os.path.join(args.data, 'val_listfile.csv'))

discretizer = Discretizer(timestep=args.timestep,
                          store_masks=True,
                          impute_strategy='previous',
                          start_time='zero')

if args.deep_supervision:
    discretizer_header = discretizer.transform(train_data_loader._data["X"][0])[1].split(',')
else:
    discretizer_header = discretizer.transform(train_reader.read_example(0)["X"])[1].split(',')
cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find("->") == -1]

normalizer = Normalizer(fields=cont_channels)  # choose here which columns to standardize
normalizer_state = args.normalizer_state
if normalizer_state is None:
    normalizer_state = 'los_ts{}.input_str-previous.start_time-zero.n5e4.normalizer'.format(args.timestep)
    normalizer_state = os.path.join(os.path.dirname(__file__), normalizer_state)
normalizer.load_params(normalizer_state)

args_dict = dict(args._get_kwargs())
args_dict['header'] = discretizer_header
args_dict['task'] = 'los'
args_dict['num_classes'] = (1 if args.partition == 'none' else 10)


# Build the model
print("==> using model {}".format(args.network))
model_module = imp.load_source(os.path.basename(args.network), args.network)
model = model_module.Network(**args_dict)
suffix = "{}.bs{}{}{}.ts{}.partition={}".format("" if not args.deep_supervision else ".dsup",
                                                args.batch_size,
                                                ".L1{}".format(args.l1) if args.l1 > 0 else "",
                                                ".L2{}".format(args.l2) if args.l2 > 0 else "",
                                                args.timestep,
                                                args.partition)
model.final_name = args.prefix + model.say_name() + suffix
print("==> model.final_name:", model.final_name)


# Compile the model
print("==> compiling the model")
optimizer_config = {'class_name': args.optimizer,
                    'config': {'lr': args.lr,
                               'beta_1': args.beta_1}}

if args.partition == 'none':
    # other options are: 'mean_squared_error', 'mean_absolute_percentage_error'
    loss_function = 'mean_squared_logarithmic_error'
else:
    loss_function = 'sparse_categorical_crossentropy'
# NOTE: categorical_crossentropy needs one-hot vectors
#       that's why we use sparse_categorical_crossentropy
# NOTE: it is ok to use keras.losses even for (B, T, D) shapes

model.compile(optimizer=optimizer_config,
              loss=loss_function)
model.summary()


# Load model weights
n_trained_chunks = 0
if args.load_state != "":
    model.load_weights(args.load_state)
    n_trained_chunks = int(re.match(".*chunk([0-9]+).*", args.load_state).group(1))

# Load data and prepare generators
if args.deep_supervision:
    train_data_gen = utils.BatchGenDeepSupervision(train_data_loader, args.partition,
                                                   discretizer, normalizer, args.batch_size, shuffle=True)
    val_data_gen = utils.BatchGenDeepSupervision(val_data_loader, args.partition,
                                                 discretizer, normalizer, args.batch_size, shuffle=False)
else:
    # Set number of batches in one epoch
    train_nbatches = 2000
    val_nbatches = 1000
    if args.small_part:
        train_nbatches = 20
        val_nbatches = 20

    train_data_gen = utils.BatchGen(reader=train_reader,
                                    discretizer=discretizer,
                                    normalizer=normalizer,
                                    partition=args.partition,
                                    batch_size=args.batch_size,
                                    steps=train_nbatches,
                                    shuffle=True)
    val_data_gen = utils.BatchGen(reader=val_reader,
                                  discretizer=discretizer,
                                  normalizer=normalizer,
                                  partition=args.partition,
                                  batch_size=args.batch_size,
                                  steps=val_nbatches,
                                  shuffle=False)
if args.mode == 'train':
    # Prepare training
    path = os.path.join(args.output_dir, 'keras_states/' + model.final_name + '.chunk{epoch}.test{val_loss}.state')

    metrics_callback = keras_utils.LengthOfStayMetrics(train_data_gen=train_data_gen,
                                                       val_data_gen=val_data_gen,
                                                       partition=args.partition,
                                                       batch_size=args.batch_size,
                                                       verbose=args.verbose)
    # make sure save directory exists
    dirname = os.path.dirname(path)
    if not os.path.exists(dirname):
        os.makedirs(dirname)
    saver = ModelCheckpoint(path, verbose=1, period=args.save_every)

    keras_logs = os.path.join(args.output_dir, 'keras_logs')
    if not os.path.exists(keras_logs):
        os.makedirs(keras_logs)
    csv_logger = CSVLogger(os.path.join(keras_logs, model.final_name + '.csv'),
                           append=True, separator=';')

    print("==> training")
    model.fit_generator(generator=train_data_gen,
                        steps_per_epoch=train_data_gen.steps,
                        validation_data=val_data_gen,
                        validation_steps=val_data_gen.steps,
                        epochs=n_trained_chunks + args.epochs,
                        initial_epoch=n_trained_chunks,
                        callbacks=[metrics_callback, saver, csv_logger],
                        verbose=args.verbose)

elif args.mode == 'test':
    # ensure that the code uses test_reader
    del train_data_gen
    del val_data_gen

    names = []
    ts = []
    labels = []
    predictions = []

    if args.deep_supervision:
        del train_data_loader
        del val_data_loader
        test_data_loader = common_utils.DeepSupervisionDataLoader(dataset_dir=os.path.join(args.data, 'test'),
                                                                  listfile=os.path.join(args.data, 'test_listfile.csv'),
                                                                  small_part=args.small_part)
        test_data_gen = utils.BatchGenDeepSupervision(test_data_loader, args.partition,
                                                      discretizer, normalizer, args.batch_size,
                                                      shuffle=False, return_names=True)
        for i in range(test_data_gen.steps):
            print("\tdone {}/{}".format(i, test_data_gen.steps), end='\r')

            ret = test_data_gen.next(return_y_true=True)
            (x, y_processed, y) = ret["data"]
            cur_names = np.array(ret["names"]).repeat(x[0].shape[1], axis=-1)
            cur_ts = ret["ts"]
            for single_ts in cur_ts:
                ts += single_ts

            pred = model.predict(x, batch_size=args.batch_size)
            if pred.shape[-1] == 1:  # regression
                pred_flatten = pred.flatten()
            else:  # classification
                pred_flatten = pred.reshape((-1, 10))
            for m, t, p, name in zip(x[1].flatten(), y.flatten(), pred_flatten, cur_names.flatten()):
                if np.equal(m, 1):
                    labels.append(t)
                    predictions.append(p)
                    names.append(name)
    else:
        del train_reader
        del val_reader
        test_reader = LengthOfStayReader(dataset_dir=os.path.join(args.data, 'test'),
                                         listfile=os.path.join(args.data, 'test_listfile.csv'))
        test_data_gen = utils.BatchGen(reader=test_reader,
                                       discretizer=discretizer,
                                       normalizer=normalizer,
                                       partition=args.partition,
                                       batch_size=args.batch_size,
                                       steps=None,  # put steps = None for a full test
                                       shuffle=False,
                                       return_names=True)

        for i in range(test_data_gen.steps):
            print("predicting {} / {}".format(i, test_data_gen.steps), end='\r')

            ret = test_data_gen.next(return_y_true=True)
            (x, y_processed, y) = ret["data"]
            cur_names = ret["names"]
            cur_ts = ret["ts"]

            x = np.array(x)
            pred = model.predict_on_batch(x)
            predictions += list(pred)
            labels += list(y)
            names += list(cur_names)
            ts += list(cur_ts)

    if args.partition == 'log':
        predictions = [metrics.get_estimate_log(x, 10) for x in predictions]
        metrics.print_metrics_log_bins(labels, predictions)
    if args.partition == 'custom':
        predictions = [metrics.get_estimate_custom(x, 10) for x in predictions]
        metrics.print_metrics_custom_bins(labels, predictions)
    if args.partition == 'none':
        metrics.print_metrics_regression(labels, predictions)
        predictions = [x[0] for x in predictions]

    path = os.path.join(os.path.join(args.output_dir, "test_predictions", os.path.basename(args.load_state)) + ".csv")
    utils.save_results(names, ts, predictions, labels, path)

else:
    raise ValueError("Wrong value for args.mode")


### FILE: .\tests\data\mimic3benchmarks\mimic3models\length_of_stay\utils.py ###
from mimic3models import metrics
from mimic3models import common_utils
import threading
import os
import numpy as np
import random


def preprocess_chunk(data, ts, discretizer, normalizer=None):
    data = [discretizer.transform(X, end=t)[0] for (X, t) in zip(data, ts)]
    if normalizer is not None:
        data = [normalizer.transform(X) for X in data]
    return data


class BatchGen(object):

    def __init__(self, reader, partition, discretizer, normalizer,
                 batch_size, steps, shuffle, return_names=False):
        self.reader = reader
        self.partition = partition
        self.discretizer = discretizer
        self.normalizer = normalizer
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.return_names = return_names

        if steps is None:
            self.n_examples = reader.get_number_of_examples()
            self.steps = (self.n_examples + batch_size - 1) // batch_size
        else:
            self.n_examples = steps * batch_size
            self.steps = steps

        self.chunk_size = min(1024, self.steps) * batch_size
        self.lock = threading.Lock()
        self.generator = self._generator()

    def _generator(self):
        B = self.batch_size
        while True:
            if self.shuffle:
                self.reader.random_shuffle()
            remaining = self.n_examples
            while remaining > 0:
                current_size = min(self.chunk_size, remaining)
                remaining -= current_size

                ret = common_utils.read_chunk(self.reader, current_size)
                Xs = ret["X"]
                ts = ret["t"]
                ys = ret["y"]
                names = ret["name"]

                Xs = preprocess_chunk(Xs, ts, self.discretizer, self.normalizer)
                (Xs, ys, ts, names) = common_utils.sort_and_shuffle([Xs, ys, ts, names], B)

                for i in range(0, current_size, B):
                    X = common_utils.pad_zeros(Xs[i:i + B])
                    y = ys[i:i+B]
                    y_true = np.array(y)
                    batch_names = names[i:i+B]
                    batch_ts = ts[i:i+B]

                    if self.partition == 'log':
                        y = [metrics.get_bin_log(x, 10) for x in y]
                    if self.partition == 'custom':
                        y = [metrics.get_bin_custom(x, 10) for x in y]

                    y = np.array(y)

                    if self.return_y_true:
                        batch_data = (X, y, y_true)
                    else:
                        batch_data = (X, y)

                    if not self.return_names:
                        yield batch_data
                    else:
                        yield {"data": batch_data, "names": batch_names, "ts": batch_ts}

    def __iter__(self):
        return self.generator

    def next(self, return_y_true=False):
        with self.lock:
            self.return_y_true = return_y_true
            return next(self.generator)

    def __next__(self):
        return self.next()


class BatchGenDeepSupervision(object):

    def __init__(self, dataloader, partition, discretizer, normalizer,
                 batch_size, shuffle, return_names=False):
        self.partition = partition
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.return_names = return_names

        self._load_per_patient_data(dataloader, discretizer, normalizer)

        self.steps = (len(self.data[1]) + batch_size - 1) // batch_size
        self.lock = threading.Lock()
        self.generator = self._generator()

    def _load_per_patient_data(self, dataloader, discretizer, normalizer):
        timestep = discretizer._timestep

        def get_bin(t):
            eps = 1e-6
            return int(t / timestep - eps)

        N = len(dataloader._data["X"])
        Xs = []
        ts = []
        masks = []
        ys = []
        names = []

        for i in range(N):
            X = dataloader._data["X"][i]
            cur_ts = dataloader._data["ts"][i]
            cur_ys = dataloader._data["ys"][i]
            name = dataloader._data["name"][i]

            cur_ys = [float(x) for x in cur_ys]

            T = max(cur_ts)
            nsteps = get_bin(T) + 1
            mask = [0] * nsteps
            y = [0] * nsteps

            for pos, z in zip(cur_ts, cur_ys):
                mask[get_bin(pos)] = 1
                y[get_bin(pos)] = z

            X = discretizer.transform(X, end=T)[0]
            if normalizer is not None:
                X = normalizer.transform(X)

            Xs.append(X)
            masks.append(np.array(mask))
            ys.append(np.array(y))
            names.append(name)
            ts.append(cur_ts)

            assert np.sum(mask) > 0
            assert len(X) == len(mask) and len(X) == len(y)

        self.data = [[Xs, masks], ys]
        self.names = names
        self.ts = ts

    def _generator(self):
        B = self.batch_size
        while True:
            if self.shuffle:
                N = len(self.data[1])
                order = list(range(N))
                random.shuffle(order)
                tmp_data = [[[None]*N, [None]*N], [None]*N]
                tmp_names = [None] * N
                tmp_ts = [None] * N
                for i in range(N):
                    tmp_data[0][0][i] = self.data[0][0][order[i]]
                    tmp_data[0][1][i] = self.data[0][1][order[i]]
                    tmp_data[1][i] = self.data[1][order[i]]
                    tmp_names[i] = self.names[order[i]]
                    tmp_ts[i] = self.ts[order[i]]
                self.data = tmp_data
                self.names = tmp_names
                self.ts = tmp_ts
            else:
                # sort entirely
                Xs = self.data[0][0]
                masks = self.data[0][1]
                ys = self.data[1]
                (Xs, masks, ys, self.names, self.ts) = common_utils.sort_and_shuffle([Xs, masks, ys,
                                                                                      self.names, self.ts], B)
                self.data = [[Xs, masks], ys]

            for i in range(0, len(self.data[1]), B):
                X = self.data[0][0][i:i+B]
                mask = self.data[0][1][i:i+B]
                y = self.data[1][i:i+B]
                names = self.names[i:i+B]
                ts = self.ts[i:i+B]

                y_true = [np.array(x) for x in y]
                y_true = common_utils.pad_zeros(y_true)
                y_true = np.expand_dims(y_true, axis=-1)

                if self.partition == 'log':
                    y = [np.array([metrics.get_bin_log(x, 10) for x in z]) for z in y]
                if self.partition == 'custom':
                    y = [np.array([metrics.get_bin_custom(x, 10) for x in z]) for z in y]

                X = common_utils.pad_zeros(X)  # (B, T, D)
                mask = common_utils.pad_zeros(mask)  # (B, T)
                y = common_utils.pad_zeros(y)
                y = np.expand_dims(y, axis=-1)

                if self.return_y_true:
                    batch_data = ([X, mask], y, y_true)
                else:
                    batch_data = ([X, mask], y)

                if not self.return_names:
                    yield batch_data
                else:
                    yield {"data": batch_data, "names": names, "ts": ts}

    def __iter__(self):
        return self.generator

    def next(self, return_y_true=False):
        with self.lock:
            self.return_y_true = return_y_true
            return next(self.generator)

    def __next__(self):
        return self.next()


def save_results(names, ts, pred, y_true, path):
    common_utils.create_directory(os.path.dirname(path))
    with open(path, 'w') as f:
        f.write("stay,period_length,prediction,y_true\n")
        for (name, t, x, y) in zip(names, ts, pred, y_true):
            f.write("{},{:.6f},{:.6f},{:.6f}\n".format(name, t, x, y))


### FILE: .\tests\data\mimic3benchmarks\mimic3models\length_of_stay\__init__.py ###


### FILE: .\tests\data\mimic3benchmarks\mimic3models\length_of_stay\logistic\main.py ###
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from mimic3benchmark.readers import LengthOfStayReader
from mimic3models import common_utils
from mimic3models.metrics import print_metrics_regression
from mimic3models.length_of_stay.utils import save_results

import os
import numpy as np
import argparse
import json


def read_and_extract_features(reader, count, period, features):
    read_chunk_size = 1000
    Xs = []
    ys = []
    names = []
    ts = []
    for i in range(0, count, read_chunk_size):
        j = min(count, i + read_chunk_size)
        ret = common_utils.read_chunk(reader, j - i)
        X = common_utils.extract_features_from_rawdata(ret['X'], ret['header'], period, features)
        Xs.append(X)
        ys += ret['y']
        names += ret['name']
        ts += ret['t']
    Xs = np.concatenate(Xs, axis=0)
    return (Xs, ys, names, ts)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--period', type=str, default='all', help='specifies which period extract features from',
                        choices=['first4days', 'first8days', 'last12hours', 'first25percent', 'first50percent', 'all'])
    parser.add_argument('--features', type=str, default='all', help='specifies what features to extract',
                        choices=['all', 'len', 'all_but_len'])
    parser.add_argument('--data', type=str, help='Path to the data of length-of-stay task',
                        default=os.path.join(os.path.dirname(__file__), '../../../data/length-of-stay/'))
    parser.add_argument('--output_dir', type=str, help='Directory relative which all output files are stored',
                        default='.')
    args = parser.parse_args()
    print(args)

    train_reader = LengthOfStayReader(dataset_dir=os.path.join(args.data, 'train'),
                                      listfile=os.path.join(args.data, 'train_listfile.csv'))

    val_reader = LengthOfStayReader(dataset_dir=os.path.join(args.data, 'train'),
                                    listfile=os.path.join(args.data, 'val_listfile.csv'))

    test_reader = LengthOfStayReader(dataset_dir=os.path.join(args.data, 'test'),
                                     listfile=os.path.join(args.data, 'test_listfile.csv'))

    print('Reading data and extracting features ...')
    n_train = min(100000, train_reader.get_number_of_examples())
    n_val = min(100000, val_reader.get_number_of_examples())

    (train_X, train_y, train_names, train_ts) = read_and_extract_features(
        train_reader, n_train, args.period, args.features)

    (val_X, val_y, val_names, val_ts) = read_and_extract_features(
        val_reader, n_val, args.period, args.features)

    (test_X, test_y, test_names, test_ts) = read_and_extract_features(
        test_reader, test_reader.get_number_of_examples(), args.period, args.features)

    print('Imputing missing values ...')
    imputer = SimpleImputer(missing_values=np.nan, strategy='mean', copy=True)
    imputer.fit(train_X)
    train_X = np.array(imputer.transform(train_X), dtype=np.float32)
    val_X = np.array(imputer.transform(val_X), dtype=np.float32)
    test_X = np.array(imputer.transform(test_X), dtype=np.float32)

    print('Normalizing the data to have zero mean and unit variance ...')
    scaler = StandardScaler()
    scaler.fit(train_X)
    train_X = scaler.transform(train_X)
    val_X = scaler.transform(val_X)
    test_X = scaler.transform(test_X)

    file_name = "{}.{}".format(args.period, args.features)

    linreg = LinearRegression()
    linreg.fit(train_X, train_y)

    result_dir = os.path.join(args.output_dir, 'results')
    common_utils.create_directory(result_dir)

    with open(os.path.join(result_dir, 'train_{}.json'.format(file_name)), "w") as res_file:
        ret = print_metrics_regression(train_y, linreg.predict(train_X))
        ret = {k: float(v) for k, v in ret.items()}
        json.dump(ret, res_file)

    with open(os.path.join(result_dir, 'val_{}.json'.format(file_name)), 'w') as res_file:
        ret = print_metrics_regression(val_y, linreg.predict(val_X))
        ret = {k: float(v) for k, v in ret.items()}
        json.dump(ret, res_file)

    prediction = linreg.predict(test_X)

    with open(os.path.join(result_dir, 'test_{}.json'.format(file_name)), 'w') as res_file:
        ret = print_metrics_regression(test_y, prediction)
        ret = {k: float(v) for k, v in ret.items()}
        json.dump(ret, res_file)

    save_results(test_names, test_ts, prediction, test_y,
                 os.path.join(args.output_dir, 'predictions', file_name + '.csv'))


if __name__ == '__main__':
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3models\length_of_stay\logistic\main_cf.py ###
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from mimic3benchmark.readers import LengthOfStayReader
from mimic3models import common_utils
from mimic3models import metrics
from mimic3models.length_of_stay.utils import save_results

import numpy as np
import argparse
import os
import json

n_bins = 10


def one_hot(index):
    x = np.zeros((n_bins,), dtype=np.int32)
    x[index] = 1
    return x


def read_and_extract_features(reader, count, period, features):
    read_chunk_size = 1000
    Xs = []
    ys = []
    names = []
    ts = []
    for i in range(0, count, read_chunk_size):
        j = min(count, i + read_chunk_size)
        ret = common_utils.read_chunk(reader, j - i)
        X = common_utils.extract_features_from_rawdata(ret['X'], ret['header'], period, features)
        Xs.append(X)
        ys += ret['y']
        names += ret['name']
        ts += ret['t']
    Xs = np.concatenate(Xs, axis=0)
    bins = np.array([one_hot(metrics.get_bin_custom(x, n_bins)) for x in ys])
    return (Xs, bins, ys, names, ts)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--period', type=str, default='all', help='specifies which period extract features from',
                        choices=['first4days', 'first8days', 'last12hours', 'first25percent', 'first50percent', 'all'])
    parser.add_argument('--features', type=str, default='all', help='specifies what features to extract',
                        choices=['all', 'len', 'all_but_len'])
    parser.add_argument('--grid-search', dest='grid_search', action='store_true')
    parser.add_argument('--no-grid-search', dest='grid_search', action='store_false')
    parser.set_defaults(grid_search=False)
    parser.add_argument('--data', type=str, help='Path to the data of length-of-stay task',
                        default=os.path.join(os.path.dirname(__file__), '../../../data/length-of-stay/'))
    parser.add_argument('--output_dir', type=str, help='Directory relative which all output files are stored',
                        default='.')
    args = parser.parse_args()
    print(args)

    if args.grid_search:
        penalties = ['l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l1', 'l1', 'l1', 'l1', 'l1']
        coefs = [1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001, 1.0, 0.1, 0.01, 0.001, 0.0001]
    else:
        penalties = ['l2']
        coefs = [0.00001]

    train_reader = LengthOfStayReader(dataset_dir=os.path.join(args.data, 'train'),
                                      listfile=os.path.join(args.data, 'train_listfile.csv'))

    val_reader = LengthOfStayReader(dataset_dir=os.path.join(args.data, 'train'),
                                    listfile=os.path.join(args.data, 'val_listfile.csv'))

    test_reader = LengthOfStayReader(dataset_dir=os.path.join(args.data, 'test'),
                                     listfile=os.path.join(args.data, 'test_listfile.csv'))

    print('Reading data and extracting features ...')
    n_train = min(100000, train_reader.get_number_of_examples())
    n_val = min(100000, val_reader.get_number_of_examples())

    (train_X, train_y, train_actual, train_names, train_ts) = read_and_extract_features(
        train_reader, n_train, args.period, args.features)

    (val_X, val_y, val_actual, val_names, val_ts) = read_and_extract_features(
        val_reader, n_val, args.period, args.features)

    (test_X, test_y, test_actual, test_names, test_ts) = read_and_extract_features(
        test_reader, test_reader.get_number_of_examples(), args.period, args.features)

    print("train set shape:  {}".format(train_X.shape))
    print("validation set shape: {}".format(val_X.shape))
    print("test set shape: {}".format(test_X.shape))

    print('Imputing missing values ...')
    imputer = SimpleImputer(missing_values=np.nan, strategy='mean', copy=True)
    imputer.fit(train_X)
    train_X = np.array(imputer.transform(train_X), dtype=np.float32)
    val_X = np.array(imputer.transform(val_X), dtype=np.float32)
    test_X = np.array(imputer.transform(test_X), dtype=np.float32)

    print('Normalizing the data to have zero mean and unit variance ...')
    scaler = StandardScaler()
    scaler.fit(train_X)
    train_X = scaler.transform(train_X)
    val_X = scaler.transform(val_X)
    test_X = scaler.transform(test_X)

    result_dir = os.path.join(args.output_dir, 'cf_results')
    common_utils.create_directory(result_dir)

    for (penalty, C) in zip(penalties, coefs):
        model_name = '{}.{}.{}.C{}'.format(args.period, args.features, penalty, C)

        train_activations = np.zeros(shape=train_y.shape, dtype=float)
        val_activations = np.zeros(shape=val_y.shape, dtype=float)
        test_activations = np.zeros(shape=test_y.shape, dtype=float)

        for task_id in range(n_bins):
            logreg = LogisticRegression(penalty=penalty, C=C, random_state=42)
            logreg.fit(train_X, train_y[:, task_id])

            train_preds = logreg.predict_proba(train_X)
            train_activations[:, task_id] = train_preds[:, 1]

            val_preds = logreg.predict_proba(val_X)
            val_activations[:, task_id] = val_preds[:, 1]

            test_preds = logreg.predict_proba(test_X)
            test_activations[:, task_id] = test_preds[:, 1]

        train_predictions = np.array([metrics.get_estimate_custom(x, n_bins) for x in train_activations])
        val_predictions = np.array([metrics.get_estimate_custom(x, n_bins) for x in val_activations])
        test_predictions = np.array([metrics.get_estimate_custom(x, n_bins) for x in test_activations])

        with open(os.path.join(result_dir, 'train_{}.json'.format(model_name)), 'w') as f:
            ret = metrics.print_metrics_custom_bins(train_actual, train_predictions)
            ret = {k: float(v) for k, v in ret.items()}
            json.dump(ret, f)

        with open(os.path.join(result_dir, 'val_{}.json'.format(model_name)), 'w') as f:
            ret = metrics.print_metrics_custom_bins(val_actual, val_predictions)
            ret = {k: float(v) for k, v in ret.items()}
            json.dump(ret, f)

        with open(os.path.join(result_dir, 'test_{}.json'.format(model_name)), 'w') as f:
            ret = metrics.print_metrics_custom_bins(test_actual, test_predictions)
            ret = {k: float(v) for k, v in ret.items()}
            json.dump(ret, f)

        save_results(test_names, test_ts, test_predictions, test_actual,
                     os.path.join(args.output_dir, 'cf_predictions', model_name + '.csv'))


if __name__ == '__main__':
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3models\length_of_stay\logistic\__init__.py ###


### FILE: .\tests\data\mimic3benchmarks\mimic3models\multitask\main.py ###
from mimic3models.multitask import utils
from mimic3benchmark.readers import MultitaskReader
from mimic3models.preprocessing import Discretizer, Normalizer
from mimic3models import metrics
from mimic3models import keras_utils
from mimic3models import common_utils
from keras.callbacks import ModelCheckpoint, CSVLogger

import mimic3models.in_hospital_mortality.utils as ihm_utils
import mimic3models.decompensation.utils as decomp_utils
import mimic3models.length_of_stay.utils as los_utils
import mimic3models.phenotyping.utils as pheno_utils

import numpy as np
import argparse
import os
import imp
import re

parser = argparse.ArgumentParser()
common_utils.add_common_arguments(parser)
parser.add_argument('--target_repl_coef', type=float, default=0.0)
parser.add_argument('--partition', type=str, default='custom', help="log, custom, none")
parser.add_argument('--ihm_C', type=float, default=1.0)
parser.add_argument('--los_C', type=float, default=1.0)
parser.add_argument('--pheno_C', type=float, default=1.0)
parser.add_argument('--decomp_C', type=float, default=1.0)
parser.add_argument('--data', type=str, help='Path to the data of multitasking',
                    default=os.path.join(os.path.dirname(__file__), '../../data/multitask/'))
parser.add_argument('--output_dir', type=str, help='Directory relative which all output files are stored',
                    default='.')
args = parser.parse_args()
print(args)

if args.small_part:
    args.save_every = 2 ** 30

target_repl = (args.target_repl_coef > 0.0 and args.mode == 'train')

# Build readers, discretizers, normalizers
train_reader = MultitaskReader(dataset_dir=os.path.join(args.data, 'train'),
                               listfile=os.path.join(args.data, 'train_listfile.csv'))

val_reader = MultitaskReader(dataset_dir=os.path.join(args.data, 'train'),
                             listfile=os.path.join(args.data, 'val_listfile.csv'))

discretizer = Discretizer(timestep=args.timestep,
                          store_masks=True,
                          impute_strategy='previous',
                          start_time='zero')

discretizer_header = discretizer.transform(train_reader.read_example(0)["X"])[1].split(',')
cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find("->") == -1]

normalizer = Normalizer(fields=cont_channels)  # choose here which columns to standardize
normalizer_state = args.normalizer_state
if normalizer_state is None:
    normalizer_state = 'mult_ts{}.input_str-{}.start_time-zero.normalizer'.format(args.timestep, args.imputation)
    normalizer_state = os.path.join(os.path.dirname(__file__), normalizer_state)
normalizer.load_params(normalizer_state)

args_dict = dict(args._get_kwargs())
args_dict['header'] = discretizer_header
args_dict['ihm_pos'] = int(48.0 / args.timestep - 1e-6)
args_dict['target_repl'] = target_repl

# Build the model
print("==> using model {}".format(args.network))
model_module = imp.load_source(os.path.basename(args.network), args.network)
model = model_module.Network(**args_dict)
suffix = ".bs{}{}{}.ts{}{}_partition={}_ihm={}_decomp={}_los={}_pheno={}".format(
    args.batch_size,
    ".L1{}".format(args.l1) if args.l1 > 0 else "",
    ".L2{}".format(args.l2) if args.l2 > 0 else "",
    args.timestep,
    ".trc{}".format(args.target_repl_coef) if args.target_repl_coef > 0 else "",
    args.partition,
    args.ihm_C,
    args.decomp_C,
    args.los_C,
    args.pheno_C)
model.final_name = args.prefix + model.say_name() + suffix
print("==> model.final_name:", model.final_name)

# Compile the model
print("==> compiling the model")
optimizer_config = {'class_name': args.optimizer,
                    'config': {'lr': args.lr,
                               'beta_1': args.beta_1}}

# Define loss functions

loss_dict = {}
loss_weights = {}

# ihm
if target_repl:
    loss_dict['ihm_single'] = 'binary_crossentropy'
    loss_dict['ihm_seq'] = 'binary_crossentropy'
    loss_weights['ihm_single'] = args.ihm_C * (1 - args.target_repl_coef)
    loss_weights['ihm_seq'] = args.ihm_C * args.target_repl_coef
else:
    loss_dict['ihm'] = 'binary_crossentropy'
    loss_weights['ihm'] = args.ihm_C

# decomp
loss_dict['decomp'] = 'binary_crossentropy'
loss_weights['decomp'] = args.decomp_C

# los
if args.partition == 'none':
    # other options are: 'mean_squared_error', 'mean_absolute_percentage_error'
    loss_dict['los'] = 'mean_squared_logarithmic_error'
else:
    loss_dict['los'] = 'sparse_categorical_crossentropy'
loss_weights['los'] = args.los_C

# pheno
if target_repl:
    loss_dict['pheno_single'] = 'binary_crossentropy'
    loss_dict['pheno_seq'] = 'binary_crossentropy'
    loss_weights['pheno_single'] = args.pheno_C * (1 - args.target_repl_coef)
    loss_weights['pheno_seq'] = args.pheno_C * args.target_repl_coef
else:
    loss_dict['pheno'] = 'binary_crossentropy'
    loss_weights['pheno'] = args.pheno_C

model.compile(optimizer=optimizer_config,
              loss=loss_dict,
              loss_weights=loss_weights)
model.summary()

# Load model weights
n_trained_chunks = 0
if args.load_state != "":
    model.load_weights(args.load_state)
    n_trained_chunks = int(re.match(".*epoch([0-9]+).*", args.load_state).group(1))

# Build data generators
train_data_gen = utils.BatchGen(reader=train_reader,
                                discretizer=discretizer,
                                normalizer=normalizer,
                                ihm_pos=args_dict['ihm_pos'],
                                partition=args.partition,
                                target_repl=target_repl,
                                batch_size=args.batch_size,
                                small_part=args.small_part,
                                shuffle=True)
val_data_gen = utils.BatchGen(reader=val_reader,
                              discretizer=discretizer,
                              normalizer=normalizer,
                              ihm_pos=args_dict['ihm_pos'],
                              partition=args.partition,
                              target_repl=target_repl,
                              batch_size=args.batch_size,
                              small_part=args.small_part,
                              shuffle=False)

if args.mode == 'train':
    # Prepare training
    path = os.path.join(args.output_dir, 'keras_states/' + model.final_name + '.epoch{epoch}.test{val_loss}.state')

    metrics_callback = keras_utils.MultitaskMetrics(train_data_gen=train_data_gen,
                                                    val_data_gen=val_data_gen,
                                                    partition=args.partition,
                                                    batch_size=args.batch_size,
                                                    verbose=args.verbose)
    # make sure save directory exists
    dirname = os.path.dirname(path)
    if not os.path.exists(dirname):
        os.makedirs(dirname)
    saver = ModelCheckpoint(path, verbose=1, period=args.save_every)

    keras_logs = os.path.join(args.output_dir, 'keras_logs')
    if not os.path.exists(keras_logs):
        os.makedirs(keras_logs)
    csv_logger = CSVLogger(os.path.join(keras_logs, model.final_name + '.csv'),
                           append=True, separator=';')

    print("==> training")
    model.fit_generator(generator=train_data_gen,
                        steps_per_epoch=train_data_gen.steps,
                        validation_data=val_data_gen,
                        validation_steps=val_data_gen.steps,
                        epochs=n_trained_chunks + args.epochs,
                        initial_epoch=n_trained_chunks,
                        callbacks=[metrics_callback, saver, csv_logger],
                        verbose=args.verbose)

elif args.mode == 'test':
    # ensure that the code uses test_reader
    del train_reader
    del val_reader
    del train_data_gen
    del val_data_gen

    test_reader = MultitaskReader(dataset_dir=os.path.join(args.data, 'test'),
                                  listfile=os.path.join(args.data, 'test_listfile.csv'))

    test_data_gen = utils.BatchGen(reader=test_reader,
                                   discretizer=discretizer,
                                   normalizer=normalizer,
                                   ihm_pos=args_dict['ihm_pos'],
                                   partition=args.partition,
                                   target_repl=target_repl,
                                   batch_size=args.batch_size,
                                   small_part=args.small_part,
                                   shuffle=False,
                                   return_names=True)
    ihm_y_true = []
    decomp_y_true = []
    los_y_true = []
    pheno_y_true = []

    ihm_pred = []
    decomp_pred = []
    los_pred = []
    pheno_pred = []

    ihm_names = []
    decomp_names = []
    los_names = []
    pheno_names = []

    decomp_ts = []
    los_ts = []
    pheno_ts = []

    for i in range(test_data_gen.steps):
        print("\tdone {}/{}".format(i, test_data_gen.steps), end='\r')
        ret = test_data_gen.next(return_y_true=True)
        (X, y, los_y_reg) = ret["data"]
        outputs = model.predict(X, batch_size=args.batch_size)

        names = list(ret["names"])
        names_extended = np.array(names).repeat(X[0].shape[1], axis=-1)

        ihm_M = X[1]
        decomp_M = X[2]
        los_M = X[3]

        assert len(outputs) == 4  # no target replication
        (ihm_p, decomp_p, los_p, pheno_p) = outputs
        (ihm_t, decomp_t, los_t, pheno_t) = y

        los_t = los_y_reg  # real value not the label

        # ihm
        for (m, t, p, name) in zip(ihm_M.flatten(), ihm_t.flatten(), ihm_p.flatten(), names):
            if np.equal(m, 1):
                ihm_y_true.append(t)
                ihm_pred.append(p)
                ihm_names.append(name)

        # decomp
        for x in ret['decomp_ts']:
            decomp_ts += x
        for (name, m, t, p) in zip(names_extended.flatten(), decomp_M.flatten(),
                                   decomp_t.flatten(), decomp_p.flatten()):
            if np.equal(m, 1):
                decomp_names.append(name)
                decomp_y_true.append(t)
                decomp_pred.append(p)

        # los
        for x in ret['los_ts']:
            los_ts += x
        if los_p.shape[-1] == 1:  # regression
            for (name, m, t, p) in zip(names_extended.flatten(), los_M.flatten(),
                                       los_t.flatten(), los_p.flatten()):
                if np.equal(m, 1):
                    los_names.append(name)
                    los_y_true.append(t)
                    los_pred.append(p)
        else:  # classification
            for (name, m, t, p) in zip(names_extended.flatten(), los_M.flatten(),
                                       los_t.flatten(), los_p.reshape((-1, 10))):
                if np.equal(m, 1):
                    los_names.append(name)
                    los_y_true.append(t)
                    los_pred.append(p)

        # pheno
        pheno_names += list(names)
        pheno_ts += list(ret["pheno_ts"])
        for (t, p) in zip(pheno_t.reshape((-1, 25)), pheno_p.reshape((-1, 25))):
            pheno_y_true.append(t)
            pheno_pred.append(p)
    print('\n')

    # ihm
    if args.ihm_C > 0:
        print("\n ================= 48h mortality ================")
        ihm_pred = np.array(ihm_pred)
        ihm_ret = metrics.print_metrics_binary(ihm_y_true, ihm_pred)

    # decomp
    if args.decomp_C > 0:
        print("\n ================ decompensation ================")
        decomp_pred = np.array(decomp_pred)
        decomp_ret = metrics.print_metrics_binary(decomp_y_true, decomp_pred)

    # los
    if args.los_C > 0:
        print("\n ================ length of stay ================")
        if args.partition == 'log':
            los_pred = [metrics.get_estimate_log(x, 10) for x in los_pred]
            los_ret = metrics.print_metrics_log_bins(los_y_true, los_pred)
        if args.partition == 'custom':
            los_pred = [metrics.get_estimate_custom(x, 10) for x in los_pred]
            los_ret = metrics.print_metrics_custom_bins(los_y_true, los_pred)
        if args.partition == 'none':
            los_ret = metrics.print_metrics_regression(los_y_true, los_pred)

    # pheno
    if args.pheno_C > 0:
        print("\n =================== phenotype ==================")
        pheno_pred = np.array(pheno_pred)
        pheno_ret = metrics.print_metrics_multilabel(pheno_y_true, pheno_pred)

    print("Saving the predictions in test_predictions/task directories ...")

    # ihm
    ihm_path = os.path.join(os.path.join(args.output_dir,
                                         "test_predictions/ihm", os.path.basename(args.load_state)) + ".csv")
    ihm_utils.save_results(ihm_names, ihm_pred, ihm_y_true, ihm_path)

    # decomp
    decomp_path = os.path.join(os.path.join(args.output_dir,
                                            "test_predictions/decomp", os.path.basename(args.load_state)) + ".csv")
    decomp_utils.save_results(decomp_names, decomp_ts, decomp_pred, decomp_y_true, decomp_path)

    # los
    los_path = os.path.join(os.path.join(args.output_dir,
                                         "test_predictions/los", os.path.basename(args.load_state)) + ".csv")
    los_utils.save_results(los_names, los_ts, los_pred, los_y_true, los_path)

    # pheno
    pheno_path = os.path.join(os.path.join(args.output_dir,
                                           "test_predictions/pheno", os.path.basename(args.load_state)) + ".csv")
    pheno_utils.save_results(pheno_names, pheno_ts, pheno_pred, pheno_y_true, pheno_path)

else:
    raise ValueError("Wrong value for args.mode")


### FILE: .\tests\data\mimic3benchmarks\mimic3models\multitask\utils.py ###
from mimic3models import metrics
from mimic3models import common_utils
import numpy as np
import threading
import random


class BatchGen(object):
    def __init__(self, reader, discretizer, normalizer, ihm_pos, partition,
                 target_repl, batch_size, small_part, shuffle, return_names=False):
        self.discretizer = discretizer
        self.normalizer = normalizer
        self.ihm_pos = ihm_pos
        self.partition = partition
        self.target_repl = target_repl
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.return_names = return_names

        N = reader.get_number_of_examples()
        if small_part:
            N = 1000
        self.steps = (N + batch_size - 1) // batch_size
        self.lock = threading.Lock()

        ret = common_utils.read_chunk(reader, N)
        Xs = ret['X']
        ts = ret['t']
        ihms = ret['ihm']
        loss = ret['los']
        phenos = ret['pheno']
        decomps = ret['decomp']

        self.data = dict()
        self.data['pheno_ts'] = ts
        self.data['names'] = ret['name']
        self.data['decomp_ts'] = []
        self.data['los_ts'] = []

        for i in range(N):
            self.data['decomp_ts'].append([pos for pos, m in enumerate(decomps[i][0]) if m == 1])
            self.data['los_ts'].append([pos for pos, m in enumerate(loss[i][0]) if m == 1])
            (Xs[i], ihms[i], decomps[i], loss[i], phenos[i]) = \
                self._preprocess_single(Xs[i], ts[i], ihms[i], decomps[i], loss[i], phenos[i])

        self.data['X'] = Xs
        self.data['ihm_M'] = [x[0] for x in ihms]
        self.data['ihm_y'] = [x[1] for x in ihms]
        self.data['decomp_M'] = [x[0] for x in decomps]
        self.data['decomp_y'] = [x[1] for x in decomps]
        self.data['los_M'] = [x[0] for x in loss]
        self.data['los_y'] = [x[1] for x in loss]
        self.data['pheno_y'] = phenos

        self.generator = self._generator()

    def _preprocess_single(self, X, max_time, ihm, decomp, los, pheno):
        timestep = self.discretizer._timestep
        eps = 1e-6

        def get_bin(t):
            return int(t / timestep - eps)

        n_steps = get_bin(max_time) + 1

        # X
        X = self.discretizer.transform(X, end=max_time)[0]
        if self.normalizer is not None:
            X = self.normalizer.transform(X)
        assert len(X) == n_steps

        # ihm
        # NOTE: when mask is 0, we set y to be 0. This is important
        #       because in the multitask networks when ihm_M = 0 we set
        #       our prediction thus the loss will be 0.
        if np.equal(ihm[1], 0):
            ihm[2] = 0
        ihm = (np.int32(ihm[1]), np.int32(ihm[2]))  # mask, label

        # decomp
        decomp_M = [0] * n_steps
        decomp_y = [0] * n_steps
        for i in range(len(decomp[0])):
            pos = get_bin(i)
            decomp_M[pos] = decomp[0][i]
            decomp_y[pos] = decomp[1][i]
        decomp = (np.array(decomp_M, dtype=np.int32),
                  np.array(decomp_y, dtype=np.int32))

        # los
        los_M = [0] * n_steps
        los_y = [0] * n_steps
        for i in range(len(los[0])):
            pos = get_bin(i)
            los_M[pos] = los[0][i]
            los_y[pos] = los[1][i]
        los = (np.array(los_M, dtype=np.int32),
               np.array(los_y, dtype=np.float32))

        # pheno
        pheno = np.array(pheno, dtype=np.int32)

        return (X, ihm, decomp, los, pheno)

    def _generator(self):
        B = self.batch_size
        while True:
            # convert to right format for sort_and_shuffle
            kv_pairs = list(self.data.items())
            data_index = [pair[0] for pair in kv_pairs].index('X')
            if data_index > 0:
                kv_pairs[0], kv_pairs[data_index] = kv_pairs[data_index], kv_pairs[0]
            mas = [kv[1] for kv in kv_pairs]

            if self.shuffle:
                N = len(self.data['X'])
                order = list(range(N))
                random.shuffle(order)
                tmp = [None] * len(mas)
                for mas_idx in range(len(mas)):
                    tmp[mas_idx] = [None] * len(mas[mas_idx])
                    for i in range(N):
                        tmp[mas_idx][i] = mas[mas_idx][order[i]]
                for i in range(len(kv_pairs)):
                    self.data[kv_pairs[i][0]] = tmp[i]
            else:
                # sort entirely
                mas = common_utils.sort_and_shuffle(mas, B)
                for i in range(len(kv_pairs)):
                    self.data[kv_pairs[i][0]] = mas[i]

            for i in range(0, len(self.data['X']), B):
                outputs = []

                # X
                X = self.data['X'][i:i+B]
                X = common_utils.pad_zeros(X, min_length=self.ihm_pos + 1)
                T = X.shape[1]

                # ihm
                ihm_M = np.array(self.data['ihm_M'][i:i+B])
                ihm_M = np.expand_dims(ihm_M, axis=-1)  # (B, 1)
                ihm_y = np.array(self.data['ihm_y'][i:i+B])
                ihm_y = np.expand_dims(ihm_y, axis=-1)  # (B, 1)
                outputs.append(ihm_y)
                if self.target_repl:
                    ihm_seq = np.expand_dims(ihm_y, axis=-1).repeat(T, axis=1)  # (B, T, 1)
                    outputs.append(ihm_seq)

                # decomp
                decomp_M = self.data['decomp_M'][i:i+B]
                decomp_M = common_utils.pad_zeros(decomp_M, min_length=self.ihm_pos + 1)
                decomp_y = self.data['decomp_y'][i:i+B]
                decomp_y = common_utils.pad_zeros(decomp_y, min_length=self.ihm_pos + 1)
                decomp_y = np.expand_dims(decomp_y, axis=-1)  # (B, T, 1)
                outputs.append(decomp_y)

                # los
                los_M = self.data['los_M'][i:i+B]
                los_M = common_utils.pad_zeros(los_M, min_length=self.ihm_pos + 1)
                los_y = self.data['los_y'][i:i+B]
                los_y_true = common_utils.pad_zeros(los_y, min_length=self.ihm_pos + 1)

                if self.partition == 'log':
                    los_y = [np.array([metrics.get_bin_log(x, 10) for x in z]) for z in los_y]
                if self.partition == 'custom':
                    los_y = [np.array([metrics.get_bin_custom(x, 10) for x in z]) for z in los_y]
                los_y = common_utils.pad_zeros(los_y, min_length=self.ihm_pos + 1)
                los_y = np.expand_dims(los_y, axis=-1)  # (B, T, 1)
                outputs.append(los_y)

                # pheno
                pheno_y = np.array(self.data['pheno_y'][i:i+B])
                outputs.append(pheno_y)
                if self.target_repl:
                    pheno_seq = np.expand_dims(pheno_y, axis=1).repeat(T, axis=1)  # (B, T, 25)
                    outputs.append(pheno_seq)

                inputs = [X, ihm_M, decomp_M, los_M]

                if self.return_y_true:
                    batch_data = (inputs, outputs, los_y_true)
                else:
                    batch_data = (inputs, outputs)

                if not self.return_names:
                    yield batch_data
                else:
                    yield {'data': batch_data,
                           'names': self.data['names'][i:i+B],
                           'decomp_ts': self.data['decomp_ts'][i:i+B],
                           'los_ts': self.data['los_ts'][i:i+B],
                           'pheno_ts': self.data['pheno_ts'][i:i + B]}

    def __iter__(self):
        return self.generator

    def next(self, return_y_true=False):
        with self.lock:
            self.return_y_true = return_y_true
            return next(self.generator)

    def __next__(self):
        return self.next()


### FILE: .\tests\data\mimic3benchmarks\mimic3models\multitask\__init__.py ###


### FILE: .\tests\data\mimic3benchmarks\mimic3models\phenotyping\main.py ###
import numpy as np
import argparse
import os
import imp
import re

from mimic3models.phenotyping import utils
from mimic3benchmark.readers import PhenotypingReader

from mimic3models.preprocessing import Discretizer, Normalizer
from mimic3models import metrics
from mimic3models import keras_utils
from mimic3models import common_utils

from keras.callbacks import ModelCheckpoint, CSVLogger

parser = argparse.ArgumentParser()
common_utils.add_common_arguments(parser)
parser.add_argument('--target_repl_coef', type=float, default=0.0)
parser.add_argument('--data', type=str, help='Path to the data of phenotyping task',
                    default=os.path.join(os.path.dirname(__file__), '../../data/phenotyping/'))
parser.add_argument('--output_dir', type=str, help='Directory relative which all output files are stored',
                    default='.')
args = parser.parse_args()
print(args)

if args.small_part:
    args.save_every = 2**30

target_repl = (args.target_repl_coef > 0.0 and args.mode == 'train')

# Build readers, discretizers, normalizers
train_reader = PhenotypingReader(dataset_dir=os.path.join(args.data, 'train'),
                                 listfile=os.path.join(args.data, 'train_listfile.csv'))

val_reader = PhenotypingReader(dataset_dir=os.path.join(args.data, 'train'),
                               listfile=os.path.join(args.data, 'val_listfile.csv'))

discretizer = Discretizer(timestep=float(args.timestep),
                          store_masks=True,
                          impute_strategy='previous',
                          start_time='zero')

discretizer_header = discretizer.transform(train_reader.read_example(0)["X"])[1].split(',')
cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find("->") == -1]

normalizer = Normalizer(fields=cont_channels)  # choose here which columns to standardize
normalizer_state = args.normalizer_state
if normalizer_state is None:
    normalizer_state = 'ph_ts{}.input_str-previous.start_time-zero.normalizer'.format(args.timestep)
    normalizer_state = os.path.join(os.path.dirname(__file__), normalizer_state)
normalizer.load_params(normalizer_state)

args_dict = dict(args._get_kwargs())
args_dict['header'] = discretizer_header
args_dict['task'] = 'ph'
args_dict['num_classes'] = 25
args_dict['target_repl'] = target_repl

# Build the model
print("==> using model {}".format(args.network))
model_module = imp.load_source(os.path.basename(args.network), args.network)
model = model_module.Network(**args_dict)
suffix = ".bs{}{}{}.ts{}{}".format(args.batch_size,
                                   ".L1{}".format(args.l1) if args.l1 > 0 else "",
                                   ".L2{}".format(args.l2) if args.l2 > 0 else "",
                                   args.timestep,
                                   ".trc{}".format(args.target_repl_coef) if args.target_repl_coef > 0 else "")
model.final_name = args.prefix + model.say_name() + suffix
print("==> model.final_name:", model.final_name)


# Compile the model
print("==> compiling the model")
optimizer_config = {'class_name': args.optimizer,
                    'config': {'lr': args.lr,
                               'beta_1': args.beta_1}}

# NOTE: one can use binary_crossentropy even for (B, T, C) shape.
#       It will calculate binary_crossentropies for each class
#       and then take the mean over axis=-1. Tre results is (B, T).
if target_repl:
    loss = ['binary_crossentropy'] * 2
    loss_weights = [1 - args.target_repl_coef, args.target_repl_coef]
else:
    loss = 'binary_crossentropy'
    loss_weights = None

model.compile(optimizer=optimizer_config,
              loss=loss,
              loss_weights=loss_weights)
model.summary()

# Load model weights
n_trained_chunks = 0
if args.load_state != "":
    model.load_weights(args.load_state)
    n_trained_chunks = int(re.match(".*epoch([0-9]+).*", args.load_state).group(1))


# Build data generators
train_data_gen = utils.BatchGen(train_reader, discretizer,
                                normalizer, args.batch_size,
                                args.small_part, target_repl, shuffle=True)
val_data_gen = utils.BatchGen(val_reader, discretizer,
                              normalizer, args.batch_size,
                              args.small_part, target_repl, shuffle=False)

if args.mode == 'train':
    # Prepare training
    path = os.path.join(args.output_dir, 'keras_states/' + model.final_name + '.epoch{epoch}.test{val_loss}.state')

    metrics_callback = keras_utils.PhenotypingMetrics(train_data_gen=train_data_gen,
                                                      val_data_gen=val_data_gen,
                                                      batch_size=args.batch_size,
                                                      verbose=args.verbose)
    # make sure save directory exists
    dirname = os.path.dirname(path)
    if not os.path.exists(dirname):
        os.makedirs(dirname)
    saver = ModelCheckpoint(path, verbose=1, period=args.save_every)

    keras_logs = os.path.join(args.output_dir, 'keras_logs')
    if not os.path.exists(keras_logs):
        os.makedirs(keras_logs)
    csv_logger = CSVLogger(os.path.join(keras_logs, model.final_name + '.csv'),
                           append=True, separator=';')

    print("==> training")
    model.fit_generator(generator=train_data_gen,
                        steps_per_epoch=train_data_gen.steps,
                        validation_data=val_data_gen,
                        validation_steps=val_data_gen.steps,
                        epochs=n_trained_chunks + args.epochs,
                        initial_epoch=n_trained_chunks,
                        callbacks=[metrics_callback, saver, csv_logger],
                        verbose=args.verbose)

elif args.mode == 'test':

    # ensure that the code uses test_reader
    del train_reader
    del val_reader
    del train_data_gen
    del val_data_gen

    test_reader = PhenotypingReader(dataset_dir=os.path.join(args.data, 'test'),
                                    listfile=os.path.join(args.data, 'test_listfile.csv'))

    test_data_gen = utils.BatchGen(test_reader, discretizer,
                                   normalizer, args.batch_size,
                                   args.small_part, target_repl,
                                   shuffle=False, return_names=True)

    names = []
    ts = []
    labels = []
    predictions = []
    for i in range(test_data_gen.steps):
        print("predicting {} / {}".format(i, test_data_gen.steps), end='\r')
        ret = next(test_data_gen)
        x = ret["data"][0]
        y = ret["data"][1]
        cur_names = ret["names"]
        cur_ts = ret["ts"]
        x = np.array(x)
        pred = model.predict_on_batch(x)
        predictions += list(pred)
        labels += list(y)
        names += list(cur_names)
        ts += list(cur_ts)

    metrics.print_metrics_multilabel(labels, predictions)
    path = os.path.join(args.output_dir, "test_predictions", os.path.basename(args.load_state)) + ".csv"
    utils.save_results(names, ts, predictions, labels, path)

else:
    raise ValueError("Wrong value for args.mode")


### FILE: .\tests\data\mimic3benchmarks\mimic3models\phenotyping\utils.py ###
import numpy as np

from mimic3models import common_utils
import threading
import random
import os


class BatchGen(object):

    def __init__(self, reader, discretizer, normalizer, batch_size,
                 small_part, target_repl, shuffle, return_names=False):
        self.batch_size = batch_size
        self.target_repl = target_repl
        self.shuffle = shuffle
        self.return_names = return_names

        self._load_data(reader, discretizer, normalizer, small_part)

        self.steps = (len(self.data[0]) + batch_size - 1) // batch_size
        self.lock = threading.Lock()
        self.generator = self._generator()

    def _load_data(self, reader, discretizer, normalizer, small_part=False):
        N = reader.get_number_of_examples()
        if small_part:
            N = 1000
        ret = common_utils.read_chunk(reader, N)
        data = ret["X"]
        ts = ret["t"]
        ys = ret["y"]
        names = ret["name"]
        data = [discretizer.transform(X, end=t)[0] for (X, t) in zip(data, ts)]
        if (normalizer is not None):
            data = [normalizer.transform(X) for X in data]
        ys = np.array(ys, dtype=np.int32)
        self.data = (data, ys)
        self.ts = ts
        self.names = names

    def _generator(self):
        B = self.batch_size
        while True:
            if self.shuffle:
                N = len(self.data[1])
                order = list(range(N))
                random.shuffle(order)
                tmp_data = [[None] * N, [None] * N]
                tmp_names = [None] * N
                tmp_ts = [None] * N
                for i in range(N):
                    tmp_data[0][i] = self.data[0][order[i]]
                    tmp_data[1][i] = self.data[1][order[i]]
                    tmp_names[i] = self.names[order[i]]
                    tmp_ts[i] = self.ts[order[i]]
                self.data = tmp_data
                self.names = tmp_names
                self.ts = tmp_ts
            else:
                # sort entirely
                X = self.data[0]
                y = self.data[1]
                (X, y, self.names, self.ts) = common_utils.sort_and_shuffle([X, y, self.names, self.ts], B)
                self.data = [X, y]

            self.data[1] = np.array(self.data[1])  # this is important for Keras
            for i in range(0, len(self.data[0]), B):
                x = self.data[0][i:i+B]
                y = self.data[1][i:i+B]
                names = self.names[i:i + B]
                ts = self.ts[i:i + B]

                x = common_utils.pad_zeros(x)
                y = np.array(y)  # (B, 25)

                if self.target_repl:
                    y_rep = np.expand_dims(y, axis=1).repeat(x.shape[1], axis=1)  # (B, T, 25)
                    batch_data = (x, [y, y_rep])
                else:
                    batch_data = (x, y)

                if not self.return_names:
                    yield batch_data
                else:
                    yield {"data": batch_data, "names": names, "ts": ts}

    def __iter__(self):
        return self.generator

    def next(self):
        with self.lock:
            return next(self.generator)

    def __next__(self):
        return self.next()


def save_results(names, ts, predictions, labels, path):
    n_tasks = 25
    common_utils.create_directory(os.path.dirname(path))
    with open(path, 'w') as f:
        header = ["stay", "period_length"]
        header += ["pred_{}".format(x) for x in range(1, n_tasks + 1)]
        header += ["label_{}".format(x) for x in range(1, n_tasks + 1)]
        header = ",".join(header)
        f.write(header + '\n')
        for name, t, pred, y in zip(names, ts, predictions, labels):
            line = [name]
            line += ["{:.6f}".format(t)]
            line += ["{:.6f}".format(a) for a in pred]
            line += [str(a) for a in y]
            line = ",".join(line)
            f.write(line + '\n')


### FILE: .\tests\data\mimic3benchmarks\mimic3models\phenotyping\__init__.py ###


### FILE: .\tests\data\mimic3benchmarks\mimic3models\phenotyping\logistic\main.py ###
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from mimic3benchmark.readers import PhenotypingReader
from mimic3models import common_utils
from mimic3models import metrics
from mimic3models.phenotyping.utils import save_results

import numpy as np
import argparse
import os
import json


def read_and_extract_features(reader, period, features):
    ret = common_utils.read_chunk(reader, reader.get_number_of_examples())
    # ret = common_utils.read_chunk(reader, 100)
    X = common_utils.extract_features_from_rawdata(ret['X'], ret['header'], period, features)
    return (X, ret['y'], ret['name'], ret['t'])


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--period', type=str, default='all', help='specifies which period extract features from',
                        choices=['first4days', 'first8days', 'last12hours', 'first25percent', 'first50percent', 'all'])
    parser.add_argument('--features', type=str, default='all', help='specifies what features to extract',
                        choices=['all', 'len', 'all_but_len'])
    parser.add_argument('--grid-search', dest='grid_search', action='store_true')
    parser.add_argument('--no-grid-search', dest='grid_search', action='store_false')
    parser.set_defaults(grid_search=False)
    parser.add_argument('--data', type=str, help='Path to the data of phenotyping task',
                        default=os.path.join(os.path.dirname(__file__), '../../../data/phenotyping/'))
    parser.add_argument('--output_dir', type=str, help='Directory relative which all output files are stored',
                        default='.')
    args = parser.parse_args()
    print(args)

    if args.grid_search:
        penalties = ['l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l1', 'l1', 'l1', 'l1', 'l1']
        coefs = [1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001, 1.0, 0.1, 0.01, 0.001, 0.0001]
    else:
        penalties = ['l1']
        coefs = [0.1]

    train_reader = PhenotypingReader(dataset_dir=os.path.join(args.data, 'train'),
                                     listfile=os.path.join(args.data, 'train_listfile.csv'))

    val_reader = PhenotypingReader(dataset_dir=os.path.join(args.data, 'train'),
                                   listfile=os.path.join(args.data, 'val_listfile.csv'))

    test_reader = PhenotypingReader(dataset_dir=os.path.join(args.data, 'test'),
                                    listfile=os.path.join(args.data, 'test_listfile.csv'))

    print('Reading data and extracting features ...')

    (train_X, train_y, train_names, train_ts) = read_and_extract_features(train_reader, args.period, args.features)
    train_y = np.array(train_y)

    (val_X, val_y, val_names, val_ts) = read_and_extract_features(val_reader, args.period, args.features)
    val_y = np.array(val_y)

    (test_X, test_y, test_names, test_ts) = read_and_extract_features(test_reader, args.period, args.features)
    test_y = np.array(test_y)

    print("train set shape:  {}".format(train_X.shape))
    print("validation set shape: {}".format(val_X.shape))
    print("test set shape: {}".format(test_X.shape))

    print('Imputing missing values ...')
    imputer = SimpleImputer(missing_values=np.nan, strategy='mean', copy=True)
    imputer.fit(train_X)
    train_X = np.array(imputer.transform(train_X), dtype=np.float32)
    val_X = np.array(imputer.transform(val_X), dtype=np.float32)
    test_X = np.array(imputer.transform(test_X), dtype=np.float32)

    print('Normalizing the data to have zero mean and unit variance ...')
    scaler = StandardScaler()
    scaler.fit(train_X)
    train_X = scaler.transform(train_X)
    val_X = scaler.transform(val_X)
    test_X = scaler.transform(test_X)

    n_tasks = 25
    result_dir = os.path.join(args.output_dir, 'results')
    common_utils.create_directory(result_dir)

    for (penalty, C) in zip(penalties, coefs):
        model_name = '{}.{}.{}.C{}'.format(args.period, args.features, penalty, C)

        train_activations = np.zeros(shape=train_y.shape, dtype=float)
        val_activations = np.zeros(shape=val_y.shape, dtype=float)
        test_activations = np.zeros(shape=test_y.shape, dtype=float)

        for task_id in range(n_tasks):
            print('Starting task {}'.format(task_id))

            logreg = LogisticRegression(penalty=penalty, C=C, random_state=42)
            logreg.fit(train_X, train_y[:, task_id])

            train_preds = logreg.predict_proba(train_X)
            train_activations[:, task_id] = train_preds[:, 1]

            val_preds = logreg.predict_proba(val_X)
            val_activations[:, task_id] = val_preds[:, 1]

            test_preds = logreg.predict_proba(test_X)
            test_activations[:, task_id] = test_preds[:, 1]

        with open(os.path.join(result_dir, 'train_{}.json'.format(model_name)), 'w') as f:
            ret = metrics.print_metrics_multilabel(train_y, train_activations)
            ret = {k: float(v) for k, v in ret.items() if k != 'auc_scores'}
            json.dump(ret, f)

        with open(os.path.join(result_dir, 'val_{}.json'.format(model_name)), 'w') as f:
            ret = metrics.print_metrics_multilabel(val_y, val_activations)
            ret = {k: float(v) for k, v in ret.items() if k != 'auc_scores'}
            json.dump(ret, f)

        with open(os.path.join(result_dir, 'test_{}.json'.format(model_name)), 'w') as f:
            ret = metrics.print_metrics_multilabel(test_y, test_activations)
            ret = {k: float(v) for k, v in ret.items() if k != 'auc_scores'}
            json.dump(ret, f)

        save_results(test_names, test_ts, test_activations, test_y,
                     os.path.join(args.output_dir, 'predictions', model_name + '.csv'))


if __name__ == '__main__':
    main()


### FILE: .\tests\data\mimic3benchmarks\mimic3models\phenotyping\logistic\__init__.py ###


### FILE: .\tests\data\physionet.org\files\mimiciii-demo\1.4\convert_columns.py ###
import pandas as pd
from datasets.mimic_utils import upper_case_column_names
from tests.settings import *

for csv in TEST_DATA_DEMO.iterdir():
    if csv.is_dir() or csv.suffix != ".csv":
        continue

    df = pd.read_csv(csv,
                     dtype={
                         "ROW_ID": 'Int64',
                         "ICUSTAY_ID": 'Int64',
                         "HADM_ID": 'Int64',
                         "SUBJECT_ID": 'Int64',
                         "row_id": 'Int64',
                         "icustay_id": 'Int64',
                         "hadm_id": 'Int64',
                         "subject_id": 'Int64'
                     },
                     low_memory=False)
    df = upper_case_column_names(df)
    df.to_csv(csv, index=False)


### FILE: .\tests\etc\convert_columns.py ###
import pandas as pd
from datasets.mimic_utils import upper_case_column_names
from tests.settings import *

for csv in TEST_DATA_DEMO.iterdir():
    if csv.is_dir() or csv.suffix != ".csv":
        continue

    df = pd.read_csv(csv,
                     dtype={
                         "ROW_ID": 'Int64',
                         "ICUSTAY_ID": 'Int64',
                         "HADM_ID": 'Int64',
                         "SUBJECT_ID": 'Int64',
                         "row_id": 'Int64',
                         "icustay_id": 'Int64',
                         "hadm_id": 'Int64',
                         "subject_id": 'Int64'
                     },
                     low_memory=False)
    df = upper_case_column_names(df)
    df.to_csv(csv, index=False)


### FILE: .\tests\etc\discretize_data.py ###
import os
import sys

sys.path.append(os.getenv("WORKINGDIR"))
import pandas as pd
from pathlib import Path
from tests.settings import *
from mimic3benchmark.readers import DecompensationReader

from mimic3models.preprocessing import Discretizer
from mimic3benchmark.readers import InHospitalMortalityReader, DecompensationReader, LengthOfStayReader, PhenotypingReader

# Set the paths to the data files
processed_paths = {
    "IHM": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "in-hospital-mortality"),
    "LOS": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "length-of-stay"),
    "PHENO": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "phenotyping"),
    "DECOMP": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "decompensation")
}

discretized_paths = {
    "IHM": Path(TEST_DATA_DIR, "generated-benchmark", "discretized", "in-hospital-mortality"),
    "LOS": Path(TEST_DATA_DIR, "generated-benchmark", "discretized", "length-of-stay"),
    "PHENO": Path(TEST_DATA_DIR, "generated-benchmark", "discretized", "phenotyping"),
    "DECOMP": Path(TEST_DATA_DIR, "generated-benchmark", "discretized", "decompensation")
}

readers = {
    "IHM": InHospitalMortalityReader,
    "LOS": LengthOfStayReader,
    "PHENO": PhenotypingReader,
    "DECOMP": DecompensationReader
}

impute_strategies = ['zero', 'normal_value', 'previous', 'next']
start_times = ['zero', 'relative']

# Discritize the data from processed directory using different discretizer settings
for task in TASK_NAMES:
    list_file_path = Path(processed_paths[task], "listfile.csv")
    list_file = pd.read_csv(list_file_path)
    if task in ["IHM", "PHENO"]:
        example_indices = list_file.index
    else:
        example_indices = list_file.groupby("stay")["period_length"].idxmax().values
    discretized_paths[task].mkdir(parents=True, exist_ok=True)
    list_file.loc[example_indices].to_csv(Path(discretized_paths[task], "listfile.csv"),
                                          index=False)

    print(f"Discretizing {task} data")
    for impute_strategy in impute_strategies:
        for start_time in start_times:
            discretizer = Discretizer(timestep=1.0,
                                      store_masks=False,
                                      impute_strategy=impute_strategy,
                                      start_time=start_time)
            reader = readers[task](dataset_dir=processed_paths[task])
            for idx in example_indices:
                sample = reader.read_example(idx)
                discretized_data = discretizer.transform(sample['X'])
                discretizer_header = discretized_data[1].split(',')
                target_dir = Path(discretized_paths[task],
                                  f"imp{impute_strategy}_start{start_time}")
                target_dir.mkdir(parents=True, exist_ok=True)
                pd.DataFrame(discretized_data[0],
                             columns=discretizer_header).to_csv(Path(target_dir, sample['name']),
                                                                index=False)


### FILE: .\tests\etc\engineer_data.py ###
import os
import sys

sys.path.append(os.getenv("WORKINGDIR"))
import pandas as pd
import numpy as np
from pathlib import Path
from tests.settings import TEST_DATA_DIR, TASK_NAMES
# This is copied into the mimic3benchmark directory once cloned
from mimic3benchmark.readers import InHospitalMortalityReader, DecompensationReader, LengthOfStayReader, PhenotypingReader
from mimic3models.in_hospital_mortality.logistic.main import read_and_extract_features as ihm_extractor
from mimic3models.phenotyping.logistic.main import read_and_extract_features as phenotyping_extractor
from mimic3models.length_of_stay.logistic.main import read_and_extract_features as los_extractor
from mimic3models.decompensation.logistic.main import read_and_extract_features as decompensation_extractor

# Set the paths to the data files
processed_paths = {
    "IHM": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "in-hospital-mortality"),
    "LOS": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "length-of-stay"),
    "PHENO": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "phenotyping"),
    "DECOMP": Path(TEST_DATA_DIR, "generated-benchmark", "processed", "decompensation")
}

engineered_paths = {
    "IHM": Path(TEST_DATA_DIR, "generated-benchmark", "engineered", "in-hospital-mortality"),
    "LOS": Path(TEST_DATA_DIR, "generated-benchmark", "engineered", "length-of-stay"),
    "PHENO": Path(TEST_DATA_DIR, "generated-benchmark", "engineered", "phenotyping"),
    "DECOMP": Path(TEST_DATA_DIR, "generated-benchmark", "engineered", "decompensation")
}

readers = {
    "IHM": InHospitalMortalityReader,
    "LOS": LengthOfStayReader,
    "PHENO": PhenotypingReader,
    "DECOMP": DecompensationReader
}

extractors = {
    "IHM": ihm_extractor,
    "LOS": los_extractor,
    "PHENO": phenotyping_extractor,
    "DECOMP": decompensation_extractor
}

# Create the readers for each task type
ihm_reader = InHospitalMortalityReader(dataset_dir=processed_paths["IHM"],
                                       listfile=Path(processed_paths["IHM"], "listfile.csv"))
decomp_reader = DecompensationReader(dataset_dir=processed_paths["DECOMP"],
                                     listfile=Path(processed_paths["DECOMP"], "listfile.csv"))
los_reader = LengthOfStayReader(dataset_dir=processed_paths["LOS"],
                                listfile=Path(processed_paths["LOS"], "listfile.csv"))
phenotyping_reader = PhenotypingReader(dataset_dir=processed_paths["PHENO"],
                                       listfile=Path(processed_paths["PHENO"], "listfile.csv"))

for task in TASK_NAMES:
    if engineered_paths[task].exists():
        continue
    if task in ["LOS", "DECOMP"]:
        print(f"Engineering data for task: {task}. This may take up to 30 min ...")
    else:
        print(f"Engineering data for task: {task}.")
    reader = readers[task](dataset_dir=processed_paths[task],
                           listfile=Path(processed_paths[task], "listfile.csv"))
    if task == "IHM":
        (X, y, train_names) = extractors[task](reader, period="all", features="all")
    elif task == "PHENO":
        (X, y, train_names, ts) = extractors[task](reader, period="all", features="all")
    else:
        n_samples = min(100000, reader.get_number_of_examples())
        (X, y, train_names, ts) = extractors[task](reader,
                                                   period="all",
                                                   features="all",
                                                   count=n_samples)

    print(f"Done engineering data for task: {task}.")
    # 10127_episode271544_timeseries.csv is included in the original DS despite the subject being a new born infant. Minimum age was set at 18
    X_df = pd.DataFrame(np.concatenate([np.array(train_names).reshape(-1, 1), X],
                                       axis=1)).set_index(0)
    if task == "IHM":
        y_df = pd.DataFrame(np.stack([np.array(train_names), y]).T).set_index(0)
    elif task == "PHENO":
        y_df = pd.DataFrame(
            np.concatenate([np.array(train_names).reshape(-1, 1),
                            np.array(y)], axis=1)).set_index(0)
    else:
        y_df = pd.DataFrame(
            np.concatenate([np.array(train_names).reshape(-1, 1),
                            np.array(y).reshape(-1, 1)],
                           axis=1)).set_index(0)
    engineered_paths[task].mkdir(parents=True, exist_ok=True)
    X_df.to_csv(str(Path(engineered_paths[task], "X.csv")))
    y_df.to_csv(str(Path(engineered_paths[task], "y.csv")))


### FILE: .\tests\etc\rename_files.py ###
import pdb
import re
import os
import pandas as pd
from pathlib import Path
import sys
sys.path.append(os.getenv("WORKINGDIR"))
from tests.settings import *


for subject_dir in Path(TEST_DATA_DIR, "generated-benchmark", "extracted").iterdir():
    if not subject_dir.is_dir():
        continue
    for csv in subject_dir.iterdir():
        if "episode" in csv.name and not "timeseries" in csv.name:
            df = pd.read_csv(csv)
            icustay_id = df["Icustay"].iloc[0]
            file_index = re.findall(r'\d+', csv.name).pop()
            csv.rename(Path(csv.parent, f"episode{icustay_id}.csv"))
            Path(csv.parent, f"episode{file_index}_timeseries.csv").rename(Path(csv.parent, f"episode{icustay_id}_timeseries.csv"))
            # I want to rename the files episodeX.csv and episodeX_timeseries.csv by replacing X, which in this case is a random number by the icustay


### FILE: .\tests\etc\revert_split.py ###
import os
from pathlib import Path
import pandas as pd
import sys

sys.path.append(os.getenv("WORKINGDIR"))
from tests.settings import *

result_paths = [
    Path(TEST_DATA_DIR, "generated-benchmark", "extracted"),
    Path(TEST_DATA_DIR, "generated-benchmark", "processed", "in-hospital-mortality"),
    Path(TEST_DATA_DIR, "generated-benchmark", "processed", "length-of-stay"),
    Path(TEST_DATA_DIR, "generated-benchmark", "processed", "multitask"),
    Path(TEST_DATA_DIR, "generated-benchmark", "processed", "phenotyping"),
    Path(TEST_DATA_DIR, "generated-benchmark", "processed", "decompensation"),
]

for path in result_paths:
    for entity in path.iterdir():
        if entity.is_dir():
            for subject_entity in entity.iterdir():
                if subject_entity.is_dir():
                    for csv in subject_entity.iterdir():
                        target_path = Path(csv.parents[2], csv.parent.name, csv.name)
                        target_path.parent.mkdir(parents=True, exist_ok=True)
                        csv.rename(target_path)
                    subject_entity.rmdir()
                else:
                    target_path = Path(subject_entity.parents[1], subject_entity.name)
                    if subject_entity.name == "listfile.csv" and target_path.exists():
                        listfile_df = pd.read_csv(subject_entity)
                        listfile_df.to_csv(target_path, mode='a', index=False, header=False)
                        subject_entity.unlink()
                    else:
                        subject_entity.rename(target_path)
            entity.rmdir()


### FILE: .\tests\etc\__init__.py ###


### FILE: .\tests\pytest_utils\discretization.py ###
import pytest
import datasets
import re
import shutil

import pandas as pd
import numpy as np
from pathlib import Path
from datasets.readers import ProcessedSetReader
from datasets.writers import DataSetWriter
from datasets.processors.discretizers import MIMICDiscretizer
from tests.pytest_utils.general import assert_dataframe_equals
from utils.IO import *
from tests.settings import *


def prepare_processed_data(task_name: str, listfile: pd.DataFrame, reader: ProcessedSetReader):
    # Legacy period length calculation may cut off legitimate data
    source_path = Path(SEMITEMP_DIR, "processed", task_name)
    target_path = Path(TEMP_DIR, "processed", task_name)
    write = DataSetWriter(target_path)
    X_processed, y_processed = reader.read_samples(read_ids=True).values()
    if "period_length" in listfile.columns:
        for subject_id in X_processed:
            for stay_ids in X_processed[subject_id]:
                X = X_processed[subject_id][stay_ids]
                X = X[X.index < listfile.loc[f"{subject_id}_episode{stay_ids}_timeseries.csv"]
                      ["period_length"] + 1e-6]
                X_processed[subject_id][stay_ids] = X

    write.write_bysubject({"X": X_processed})
    write.write_bysubject({"y": y_processed})

    for file in source_path.iterdir():
        if not file.is_file():
            continue
        shutil.copy(str(file), str(target_path))
    return


def assert_strategy_equals(X_strategy: dict, test_data_dir: Path):
    tested_stay = set()
    subject_count = 0
    stay_count = 0

    tests_io(f"Subjects tested: {subject_count}\n"
             f"Stays tested: {stay_count}")

    for test_file_path in test_data_dir.iterdir():
        test_df = pd.read_csv(test_file_path)
        test_df.index.name = "bins"
        test_df = test_df.reset_index()

        # Extract subject id and stay id
        match = re.search(r"(\d+)_episode(\d+)_timeseries\.csv", test_file_path.name)
        subject_id = int(match.group(1))
        stay_id = int(match.group(2))
        tested_stay.add(stay_id)

        # Read sample
        subject_count += int(all([stay in tested_stay for stay in X_strategy[subject_id]]))
        stay_count += 1

        X = X_strategy[subject_id][stay_id]

        # Ensure column identity
        test_df.columns = test_df.columns.str.strip(" ")
        X.columns = X.columns.str.strip(" ")

        missing_columns = set(test_df) - set(X)
        additional_columns = set(X) - set(test_df)
        assert not missing_columns
        assert not additional_columns

        # Test X against ground truth
        test_df = test_df[X.columns]
        assert_dataframe_equals(X.astype(float), test_df)

        tests_io(f"Subjects tested: {subject_count}\n"
                 f"Stays tested: {stay_count}",
                 flush_block=True)


### FILE: .\tests\pytest_utils\extraction.py ###
import pandas as pd
from typing import Dict
from utils.IO import *
from tests.settings import *
from tests.pytest_utils.general import assert_dataframe_equals


def compare_extracted_datasets(generated_data: Dict[str, Dict[str, Dict[str, pd.DataFrame]]],
                               test_data: Dict[str, Dict[str, Dict[str, pd.DataFrame]]]):
    # Filtypes are: timeseries, subject_events, episodic_data, subject_icu_stays
    tests_io(f"Comparing extracted datasets\n"
             f"Compared subjects: {0}\n"
             f"Compared timeseries: {0}\n"
             f"Compared subject events: {0}\n"
             f"Compared episodic data: {0}"
             f"Compared stay data: {0}")
    n_subjects = 0
    n_timeseries = 0
    n_subject_events = 0
    n_episodic_data = 0
    n_stay_data = 0
    for subject_id, subject_data in generated_data.items():
        for file_type, type_data in subject_data.items():
            if file_type == "timeseries":
                for stay_id, stay_data in type_data.items():
                    # Timeseries is structured
                    assert_dataframe_equals(stay_data, test_data[subject_id][file_type][stay_id])
                    n_timeseries += 1
            elif file_type == "subject_events":
                sorted_type_data = type_data.sort_values(
                    by=["SUBJECT_ID", "HADM_ID", "ICUSTAY_ID", "CHARTTIME", "ITEMID"])
                sorted_type_data = sorted_type_data.reset_index(drop=True)
                sorted_gt_data = test_data[subject_id][file_type].sort_values(
                    by=["SUBJECT_ID", "HADM_ID", "ICUSTAY_ID", "CHARTTIME", "ITEMID"])
                sorted_gt_data = sorted_gt_data.reset_index(drop=True)
                assert_dataframe_equals(sorted_type_data, sorted_gt_data)
                n_subject_events += 1
            else:
                assert_dataframe_equals(type_data, test_data[subject_id][file_type])
                if file_type == "episodic_data":
                    n_episodic_data += 1
                else:
                    n_stay_data += 1

            tests_io(
                f"Comparing extracted datasets\n"
                f"Compared subjects: {n_subjects}\n"
                f"Compared timeseries: {n_timeseries}\n"
                f"Compared subject events: {n_subject_events}\n"
                f"Compared episodic data: {n_episodic_data}\n"
                f"Compared stay data: {n_stay_data}",
                flush_block=True)

        n_subjects += 1

    tests_io(
        f"Comparing extracted datasets\n"
        f"Compared subjects: {n_subjects}\n"
        f"Compared timeseries: {n_timeseries}\n"
        f"Compared subject events: {n_subject_events}\n"
        f"Compared episodic data: {n_episodic_data}\n"
        f"Compared stay data: {n_stay_data}",
        flush_block=True)


### FILE: .\tests\pytest_utils\feature_engineering.py ###
import re
import pandas as pd
import numpy as np


def extract_test_ids(df: pd.DataFrame):
    regex = r"(\d+)_episode(\d+)_timeseries\.csv"
    df["SUBJECT_ID"] = df["0"].apply(lambda x: re.search(regex, x).group(1))
    df["ICUSTAY_ID"] = df["0"].apply(lambda x: re.search(regex, x).group(2))
    df = df.drop("0", axis=1)
    return df


def concatenate_dataset(data) -> pd.DataFrame:
    data_stack = list()
    for subject_id, subject_stays in data.items():
        for stay_id, frame in subject_stays.items():
            if len(np.squeeze(frame).shape) == 1:
                data_stack.append(np.squeeze(frame).tolist() + [subject_id, stay_id])
            else:
                for row in np.squeeze(frame).tolist():
                    data_stack.append(row + [subject_id, stay_id])

    dfs = pd.DataFrame(data_stack,
                       columns=[str(idx) for idx in range(1, 715)] + ["SUBJECT_ID", "ICUSTAY_ID"])

    if not len(dfs):
        return
    return dfs


### FILE: .\tests\pytest_utils\general.py ###
import pandas as pd
import numpy as np
from utils.IO import *
from utils import is_numerical, is_colwise_numerical


def assert_dataframe_equals(generated_df: pd.DataFrame,
                            test_df: pd.DataFrame,
                            rename: dict = {},
                            normalize_by: str = "generated",
                            compare_mode: str = "single_entry"):
    """Compares the dataframes and print out the results. The age column cannot be compared, as the results are
    erroneous in the original dataset.

    Args:
        generated_df (pd.DataFrame): DataFrame generated by the MIMIC-III-Rework
        test_df (pd.DataFrame): DataFrame generated by the original MIMIC-III code.
        rename (dict, optional): Renaming of columns if necessary. Defaults to {}.
        normalize_by (str, optional): Define by which dataframe to structure the comparision. Can be 'generated' or 'groundtruth'. Defaults to "generated".
        compare_mode (str, optional): Define wether to compare data in absolute or proxemity terms. Data from numpy data files should be compared using the 
                                      proxemity mode. Can be 'absolute' or 'proxemity'. Defaults to "abolute".
    """
    assert normalize_by in ['generated', 'groundtruth'
                           ], "normalize_by needs to be one of 'generated' or 'groundtruth'."
    assert compare_mode in ['single_entry', 'multiline'
                           ], "compare_mode needs to be one of 'single_entry' or 'multiline'."

    # Assert shape 2
    assert len(generated_df.columns) == len(test_df.columns), (
        f"Generated and ground truth dataframes do not have the same amount of columns.\n"
        f"Generated: {len(generated_df.columns)}, Ground truth: {len(test_df.columns)}.")

    # Assert shape 1
    if not len(generated_df) == len(test_df):
        tests_io("Dataframe has diff!")
        difference = pd.concat([test_df, generated_df]).drop_duplicates(keep=False)
        tests_io(
            f"Length generated: {len(generated_df)}\nLength test: {len(test_df)}\nDiff is: \n{difference}"
        )
        assert len(generated_df) == len(test_df), (
            f"Generated and ground truth dataframes do not have the same amount of rows.\n"
            f"Generated: {len(generated_df)}, Ground truth: {len(test_df)}.")

    generated_df = generated_df.rename(columns=rename)
    if normalize_by == "generated":
        # Age erroneous in test code
        if "AGE" in generated_df:
            generated_df = generated_df.drop(columns=["AGE"])
        elif "Age" in test_df:
            generated_df = generated_df.drop(columns=["Age"])
        test_df = test_df[generated_df.columns]
    else:
        # Age erroneous in test code
        if "Age" in test_df:
            test_df = test_df.drop(columns=["Age"])
        elif "AGE" in test_df:
            test_df = test_df.drop(columns=["AGE"])
        generated_df = generated_df[test_df.columns]

    # Check dtypes TODO! used to work but fails on extraction because IDs are floats
    # assert (test_df.dtypes == generated_df.dtypes).all(), (
    #     f"Generated and ground truth dataframes do not have the same dtypes.\n"
    #     f"Generated: {generated_df.dtypes[(test_df.dtypes != generated_df.dtypes)]}, Ground truth: { test_df.dtypes[(test_df.dtypes != generated_df.dtypes)]}."
    # )

    # Check performed column wise
    if compare_mode == "single_entry":
        difference = 0
        frame_diff = (generated_df.fillna("NaN") != test_df.fillna("NaN"))
        for column_name in frame_diff:
            if not frame_diff[column_name].any():
                continue
            gen_col = generated_df[[column_name]]
            test_col = test_df[[column_name]]

            # Absolute vs threshold comparision depending on dtype:
            if is_numerical(gen_col) or is_numerical(test_col):
                # Checking for diffs with general float type
                # Casting float because pandas float and integer are not supported
                # Pandas has no in-house implementation of isclose
                column_diff = np.squeeze(
                    ~np.isclose(gen_col.astype(float), test_col.astype(float), equal_nan=True))
            else:
                column_diff = np.squeeze((gen_col.fillna("Na") != \
                                          test_col.fillna("Na")).values)
            if column_diff.shape == ():
                column_diff = column_diff.reshape(1)
            # If all identical continue
            if not column_diff.any():
                continue
            difference += 1
            display_diff_df = pd.concat([gen_col, test_col], axis=1)
            display_diff_df.index = gen_col.index
            display_diff_df.columns = ["Generated", "Groundtruth"]
            display_diff_df = display_diff_df[column_diff]
            tests_io(f"For column {column_name}:\n" f"-------------------\n" f"{display_diff_df}")
    elif compare_mode == "multiline":
        # For every subject and stay id
        stay_count = 0
        difference = 0
        sample_count = 0
        subject_buffer = list()
        if "SUBJECT_ID" in generated_df and "ICUSTAY_ID" in generated_df:
            is_numerical_dict = is_colwise_numerical(test_df)
            is_numerical_dict = {
                column: is_numerical_dict[column] or value
                for column, value in is_colwise_numerical(generated_df).items()
            }
            tests_io(f"Total stays checked: {stay_count}\n"
                     f"Total subjects checked: {len(set(subject_buffer))}\n"
                     f"Total samples checked: {sample_count}\n"
                     f"Total differences found: {difference}")
            for subject_id in generated_df["SUBJECT_ID"].unique():
                # Get all rows with the same subject_id
                subject_data = generated_df[generated_df["SUBJECT_ID"] == subject_id]
                test_subject_data = test_df[test_df["SUBJECT_ID"] == subject_id]
                for icustay_id in subject_data["ICUSTAY_ID"].unique():
                    # Get all rows with the same icustay_id
                    gen_rows = subject_data[subject_data["ICUSTAY_ID"] == icustay_id]
                    test_rows = test_subject_data[test_subject_data["ICUSTAY_ID"] == icustay_id]
                    for _, gen_row in gen_rows.iterrows():
                        diff = compare_homogenous_multiline_df(gen_row.to_frame().T, test_rows)

                        if diff:
                            tests_io(
                                f"For file {subject_id}_episode{icustay_id}_timeseries.csv, no equivalent entry in test_df found:\n"
                                f"-------------------\n"
                                f"{gen_row}")
                            difference += 1
                        sample_count += 1
                        tests_io(
                            f"Total stays checked: {stay_count}\n"
                            f"Total subjects checked: {len(set(subject_buffer))}\n"
                            f"Total samples checked: {sample_count}\n"
                            f"Total differences found: {difference}",
                            flush_block=True)

                    # Progress variables
                    stay_count += 1
                    subject_buffer.append(subject_id)
        else:
            # Find a matching row in test_df with the same index
            difference = compare_homogenous_multiline_df(generated_df, test_df, verbose=True)

    assert not difference, f"Diffs detected between generated and ground truth files: {difference}!"


def split_dataframes_by_type(df: pd.DataFrame, numerical_dict: dict):
    numerical_cols = [col for col, is_num in numerical_dict.items() if is_num]
    non_numerical_cols = [col for col, is_num in numerical_dict.items() if not is_num]
    return df[numerical_cols].astype(float), df[non_numerical_cols].astype("object")


def compare_homogenous_multiline_df(generated_df: pd.DataFrame,
                                    test_df: pd.DataFrame,
                                    verbose: bool = False):
    numerical_dict = is_colwise_numerical(test_df)
    numerical_dict.update({
        column: numerical_dict.get(column, False) or value
        for column, value in is_colwise_numerical(generated_df).items()
    })

    # Split both DataFrames
    gen_num_df, gen_non_num_df = split_dataframes_by_type(generated_df, numerical_dict)
    test_num_df, test_non_num_df = split_dataframes_by_type(test_df, numerical_dict)

    difference = 0
    sample_count = 0

    # Iterate over generated DataFrame rows
    for idx in gen_num_df.index:
        gen_num_row = gen_num_df.loc[idx]
        gen_non_num_row = gen_non_num_df.loc[idx]
        num_match = np.isclose(test_num_df.astype(float).values,
                               gen_num_row.astype(float).values,
                               equal_nan=True).all(axis=1)
        non_num_match = (gen_non_num_row.fillna("NaN") == test_non_num_df.fillna("NaN")).all(
            axis=1).values
        found = (num_match & non_num_match).any()
        if not found:
            # I don't remember why I did this, its been over a month
            difference += 1
            raise LookupError(
                f"No equivalent entry found for row index {idx}:\n{generated_df.loc[idx].to_frame().T}"
            )
        sample_count += 1
        if verbose:
            info_io(f"Total samples checked: {sample_count}\nTotal differences found: {difference}",
                    flush_block=True)

    return difference


### FILE: .\tests\pytest_utils\preprocessing.py ###
import shelve
import pandas as pd
import numpy as np
from pathlib import Path
from utils.IO import *
from tests.pytest_utils.general import assert_dataframe_equals
from datasets.readers import ProcessedSetReader
from tests.settings import *


def assert_reader_equals(reader: ProcessedSetReader, test_data_dir: Path):
    """_summary_

    Args:
        reader (SampleSetReader): _description_
        test_data_dir (Path): _description_
    """
    assert reader.subject_ids, "The reader subjects are empty! Extraction must have failed."
    subject_count = 0
    stay_count = 0
    tests_io(f"Stays frames compared: {stay_count}\n"
             f"Total subjects checked: {subject_count}")
    listfile = pd.read_csv(Path(test_data_dir, "listfile.csv"))
    for subject_id in reader.subject_ids:
        X_stays, y_stays = reader.read_sample(subject_id, read_ids=True).values()
        subject_count += 1
        stay_count += assert_subject_data_equals(subject_id=subject_id,
                                                 X_stays=X_stays,
                                                 y_stays=y_stays,
                                                 test_data_dir=test_data_dir,
                                                 root_path=reader.root_path,
                                                 listfile=listfile)
        tests_io(f"Compared subjects: {subject_count}\n"
                 f"Compared stays: {stay_count}\n",
                 flush_block=True)


def assert_dataset_equals(X: dict, y: dict, generated_dir: Path, test_data_dir: Path):
    """_summary_

    Args:
        reader (SampleSetReader): _description_
        test_data_dir (Path): _description_
    """
    assert len(X) and len(y), "The reader subjects are empty! Extraction must have failed."
    subject_count = 0
    stay_count = 0
    tests_io(f"Stays frames compared: {stay_count}\n"
             f"Total subjects checked: {subject_count}")
    listfile = pd.read_csv(Path(test_data_dir, "listfile.csv"))
    for subject_id in X:
        X_stays, y_stays = X[subject_id], y[subject_id]
        subject_count += 1
        stay_count += assert_subject_data_equals(subject_id=subject_id,
                                                 X_stays=X_stays,
                                                 y_stays=y_stays,
                                                 test_data_dir=test_data_dir,
                                                 root_path=generated_dir,
                                                 listfile=listfile)
        tests_io(f"Compared subjects: {subject_count}\n"
                 f"Compared stays: {stay_count}\n",
                 flush_block=True)


def assert_subject_data_equals(subject_id: int,
                               X_stays: dict,
                               y_stays: dict,
                               test_data_dir: Path,
                               root_path: Path,
                               listfile: pd.DataFrame = None):
    stay_count = 0
    for stay_id in X_stays.keys():
        X = X_stays[stay_id]
        y = y_stays[stay_id]
        try:
            test_data = pd.read_csv(
                Path(test_data_dir, f"{subject_id}_episode{stay_id}_timeseries.csv"))
        except:
            raise FileNotFoundError(f"Test set is missing {subject_id}"
                                    "_episode{stay_id}_timeseries.csv")
        y_true = listfile[listfile["stay"] == f"{subject_id}_episode{stay_id}_timeseries.csv"]
        if len(y) == 1:
            if "y_true" in y_true:
                assert np.isclose(float(np.squeeze(y_true["y_true"].values)), float(np.squeeze(y)))
            else:
                y_true_vals = y_true[[
                    value for value in y_true.columns if not value in ["stay", "period_length"]
                ]].values
                assert np.allclose(np.squeeze(y_true_vals), np.squeeze(y.values))
        else:
            if "y_true" in y_true:
                y_true = y_true.sort_values(by=["period_length"])
                assert len(
                    y_true) == y_true["period_length"].max() - y_true["period_length"].min() + 1
                assert np.allclose(y_true["y_true"].values, np.squeeze(y.values))

        # Testing the preprocessing tracker at the same time
        with shelve.open(str(Path(root_path, "progress"))) as db:
            assert int(subject_id) in db["subjects"]
            assert int(stay_id) in db["subjects"][subject_id]
            assert db["subjects"][subject_id][stay_id] == len(y)
            assert db["subjects"][subject_id]["total"] == sum([
                lengths for stay_id, lengths in db["subjects"][subject_id].items()
                if stay_id != "total"
            ])

        assert_dataframe_equals(X.reset_index(),
                                test_data, {"hours": "Hours"},
                                normalize_by="groundtruth")
        stay_count += 1
    return stay_count


### FILE: .\tests\pytest_utils\__init__.py ###
import shutil
from pathlib import Path
from tests.settings import *


def copy_dataset(folder: Path):
    if not Path(TEMP_DIR, folder).is_dir():
        source_path = Path(SEMITEMP_DIR, folder)
        target_path = Path(TEMP_DIR, folder)
        source_path.parent.mkdir(parents=True, exist_ok=True)
        target_path.parent.mkdir(parents=True, exist_ok=True)
        shutil.copytree(str(Path(SEMITEMP_DIR, folder)), str(Path(TEMP_DIR, folder)))


### FILE: .\tests\test_case_handler\test_mimic_case_handler.py ###
import datasets
import os
import pandas as pd
import json
import shutil
from pathlib import Path
from utils.IO import *
from tests.settings import *

base_path = Path(TEST_DATA_DIR, "configs", "case_configs")

test_config_paths = [
    Path(base_path, "logistic_decomp"),
    Path(base_path, "logistic_ihm"),
    Path(base_path, "logistic_los"),
    Path(base_path, "logistic_phenotyping"),
    Path(base_path, "logistic_subcases")
]

# def test_folder_creation_subcases():


### FILE: .\tests\test_case_handler\__init__.py ###


### FILE: .\tests\test_datasets\test_data_split.py ###
"""
TODO! Add current iter and test restoral
"""

import datasets
import shutil
import pytest
import random
import pandas as pd
import numpy as np
from typing import List, Dict
from pathlib import Path
from utils.IO import *
from tests.settings import *
from datasets.readers import ProcessedSetReader, SplitSetReader
from sklearn import model_selection
from pathlib import Path
from datasets.trackers import DataSplitTracker, PreprocessingTracker
from pathos.multiprocessing import Pool, cpu_count
from datasets.split.splitters import ReaderSplitter
from utils import dict_subset, is_numerical


@pytest.mark.parametrize("preprocessing_style", ["discretized", "engineered"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_ratio_split(
    task_name,
    preprocessing_style,
    discretized_readers: Dict[str, ProcessedSetReader],
    engineered_readers: Dict[str, ProcessedSetReader],
):
    tests_io(f"Test case by ratio for task: {task_name}", level=0)
    # Discretization or feature engineering
    if preprocessing_style == "discretized":
        reader = discretized_readers[task_name]
    else:
        reader = engineered_readers[task_name]

    tolerance = (1e-2 if task_name in ["DECOMP", "LOS"] else 1e-1)
    for test_size in [-0.1, 1.1]:
        with pytest.raises(ValueError):
            _: SplitSetReader = datasets.train_test_split(reader, test_size=test_size, val_size=0.2)
    for val_size in [-0.1, 1.1]:
        with pytest.raises(ValueError):
            _: SplitSetReader = datasets.train_test_split(reader, test_size=0.1, val_size=val_size)

    reader = discretized_readers[task_name]
    all_split_subjects = dict()
    curr_iter = 0

    # Test splitting for a few values
    for test_size in [0.0, 0.2, 0.4]:
        all_split_subjects[test_size] = dict()
        for val_size in [0.0, 0.2, 0.4]:
            all_split_subjects[test_size][val_size] = assert_split_sanity(
                test_size, val_size, tolerance, reader,
                datasets.train_test_split(reader,
                                          test_size=test_size,
                                          val_size=val_size,
                                          storage_path=Path(TEMP_DIR, "split_test",
                                                            str(curr_iter))))
            curr_iter += 1

    # Test restoring splits for previous values
    for test_size in [0.0, 0.2, 0.4]:
        for val_size in [0.0, 0.2, 0.4]:
            split_reader = datasets.train_test_split(reader,
                                                     test_size=test_size,
                                                     val_size=val_size,
                                                     storage_path=Path(
                                                         TEMP_DIR, "split_test", str(curr_iter)))
            for split_name in all_split_subjects[test_size][val_size]:
                assert set(all_split_subjects[test_size][val_size][split_name]) == \
                    set(getattr(split_reader, split_name).subject_ids)

            curr_iter += 1


@pytest.mark.parametrize("preprocessing_style", ["discretized", "engineered"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_demographic_filter(
    task_name,
    preprocessing_style,
    discretized_readers: Dict[str, ProcessedSetReader],
    engineered_readers: Dict[str, ProcessedSetReader],
):
    tests_io("Test case by demographic filter", level=0)
    # Discretization or feature engineering
    if preprocessing_style == "discretized":
        reader = discretized_readers[task_name]
    else:
        reader = engineered_readers[task_name]

    # Test on unknown demographic
    with pytest.raises(ValueError):
        demographic_filter = {"INVALID": {"less": 0.5}}
        _: SplitSetReader = datasets.train_test_split(reader, demographic_filter=demographic_filter)

    # Test on invalid range
    for less_key in ["less", "leq"]:
        for greater_key in ["greater", "geq"]:
            with pytest.raises(ValueError):
                demographic_filter = {"AGE": {less_key: 10, greater_key: 90}}
                _: SplitSetReader = datasets.train_test_split(reader,
                                                              demographic_filter=demographic_filter)

    subject_info_df = pd.read_csv(Path(reader.root_path, "subject_info.csv"))
    subject_info_df = subject_info_df[subject_info_df["SUBJECT_ID"].isin(reader.subject_ids)]
    curr_iter = 0
    all_filters = list()
    for attribute in set(subject_info_df.columns) - set(["SUBJECT_ID", "ICUSTAY_ID"]):
        attribute_data = subject_info_df[attribute]
        if is_numerical(attribute_data.to_frame()):
            # Test on all range key words
            sample_filters = sample_numeric_filter(attribute, attribute_data)
            for demographic_filter in sample_filters:
                split_reader: SplitSetReader = datasets.train_test_split(
                    reader,
                    demographic_filter=demographic_filter,
                    storage_path=Path(TEMP_DIR, str(curr_iter)))
                if "train" in split_reader.split_names:
                    check_numerical_attribute(attribute=attribute,
                                              demographic_filter=demographic_filter,
                                              subject_info_df=subject_info_df,
                                              subject_ids=split_reader.train.subject_ids)
            all_filters.extend(sample_filters)
        else:
            # Categorical column
            demographic_filter = sample_categorical_filter(attribute, attribute_data)
            split_reader: SplitSetReader = datasets.train_test_split(
                reader,
                demographic_filter=demographic_filter,
                storage_path=Path(TEMP_DIR, str(curr_iter)))
            if "train" in split_reader.split_names:
                check_categorical_attribute(attribute=attribute,
                                            demographic_filter=demographic_filter,
                                            subject_info_df=subject_info_df,
                                            subject_ids=split_reader.train.subject_ids)
            all_filters.append(demographic_filter)
        curr_iter += 1

    for _ in range(10):
        demographic_filter = sample_hetero_filter(3, subject_info_df)
        split_reader: SplitSetReader = datasets.train_test_split(
            reader,
            demographic_filter=demographic_filter,
            storage_path=Path(TEMP_DIR, str(curr_iter)))
        if "train" in split_reader.split_names:
            check_hetero_attributes(demographic_filter=demographic_filter,
                                    subject_info_df=subject_info_df,
                                    subject_ids=split_reader.train.subject_ids)
        curr_iter += 1


def test_demo_split(
    task_name,
    preprocessing_style,
    discretized_readers: Dict[str, ProcessedSetReader],
    engineered_readers: Dict[str, ProcessedSetReader],
):
    tests_io("Test case by demographic filter", level=0)
    # Discretization or feature engineering
    if preprocessing_style == "discretized":
        reader = discretized_readers[task_name]
    else:
        reader = engineered_readers[task_name]

    subject_info_df = pd.read_csv(Path(reader.root_path, "subject_info.csv"))
    subject_info_df = subject_info_df[subject_info_df["SUBJECT_ID"].isin(reader.subject_ids)]
    curr_iter = 0

    for _ in range(10):
        demographic_filter = sample_hetero_filter(1, subject_info_df)

        split_reader: SplitSetReader = datasets.train_test_split(
            reader, demographic_split={"test": demographic_filter})
        if "test" in split_reader.split_names:
            check_hetero_attributes(demographic_filter=demographic_filter,
                                    subject_info_df=subject_info_df,
                                    subject_ids=split_reader.test.subject_ids)

            assert not set(split_reader.test.subject_ids) & set(split_reader.train.subject_ids)
        #TODO! Add current iter and test restoral
        tests_io(f"Succeeded testing the filter for test!")

    for _ in range(10):
        test_filter, val_filter = sample_hetero_filter(1, subject_info_df, val_set=True)
        #TODO! Add current iter and test restoral
        split_reader: SplitSetReader = datasets.train_test_split(reader,
                                                                 demographic_split={
                                                                     "test": test_filter,
                                                                     "val": val_filter
                                                                 })
        if "test" in split_reader.split_names:
            check_hetero_attributes(demographic_filter=test_filter,
                                    subject_info_df=subject_info_df,
                                    subject_ids=split_reader.test.subject_ids)
            if "train" in split_reader.split_names:
                assert not set(split_reader.test.subject_ids) & set(split_reader.train.subject_ids)

        if "val" in split_reader.split_names:
            check_hetero_attributes(demographic_filter=val_filter,
                                    subject_info_df=subject_info_df,
                                    subject_ids=split_reader.val.subject_ids)
            if "test" in split_reader.split_names:
                assert not set(split_reader.val.subject_ids) & set(split_reader.train.subject_ids)
        if "val" in split_reader.split_names and "test" in split_reader.split_names:
            assert not set(split_reader.test.subject_ids) & set(split_reader.val.subject_ids)

        curr_iter += 1
        tests_io(f"Succeeded testing the filter for val and test!")


def test_demo_and_ratio_split(
    task_name,
    preprocessing_style,
    discretized_readers: Dict[str, ProcessedSetReader],
    engineered_readers: Dict[str, ProcessedSetReader],
):
    tests_io("Test case demographic split with specified ratios", level=0)
    # Discretization or feature engineering
    if preprocessing_style == "discretized":
        reader = discretized_readers[task_name]
    else:
        reader = engineered_readers[task_name]

    subject_info_df = pd.read_csv(Path(reader.root_path, "subject_info.csv"))
    subject_info_df = subject_info_df[subject_info_df["SUBJECT_ID"].isin(reader.subject_ids)]
    attribute = random.choice(["WARDID", "AGE"])
    attribute_data = subject_info_df[attribute]
    filters = sample_numeric_filter(attribute, attribute_data)

    tolerance = (1e-2 if task_name in ["DECOMP", "LOS"] else 1e-1)
    test_size = 0.2
    none_reader = 0

    for demographic_filter in filters:
        tests_io(f"Specified demographic filter: 'test': {demographic_filter}")
        if not all([value for value in demographic_filter.values()]):
            continue
        try:
            split_reader: SplitSetReader = datasets.train_test_split(
                reader, 0.2, demographic_split={"test": demographic_filter})
        except ValueError:
            none_reader += 1
            continue

        if "test" in split_reader.split_names and "train" in split_reader.split_names:
            check_hetero_attributes(demographic_filter=demographic_filter,
                                    subject_info_df=subject_info_df,
                                    subject_ids=split_reader.test.subject_ids)

            assert_split_sanity(test_size, 0, tolerance, reader, split_reader, reduced_set=True)
        else:
            none_reader += 1
    if none_reader == len(filters):
        tests_io(f"All splits have invalid sets for {attribute}!")

    filters = sample_numeric_filter(attribute, attribute_data, val_set=True)

    split_size = 0.2
    none_reader = 0
    for test_filter, val_filter in filters:
        tests_io(f"Specified demographic filter: 'test': {test_filter}, 'val': {val_filter}")

        split_filters = {"test": test_filter, "val": val_filter}
        try:
            split_reader: SplitSetReader = datasets.train_test_split(
                reader, split_size, split_size, demographic_split=split_filters)
        except ValueError as e:
            none_reader += 1
            continue
        if len(split_reader.split_names) > 2:
            for set_name in split_reader.split_names:
                if set_name in ["test", "val"]:
                    check_hetero_attributes(demographic_filter=split_filters[set_name],
                                            subject_info_df=subject_info_df,
                                            subject_ids=getattr(split_reader, set_name).subject_ids)

            assert_split_sanity(split_size,
                                split_size,
                                tolerance,
                                reader,
                                split_reader,
                                reduced_set=True)

        else:
            none_reader += 1

    if none_reader == len(filters):
        tests_io(f"All splits have invalid sets for {attribute}!")


def test_train_size(task_name, preprocessing_style, discretized_readers: Dict[str,
                                                                              ProcessedSetReader],
                    engineered_readers: Dict[str, ProcessedSetReader]):

    tests_io("Test case train size", level=0)
    # Discretization or feature engineering
    if preprocessing_style == "discretized":
        reader = discretized_readers[task_name]
    else:
        reader = engineered_readers[task_name]

    # Train size with ratio
    tolerance = (1e-2 if task_name in ["DECOMP", "LOS"] else 1e-1)

    curr_iter = 0
    write_bool = True
    train_size = 8

    for val_size in [0.0, 0.2]:
        if val_size and write_bool:

            write_bool = False
        for test_size in [0.2, 0.4]:
            tests_io(
                f"Specified train_size: {train_size}; test_size: {test_size}; val_size: {val_size}")
            split_reader: SplitSetReader = \
                datasets.train_test_split(reader,
                                          test_size=test_size,
                                          val_size=val_size,
                                          storage_path=Path(
                                              TEMP_DIR, "split_test",
                                              str(curr_iter) + "base"))

            test_split_reader: SplitSetReader = \
                datasets.train_test_split(reader,
                                          test_size=test_size,
                                          val_size=val_size,
                                          train_size=train_size,
                                          storage_path=Path(TEMP_DIR, "split_test",
                                          str(curr_iter) + "_test"))
            if len(split_reader.train.subject_ids) < train_size:
                assert len(test_split_reader.train.subject_ids) == len(
                    split_reader.train.subject_ids)
            else:
                assert len(test_split_reader.train.subject_ids) == train_size
            assert_split_sanity(test_size,
                                val_size,
                                tolerance,
                                reader,
                                test_split_reader,
                                reduced_set=True)
            curr_iter += 1

    # Train size with demographic split
    tests_io("Specific train size with demographic split")
    subject_info_df = pd.read_csv(Path(reader.root_path, "subject_info.csv"))
    subject_info_df = subject_info_df[subject_info_df["SUBJECT_ID"].isin(reader.subject_ids)]
    attribute = random.choice(["WARDID", "AGE"])
    attribute_data = subject_info_df[attribute]
    filters = sample_numeric_filter(attribute, attribute_data)

    test_size = 0.2
    val_size = 0.2
    none_reader = 0

    for demographic_filter in filters:
        print(demographic_filter)
        try:
            split_reader: SplitSetReader = datasets.train_test_split(
                reader,
                test_size,
                val_size,
                train_size=train_size,
                demographic_split={"test": demographic_filter})
        except ValueError:
            none_reader += 1
            continue

        if not set(["test", "train", "val"]) - set(split_reader.split_names):
            check_hetero_attributes(demographic_filter=demographic_filter,
                                    subject_info_df=subject_info_df,
                                    subject_ids=split_reader.test.subject_ids)

            assert_split_sanity(test_size,
                                val_size,
                                tolerance,
                                reader,
                                split_reader,
                                reduced_set=True)
        else:
            none_reader += 1
    if none_reader == len(filters):
        tests_io(f"All splits have invalid sets for {attribute}!")


def sample_categorical_filter(attribute: str, attribute_sr: pd.Series, val_set: bool = False):
    # Randomly choose half of the possible categories
    categories = attribute_sr.unique()
    test_choices = random.sample(categories.tolist(), k=int(np.floor(len(categories) / 2)))
    # Demographic filter
    if val_set and len(test_choices):
        val_choices = random.sample(test_choices, k=int(len(test_choices) / 2))
        test_choices = list(set(test_choices) - set(val_choices))
        if test_choices and val_choices:
            return {attribute: {"choice": test_choices}}, {attribute: {"choice": val_choices}}
        else:
            return {}, {}

    return {attribute: {"choice": test_choices}}


def sample_numeric_filter(attribute: str, attribute_sr: pd.Series, val_set: bool = False):

    def get_range_key(greater_key: str,
                      less_key: str,
                      min_value: int = -1e10,
                      max_value: int = -1e10):
        curr_filter = dict()
        curr_range = max_value - min_value
        if greater_key is not None:
            lower_bound = random.uniform(min_value, min_value + curr_range / 2)
            curr_filter[greater_key] = lower_bound
        else:
            lower_bound = min_value
        if less_key is not None:
            upper_bound = random.uniform(lower_bound + curr_range / 10, max_value)
            curr_filter[less_key] = upper_bound
        else:
            upper_bound = max_value
        return curr_filter

    filters = list()
    inversion_mapping = {"less": "greater", "leq": "geq", "greater": "less", "geq": "leq"}
    for less_key in ["less", "leq", None]:
        for greater_key in ["greater", "geq", None]:
            # Get a valid range
            if less_key is None and greater_key is None:
                continue
            test_filter = get_range_key(greater_key, less_key, attribute_sr.min(),
                                        attribute_sr.max())
            if val_set:
                if test_filter.get(less_key) is None:
                    if test_filter.get(greater_key) is None:
                        # Both are None
                        continue
                    # Only less is not None
                    val_filter = get_range_key(greater_key,
                                               inversion_mapping[greater_key],
                                               min_value=attribute_sr.min(),
                                               max_value=test_filter[greater_key])
                else:
                    if test_filter.get(greater_key) is None:
                        # Only greater is not None
                        val_filter = get_range_key(inversion_mapping[less_key],
                                                   less_key,
                                                   min_value=test_filter[less_key],
                                                   max_value=attribute_sr.max())
                    else:
                        # Both are set
                        if random.choice([True, False]):
                            val_filter = get_range_key(greater_key,
                                                       less_key,
                                                       min_value=test_filter[less_key],
                                                       max_value=attribute_sr.max())
                        else:
                            val_filter = get_range_key(greater_key,
                                                       less_key,
                                                       min_value=attribute_sr.min(),
                                                       max_value=test_filter[greater_key])
                filters.append(({attribute: test_filter}, {attribute: val_filter}))
            else:
                filters.append({attribute: test_filter})
    return filters


def sample_hetero_filter(num: int, subject_info_df: pd.DataFrame, val_set: bool = False):
    # Test on random demographic
    all_filters = list()
    for attribute in set(subject_info_df.columns) - set(["SUBJECT_ID", "ICUSTAY_ID"]):
        if is_numerical(subject_info_df[attribute].to_frame()):
            # Test on all range key words
            possible_filter = sample_numeric_filter(attribute=attribute,
                                                    attribute_sr=subject_info_df[attribute],
                                                    val_set=val_set)
            all_filters.extend(possible_filter)
        else:
            # Categorical column
            possible_filter = sample_categorical_filter(attribute=attribute,
                                                        attribute_sr=subject_info_df[attribute],
                                                        val_set=val_set)
            all_filters.append(possible_filter)
    test_filter = dict()
    val_filter = dict()
    filter_selection = random.sample(all_filters, k=max(num * 5, len(all_filters)))
    attributes = list()
    for curr_filter in filter_selection:
        # No duplicate for same attribute
        if val_set and not set(curr_filter[0].keys()) & set(attributes):
            # No empty filters
            if val_set and list(curr_filter[0].values())[0] and list(curr_filter[1].values())[0]:
                test_filter.update(curr_filter[0])
                val_filter.update(curr_filter[1])
                attributes.extend(curr_filter[0].keys())
        elif not val_set and not set(curr_filter.keys()) & set(attributes):
            # No empty filters
            if not val_set and list(curr_filter.values())[0]:
                test_filter.update(curr_filter)
                attributes.extend(curr_filter.keys())
        if len(test_filter) == num:
            break
    if val_set:
        return test_filter, val_filter
    return test_filter


def assert_split_sanity(test_size: float,
                        val_size: float,
                        tolerance: float,
                        reader: ProcessedSetReader,
                        split_reader: SplitSetReader,
                        reduced_set: bool = False):
    tracker = PreprocessingTracker(Path(reader.root_path, "progress"))
    subject_counts = tracker.subjects
    subject_ids = tracker.subject_ids
    split_samples = dict()
    split_subjects = dict()

    # Extract result for each split
    for split_name in split_reader.split_names:
        # Check sample is not empty
        random_sample = getattr(split_reader, split_name).random_samples()
        assert len(random_sample["X"])
        # Get subject IDs and sample counts
        split_ids = getattr(split_reader, split_name).subject_ids
        split_subjects[split_name] = split_ids
        split_samples[split_name] = sum([
            stay_counts["total"]
            for stay_counts in dict_subset(subject_counts, split_ids).values()
            if isinstance(stay_counts, dict)
        ])
    # Assert ratios are respected
    # Assert no duplicte subjects
    assert not set().intersection(*split_subjects.values())
    if not reduced_set:
        assert sum([
            len(getattr(split_reader, split_name).subject_ids)
            for split_name in split_reader.split_names
        ]) == len(subject_ids)
    check_split_sizes(split_samples, test_size, val_size, tolerance)
    return split_subjects


def check_split_sizes(split_samples: dict, test_size: float, val_size: float, tolerance: float):
    total_samples = sum(split_samples.values())
    assert abs(split_samples["train"] / total_samples - (1 - test_size - val_size)) < tolerance

    if test_size:
        assert abs(split_samples["test"] / total_samples - test_size) < tolerance

    if val_size:
        assert abs(split_samples["val"] / total_samples - val_size) < tolerance


def check_hetero_attributes(demographic_filter: dict, subject_info_df: pd.DataFrame,
                            subject_ids: List[int]):

    for attribute in demographic_filter:
        attribute_data = subject_info_df[attribute]
        if is_numerical(attribute_data.to_frame()):
            check_numerical_attribute(attribute=attribute,
                                      demographic_filter=demographic_filter,
                                      subject_info_df=subject_info_df,
                                      subject_ids=subject_ids)
        else:
            check_categorical_attribute(attribute=attribute,
                                        demographic_filter=demographic_filter,
                                        subject_info_df=subject_info_df,
                                        subject_ids=subject_ids)


def check_categorical_attribute(attribute: str, demographic_filter: dict,
                                subject_info_df: pd.DataFrame, subject_ids: List[int]):
    selected_attributes = subject_info_df[subject_info_df["SUBJECT_ID"].isin(
        subject_ids)][attribute]
    # Ensure all entries are from choices. Subjects with stays where the attribute is from selected and not selected categories
    # are not included in the demographic.
    assert not set(selected_attributes) - set(demographic_filter[attribute]["choice"])


def check_numerical_attribute(attribute: str, demographic_filter: dict,
                              subject_info_df: pd.DataFrame, subject_ids: List[int]):
    # Make the split
    selected_attributes = subject_info_df[subject_info_df["SUBJECT_ID"].isin(
        subject_ids)][attribute]
    assert_range(selected_attributes, demographic_filter[attribute])


def assert_range(column, demographic_filter, invert=False):
    """
    Asserts that all or not any elements in the column meet the specified range conditions.
    
    Parameters:
        column (iterable): The input series or array-like structure to check.
        demographic_filter (dict): A dictionary containing range conditions like 'less', 'leq', 'greater', and 'geq'.
        invert (bool): If True, use 'not any' logic; otherwise, use 'all' logic.
    """

    def range_condition(val):
        # Define a function that checks the conditions specified in demographic_filter
        if "less" in demographic_filter:
            if "greater" in demographic_filter:
                return (val < demographic_filter["less"]) and (val > demographic_filter["greater"])
            elif "geq" in demographic_filter:
                return (val < demographic_filter["less"]) and (val >= demographic_filter["geq"])
            else:
                return val < demographic_filter["less"]
        elif "leq" in demographic_filter:
            if "greater" in demographic_filter:
                return (val <= demographic_filter["leq"]) and (val > demographic_filter["greater"])
            elif "geq" in demographic_filter:
                return (val <= demographic_filter["leq"]) and (val >= demographic_filter["geq"])
            else:
                return val <= demographic_filter["leq"]
        elif "greater" in demographic_filter:
            return val > demographic_filter["greater"]
        elif "geq" in demographic_filter:
            return val >= demographic_filter["geq"]
        return True

    # Apply the appropriate assertion logic
    if invert:
        assert not any(
            range_condition(x)
            for x in column), "Condition failed: Some elements meet the specified range condition"
    else:
        assert all(range_condition(x) for x in column
                  ), "Condition failed: Not all elements meet the specified range condition"


if __name__ == "__main__":
    reader = datasets.load_data(chunksize=75836,
                                source_path=TEST_DATA_DEMO,
                                storage_path=SEMITEMP_DIR,
                                discretize=True,
                                time_step_size=1.0,
                                start_at_zero=True,
                                impute_strategy='previous',
                                task='DECOMP')
    # test_demographic_filter("DECOMP", "discretized", {"DECOMP": reader}, {})
    # test_demo_split("DECOMP", "discretized", {"DECOMP": reader}, {})
    # test_demo_and_ratio_split("DECOMP", "discretized", {"DECOMP": reader}, {})

    # for _ in range(10):
    #     test_demo_and_ratio_split("DECOMP", "discretized", {"DECOMP": reader}, {})
    # test_train_size("DECOMP", "discretized", {"DECOMP": reader}, {})

    discretized_readers = dict()
    engineered_readers = dict()
    if TEMP_DIR.is_dir():
        shutil.rmtree(TEMP_DIR)
    for task_name in TASK_NAMES:
        reader = datasets.load_data(chunksize=75836,
                                    source_path=TEST_DATA_DEMO,
                                    storage_path=SEMITEMP_DIR,
                                    discretize=True,
                                    time_step_size=1.0,
                                    start_at_zero=True,
                                    impute_strategy='previous',
                                    task=task_name)
        discretized_readers[task_name] = reader
        reader = datasets.load_data(chunksize=75836,
                                    source_path=TEST_DATA_DEMO,
                                    storage_path=SEMITEMP_DIR,
                                    engineer=True,
                                    task=task_name)
        engineered_readers[task_name] = reader
        for processing_style in ["discretized", "engineered"]:
            test_demographic_filter(task_name, processing_style, discretized_readers,
                                    engineered_readers)
            test_demo_split(task_name, processing_style, discretized_readers, engineered_readers)
            test_demo_and_ratio_split(task_name, processing_style, discretized_readers,
                                      engineered_readers)
            test_train_size(task_name, processing_style, discretized_readers, engineered_readers)
            test_ratio_split(task_name, processing_style, discretized_readers, engineered_readers)
    pass


### FILE: .\tests\test_datasets\test_discretizer.py ###
import pytest
import datasets
import shutil
import pandas as pd
import numpy as np
from typing import Dict
from pathlib import Path
from datasets.readers import ProcessedSetReader
from datasets.writers import DataSetWriter
from tests.pytest_utils.discretization import assert_strategy_equals, prepare_processed_data
from utils.IO import *
from tests.pytest_utils import copy_dataset
from tests.settings import *
from tests.conftest import discretizer_listfiles


@pytest.mark.parametrize("start_strategy", ["zero", "relative"])
@pytest.mark.parametrize("impute_strategy", ["next", "normal_value", "previous", "zero"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_discretizer(task_name: str, start_strategy: str, impute_strategy: str,
                     discretizer_listfiles: Dict[str, pd.DataFrame],
                     preprocessed_readers: Dict[str, ProcessedSetReader]):

    copy_dataset("extracted")
    listfile = discretizer_listfiles[task_name]
    test_data_dir = Path(TEST_GT_DIR, "discretized", TASK_NAME_MAPPING[task_name])
    test_dir_path = Path(test_data_dir, f"imp{impute_strategy}_start{start_strategy}")

    tests_io(f"Testing discretizer for task {task_name}\n"
             f"Impute strategy '{impute_strategy}' \n"
             f"Start strategy '{start_strategy}'")
    if impute_strategy == "normal_value":
        impute_strategy = "normal"

    prepare_processed_data(task_name, listfile, preprocessed_readers[task_name])

    reader = datasets.load_data(chunksize=75837,
                                source_path=TEST_DATA_DEMO,
                                storage_path=TEMP_DIR,
                                discretize=True,
                                time_step_size=1.0,
                                start_at_zero=(start_strategy == "zero"),
                                impute_strategy=impute_strategy,
                                task=task_name)

    X_discretized, _ = reader.read_samples(read_ids=True).values()

    assert_strategy_equals(X_discretized, test_dir_path)

    tests_io("Succeeded in testing!")


if __name__ == "__main__":

    import re
    listfiles = dict()
    # Preparing the listfiles
    for task_name in TASK_NAMES:
        # Path to discretizer sets
        test_data_dir = Path(TEST_GT_DIR, "discretized", TASK_NAME_MAPPING[task_name])
        # Listfile with truth values
        listfile = pd.read_csv(Path(test_data_dir, "listfile.csv")).set_index("stay")
        stay_name_regex = r"(\d+)_episode(\d+)_timeseries\.csv"

        listfile = listfile.reset_index()
        listfile["subject"] = listfile["stay"].apply(
            lambda x: re.search(stay_name_regex, x).group(1))
        listfile["icustay"] = listfile["stay"].apply(
            lambda x: re.search(stay_name_regex, x).group(2))
        listfile = listfile.set_index("stay")
        listfiles[task_name] = listfile
    readers = dict()
    for task_name in TASK_NAMES:
        # Simulate semi_temp fixture
        reader = datasets.load_data(chunksize=75837,
                                    source_path=TEST_DATA_DEMO,
                                    storage_path=SEMITEMP_DIR,
                                    preprocess=True,
                                    task=task_name)
        readers[task_name] = reader
        for start_strategy in ["zero", "relative"]:
            for impute_strategy in ["next", "normal_value", "previous", "zero"]:
                if TEMP_DIR.is_dir():
                    shutil.rmtree(str(TEMP_DIR))
                test_discretizer(task_name=task_name,
                                 impute_strategy=impute_strategy,
                                 start_strategy=start_strategy,
                                 discretizer_listfiles=listfiles,
                                 preprocessed_readers=readers)


### FILE: .\tests\test_datasets\test_extracted_data.py ###
import datasets
import shutil
import pandas as pd
from tests.decorators import repeat
from datasets.readers import ExtractedSetReader
from pathlib import Path
from utils.IO import *
from tests.settings import *
from tests.pytest_utils.general import assert_dataframe_equals

top_level_files = ["diagnoses.csv", "icu_history.csv"]


@repeat(2)
def test_iterative_extraction():
    """ Tests the preprocessing handled by the load data method of the dataset module (extracted data).
    """
    tests_io("Test case iterative extraction.", level=0)
    # Extract the data
    test_data_dir = Path(TEST_GT_DIR, "extracted")
    reader: ExtractedSetReader = datasets.load_data(
        chunksize=900000,  # Using a large number to run on single read for comparision
        source_path=TEST_DATA_DEMO,
        storage_path=TEMP_DIR)

    compare_diagnoses_and_history(test_data_dir)
    tests_io("Datset creation successfully tested against original code!")
    compare_subject_directories(test_data_dir, reader.read_subjects(read_ids=True))
    tests_io("Dataset restoration successfully tested against original code!")


@repeat(2)
def test_compact_extraction():
    # Extract the data
    tests_io("Test case compact extraction.", level=0)
    test_data_dir = Path(TEST_GT_DIR, "extracted")
    dataset = datasets.load_data(source_path=TEST_DATA_DEMO, storage_path=TEMP_DIR)
    compare_diagnoses_and_history(test_data_dir)
    tests_io("Dataset creation successfully tested against original code!")
    compare_subject_directories(test_data_dir, dataset)
    tests_io("Dataset restoration successfully tested against original code!")


def compare_diagnoses_and_history(test_data_dir: Path):
    # Compare files to ground truth
    for file_name in top_level_files:
        file_settings = TEST_SETTINGS[file_name]
        test_file_name = file_settings["name_mapping"]
        tests_io(f"Test {file_name}")
        generated_df = pd.read_csv(Path(TEMP_DIR, "extracted", file_name))
        test_df = pd.read_csv(Path(test_data_dir, test_file_name))
        if "columns" in file_settings:
            generated_df = generated_df[file_settings["columns"]]
        assert_dataframe_equals(generated_df, test_df, normalize_by="groundtruth")


def compare_subject_directories(test_data_dir: Path, dataset: dict):
    generated_dirs = [
        directory for directory in Path(TEMP_DIR, "extracted").iterdir()
        if directory.is_dir() and directory.name.isnumeric()
    ]
    assert len(dataset)

    tests_io("subject_events.csv: 0 subject\n"
             "subject_diagnoses.csv: 0 subject\n"
             "timeseries.csv: 0 subject, 0 stays\n"
             "episodic_data.csv: 0 subject")

    counts = {
        "subject_events.csv": 0,
        "subject_diagnoses.csv": 0,
        "timeseries_stay_id.csv": 0,
        "episodic_data.csv": 0
    }

    ts_counts = 0

    # Compare the subject directories
    for directory in generated_dirs:
        stay_ids = list()

        # File equivalences:
        # Generated: Ground truth
        # subject_events.csv: events.csv
        # subject_diagnoses.csv: diagnoses.csv
        # timeseries_stay_id.csv': episodestay_id_timeseries.csv
        # episodic_data.csv: episodestay_id.csv

        for file_name, file_settings in TEST_SETTINGS.items():
            if file_name in top_level_files:  # already checked
                continue
            test_file_name = file_settings["name_mapping"]
            # Stay ID from early files
            if "stay_id" in file_name or "stay_id" in test_file_name:
                # Episodic data and timeseries are stored per stay_id in ground truth
                for stay_id in stay_ids:
                    # Insert the stay id
                    stay_file_name = file_name.replace("stay_id", str(int(stay_id)))
                    test_stay_file_name = test_file_name.replace("stay_id", str(int(stay_id)))
                    # Read the generated df
                    if stay_file_name == "episodic_data.csv":
                        continue
                        # Incorrect in original repository. Remove continue if issue is resolved
                        # https://github.com/YerevaNN/mimic3-benchmarks/issues/101
                        # Episodic data stored together in this implementation
                        generated_df = generated_df[generated_df["Icustay"] == stay_id].reset_index(
                            drop=True)

                    generated_df = dataset[int(directory.name)][str(Path(file_name).stem).replace(
                        "_stay_id", "")][stay_id].reset_index()

                    # Read the test df
                    test_df = pd.read_csv(Path(test_data_dir, directory.name, test_stay_file_name))
                    assert_dataframe_equals(generated_df, test_df, rename=file_settings["rename"])

                if file_name == "timeseries_stay_id.csv":
                    counts["timeseries_stay_id.csv"] += len(stay_ids)
                    ts_counts += 1
                elif file_name == "episodic_data.csv":
                    counts[
                        "episodic_data.csv"] += 0  # Incorrect in original repository. Remove 0 if issue is resolved
                tests_io(
                    f"subject_events.csv: {counts['subject_events.csv']} subjects\n"
                    f"subject_diagnoses.csv: {counts['subject_diagnoses.csv']} subjects\n"
                    f"timeseries.csv: {ts_counts} subjects, {counts['timeseries_stay_id.csv']} stays\n"
                    f"episodic_data.csv: {counts[file_name]} subjects",
                    flush_block=True)
            else:
                generated_df = dataset[int(directory.name)][str(Path(file_name).stem)]
                if file_name == "subject_events.csv":
                    stay_ids = generated_df["ICUSTAY_ID"].unique()

                test_df = pd.read_csv(Path(test_data_dir, directory.name, test_file_name))
                assert_dataframe_equals(generated_df, test_df, rename=file_settings["rename"])
                counts[file_name] += 1
                tests_io(
                    f"subject_events.csv: {counts['subject_events.csv']} subjects\n"
                    f"subject_diagnoses.csv: {counts['subject_diagnoses.csv']} subjects\n"
                    f"timeseries.csv: {ts_counts} subjects, {counts['timeseries_stay_id.csv']} stays\n"
                    f"episodic_data.csv: {counts[file_name]} subjects",
                    flush_block=True)


if __name__ == "__main__":

    if TEMP_DIR.is_dir():
        shutil.rmtree(TEMP_DIR)
    test_iterative_extraction()
    if TEMP_DIR.is_dir():
        shutil.rmtree(TEMP_DIR)
    test_compact_extraction()
    if TEMP_DIR.is_dir():
        shutil.rmtree(TEMP_DIR)


### FILE: .\tests\test_datasets\test_feature_engine.py ###
import datasets
import pytest
import pandas as pd
from pathlib import Path
from utils.IO import *
from tests.settings import *
from tests.pytest_utils import copy_dataset
from tests.pytest_utils.general import assert_dataframe_equals
from tests.decorators import repeat
from tests.pytest_utils.feature_engineering import extract_test_ids, concatenate_dataset

compare_mode_map = {
    "IHM": "single_entry",
    "LOS": "multiline",
    "PHENO": "single_entry",
    "DECOMP": "multiline"
}


@pytest.mark.parametrize("task_name", TASK_NAMES)
@repeat(2)
def test_iterative_engineer_task(task_name: str):
    """ Tests the feature engineering for in hospital mortality, done for linear linear models.
    Preprocessing tests need to be completed before running this test.
    """

    tests_io(f"Test case iterative engineering for task {task_name}", level=0)

    generated_path = Path(TEMP_DIR, "engineered", task_name)  # Outpath for task generation
    test_data_dir = Path(TEST_GT_DIR, "engineered",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    copy_dataset("extracted")
    copy_dataset(Path("processed", task_name))

    tests_io(f"Engineering data for task {task_name}.")
    reader = datasets.load_data(chunksize=75837,
                                source_path=TEST_DATA_DEMO,
                                storage_path=TEMP_DIR,
                                engineer=True,
                                task=generated_path.name)

    # Concatenate engineered samples into labeleod data frame
    generated_samples = reader.read_samples(read_ids=True)
    generated_df = concatenate_dataset(generated_samples["X"])

    # Load test data and extract ids
    test_df = pd.read_csv(Path(test_data_dir, "X.csv"))
    test_df = extract_test_ids(test_df)

    # Align unstructured frames
    generated_df = generated_df.sort_values(by=generated_df.columns.to_list())
    generated_df = generated_df.reset_index(drop=True)
    test_df = test_df.sort_values(by=generated_df.columns.to_list())
    test_df = test_df.reset_index(drop=True)
    assert_dataframe_equals(generated_df,
                            test_df,
                            rename={"hours": "Hours"},
                            normalize_by="groundtruth")

    tests_io("Testing against ground truth data.")
    tests_io(f"Total stays checked: {generated_df['SUBJECT_ID'].nunique()}\n"
             f"Total subjects checked: {generated_df['ICUSTAY_ID'].nunique()}\n"
             f"Total samples checked: {len(generated_df)}")

    tests_io(f"{task_name} feature engineering successfully tested against original code!")

    return


@pytest.mark.parametrize("task_name", TASK_NAMES)
@repeat(2)
def test_compact_engineer_task(task_name: str):
    """ Tests the feature engineering for in hospital mortality, done for linear linear models.
    Preprocessing tests need to be completed before running this test.
    """

    tests_io(f"Test case compact engineering for task {task_name}", level=0)

    task_name_mapping = dict(zip(TASK_NAMES, FTASK_NAMES))

    generated_path = Path(TEMP_DIR, "engineered", task_name)  # Outpath for task generation
    test_data_dir = Path(TEST_GT_DIR, "engineered",
                         task_name_mapping[task_name])  # Ground truth data dir

    copy_dataset("extracted")
    copy_dataset(Path("processed", task_name))

    # Preprocess the data
    tests_io(f"Engineering data for task {task_name}.")
    dataset = datasets.load_data(source_path=TEST_DATA_DEMO,
                                 storage_path=TEMP_DIR,
                                 engineer=True,
                                 task=generated_path.name)

    # Concatenate engineered samples into labeled data frame
    generated_df = concatenate_dataset(dataset["X"])

    # Load test data and extract ids
    test_df = pd.read_csv(Path(test_data_dir, "X.csv"))
    test_df = extract_test_ids(test_df)

    # Align unstructured frames
    generated_df = generated_df.sort_values(by=generated_df.columns.to_list())
    generated_df = generated_df.reset_index(drop=True)
    test_df = test_df.sort_values(by=generated_df.columns.to_list())
    test_df = test_df.reset_index(drop=True)
    assert_dataframe_equals(generated_df,
                            test_df,
                            rename={"hours": "Hours"},
                            normalize_by="groundtruth")
    tests_io("Testing against ground truth data.")
    tests_io(f"Total stays checked: {generated_df['SUBJECT_ID'].nunique()}\n"
             f"Total subjects checked: {generated_df['ICUSTAY_ID'].nunique()}\n"
             f"Total samples checked: {len(generated_df)}")

    tests_io(f"{task_name} feature engineering successfully tested against original code!")

    return


if __name__ == "__main__":
    import shutil
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    for task in TASK_NAMES:
        _ = datasets.load_data(
            chunksize=75835,
            task=task,
            preprocess=True,
            source_path=TEST_DATA_DEMO,
            storage_path=SEMITEMP_DIR)
        if TEMP_DIR.is_dir():
            shutil.rmtree(str(TEMP_DIR))
        # test_compact_engineer_task(task)
        if TEMP_DIR.is_dir():
            shutil.rmtree(str(TEMP_DIR))
        test_iterative_engineer_task(task)


### FILE: .\tests\test_datasets\test_preprocessor.py ###
import datasets
import pandas as pd
import re
import pytest
from pathlib import Path
from datasets.readers import ExtractedSetReader
from utils.IO import *
from tests.pytest_utils import copy_dataset
from tests.decorators import repeat
from tests.settings import *
from tests.pytest_utils.preprocessing import assert_reader_equals, assert_dataset_equals

kwargs = {
    "IHM": {
        "minimum_length_of_stay": IHM_SETTINGS['label_start_time']
    },
    "DECOMP": {
        "label_start_time": DECOMP_SETTINGS['label_start_time']
    },
    "LOS": {
        "label_start_time": LOS_SETTINGS['label_start_time']
    },
    "PHENO": {}
}


@pytest.mark.parametrize("task_name", TASK_NAMES)
@repeat(2)
def test_iterative_processing_task(task_name: str):
    """_summary_

    Args:
        task_name (str): _description_
    """
    tests_io(f"Test iterative preprocessor for task {task_name}", level=0)

    # Resolve task name
    generated_path = Path(TEMP_DIR, "processed", task_name)  # Outpath for task generation
    test_data_dir = Path(TEST_GT_DIR, "processed",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    copy_dataset("extracted")

    # Load
    reader = datasets.load_data(chunksize=75835,
                                source_path=TEST_DATA_DEMO,
                                storage_path=TEMP_DIR,
                                preprocess=True,
                                task=generated_path.name)

    assert_file_creation(reader.root_path, test_data_dir, **kwargs[generated_path.name])

    tests_io(f"All {task_name} files have been created as expected")
    # Compare the dataframes in the directory
    assert_reader_equals(reader, test_data_dir)
    tests_io(f"{task_name} preprocessing successfully tested against original code!")

    return


@pytest.mark.parametrize("task_name", TASK_NAMES)
@repeat(2)
def test_compact_processing_task(task_name: str):
    """_summary_

    Args:
        task_name (str): _description_
    """
    tests_io(f"Test compact preprocessor for task {task_name}", level=0)

    # Resolve task name
    generated_path = Path(TEMP_DIR, "processed", task_name)  # Outpath for task generation
    test_data_dir = Path(TEST_GT_DIR, "processed",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    copy_dataset("extracted")

    # Load/Create data
    dataset = datasets.load_data(source_path=TEST_DATA_DEMO,
                                 storage_path=TEMP_DIR,
                                 preprocess=True,
                                 task=generated_path.name)

    assert_file_creation(generated_path, test_data_dir, **kwargs[generated_path.name])

    tests_io(f"All {task_name} files have been created as expected")
    # Compare the dataframes in the directory
    assert_dataset_equals(dataset["X"], dataset["y"], generated_path, test_data_dir)
    tests_io(f"{task_name} preprocessing successfully tested against original code!")

    return


def assert_file_creation(root_path: Path,
                         test_data_dir: Path,
                         label_start_time: float = None,
                         minimum_length_of_stay: float = None):
    """_summary_

    Args:
        reader (SampleSetReader): _description_
        test_data_dir (Path): _description_
    """
    count = 0
    removed_count = 0
    subject_ids_checked = list()
    tests_io(f"Total stays checked: {count}\n"
             f"Total subjects checked: {len(set(subject_ids_checked))}\n"
             f"Subjects removed correctly: {removed_count}")

    assert root_path.is_dir(), f"Generated directory {root_path} does not exist"
    assert test_data_dir.is_dir(), f"Test directory {test_data_dir} does not exist"
    # Test wether all files have been created correctly
    for file_path in Path(test_data_dir).iterdir():
        if file_path.name == "listfile.csv":
            continue
        match = re.search(r"(\d+)_episode(\d+)_timeseries\.csv", file_path.name)
        subject_id = int(match.group(1))
        stay_id = int(match.group(2))
        # Files that are not longer than the minimum that needs to be elapsed before label creation are removed
        test_data = pd.read_csv(Path(test_data_dir,
                                     f"{subject_id}_episode{stay_id}_timeseries.csv"))
        # Increment for logging
        count += 1
        subject_ids_checked.append(subject_id)

        if label_start_time is not None:
            if label_start_time > test_data["Hours"].max():
                assert not Path(root_path, str(subject_id), f"X_{stay_id}.csv").is_file(
                ), f"Sample file X_{stay_id}.csv for subject {subject_id} should be deleted due to minimum label start time."
                assert not Path(root_path, str(subject_id), f"y_{stay_id}.csv").is_file(
                ), f"Label file y_{stay_id}.csv for subject {subject_id} should be deleted due to minimum label start time."
                removed_count += 1
                continue

        if minimum_length_of_stay is not None:
            test_episode_data = pd.read_csv(
                Path(test_data_dir.parent.parent, "extracted", str(subject_id),
                     f"episode{stay_id}.csv"))
            if test_episode_data["Length of Stay"][0] > minimum_length_of_stay:
                assert not Path(root_path, str(subject_id), f"X_{stay_id}.csv").is_file(
                ), f"Sample file X_{stay_id}.csv for subject {subject_id} should be deleted due to minimum length of stay."
                assert not Path(root_path, str(subject_id), f"y_{stay_id}.csv").is_file(
                ), f"Label file y_{stay_id}.csv for subject {subject_id} should be deleted due to minimum length of stay."
                removed_count += 1
                continue

        assert Path(root_path, str(subject_id), f"X_{stay_id}.csv").is_file(
        ), f"Missing sample file X_{stay_id}.csv for subject {subject_id}"
        assert Path(root_path, str(subject_id), f"y_{stay_id}.csv").is_file(
        ), f"Missing label file y_{stay_id}.csv for subject {subject_id}"
        tests_io(
            f"Total stays checked: {count}\n"
            f"Total subjects checked: {len(set(subject_ids_checked))}\n"
            f"Subjects removed correctly: {removed_count}",
            flush_block=True)


if __name__ == "__main__":
    import shutil
    _ = datasets.load_data(
            chunksize=75835,
            source_path=TEST_DATA_DEMO,
            storage_path=SEMITEMP_DIR)
    for task in TASK_NAMES:
        if Path(TEMP_DIR).is_dir():
            shutil.rmtree(str(Path(TEMP_DIR)))
        test_compact_processing_task(task)
        if Path(TEMP_DIR).is_dir():
            shutil.rmtree(str(TEMP_DIR))
        # test_iterative_processing_task(task)


### FILE: .\tests\test_datasets\__init__.py ###


### FILE: .\tests\test_datasets\test_options\test_chunksize_option.py ###
import datasets
import os
import pandas as pd
import json
import shutil
import pytest
from itertools import chain
from typing import Dict
from pathlib import Path
from utils.IO import *
from tests.settings import *
from tests.pytest_utils.general import assert_dataframe_equals
from tests.pytest_utils.preprocessing import assert_reader_equals
from tests.pytest_utils.feature_engineering import extract_test_ids, concatenate_dataset

from datasets.readers import ExtractedSetReader, ProcessedSetReader
'''
def test_path_options():
    datasets.load_data(chunksize=5000000,
                       source_path=TEST_DATA_DEMO,
                       storage_path=TEMP_DIR,
                       preprocess=True,
                       task=generated_path.name)


def test_chunksize_option():
    ...
'''

if __name__ == "__main__":
    # test_num_samples_options()
    # extraction_reader = datasets.load_data(chunksize=75835,
    #                                        source_path=TEST_DATA_DEMO,
    #                                        preprocess=True,
    #                                        task="IHM",
    #                                        storage_path=SEMITEMP_DIR)

    # stest_num_subjects_extraction_only(extraction_reader)
    # test_num_subjects_preprocessing_only("IHM")
    if Path(TEMP_DIR, "engineered").is_dir():
        shutil.rmtree(str(Path(TEMP_DIR, "engineered")))
    test_num_subjects_engineer_only("IHM")


### FILE: .\tests\test_datasets\test_options\test_num_subjects.py ###
import datasets
import pandas as pd
import shutil
import pytest
from tests.pytest_utils import copy_dataset
from typing import Dict
from pathlib import Path
from utils.IO import *
from tests.settings import *
from tests.pytest_utils.general import assert_dataframe_equals
from tests.pytest_utils.preprocessing import assert_reader_equals, assert_dataset_equals
from tests.pytest_utils.feature_engineering import extract_test_ids, concatenate_dataset
from tests.pytest_utils.extraction import compare_extracted_datasets
from datasets.readers import ExtractedSetReader, ProcessedSetReader


@pytest.mark.parametrize("extraction_style", ["iterative", "compact"])
def test_num_subjects_extraction(extracted_reader: ExtractedSetReader, extraction_style: str):
    # Test only extraction
    tests_io(f"Test case num subjects for extraction", level=0)

    # Test on existing directory
    for num_subjects in [1, 11, 21]:
        tests_io(f"-> Testing extract-only with {num_subjects} subjects on existing directory.")
        extract_and_compare(num_subjects=num_subjects,
                            extraction_style=extraction_style,
                            extracted_reader=extracted_reader)

    tests_io(f"-> Succeeded in testing on existing directory.")

    # Test on empty directory
    extracted_dir = Path(TEMP_DIR, "extracted")
    for num_subjects in [11, 21]:
        # Remove existing dir
        if extracted_dir.is_dir():
            shutil.rmtree(str(extracted_dir))
        # Compare extraction
        tests_io(f"-> Testing extract-only with {num_subjects} subjects on empty directory.")
        extract_and_compare(num_subjects=num_subjects,
                            extraction_style=extraction_style,
                            extracted_reader=extracted_reader)

    tests_io(f"-> Succeeded in testing on empty directory.")

    # Test reducing subject count
    extract_and_compare(num_subjects=1,
                        extraction_style=extraction_style,
                        extracted_reader=extracted_reader)
    tests_io(f"-> Succeeded in reducing number of subjects.")


@pytest.mark.parametrize("extraction_style", ["iterative", "compact"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_num_subjects_preprocessing_only(task_name: str, extraction_style: str):
    tests_io(f"Test case num subjects for preprocessing-only for task {task_name}.", level=0)
    test_data_dir = Path(TEST_GT_DIR, "processed",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    # Make sure extracted data is available
    tests_io(f"Preparing fully extracted directory")
    copy_dataset("extracted")

    # Test on existing directory
    for num_subjects in [1, 11, 21]:
        tests_io(f"-> Testing preprocessing-only with {num_subjects}"
                 " subjects on existing directory.")
        process_and_compare(num_subjects=num_subjects,
                            task_name=task_name,
                            extraction_style=extraction_style,
                            test_data_dir=test_data_dir)

    tests_io(f"-> Succeeded in testing on existing directory.")

    # Test on empty directory
    processed_dir = Path(TEMP_DIR, "processed")
    for num_subjects in [11, 21]:
        if processed_dir.is_dir():
            shutil.rmtree(str(processed_dir))
        tests_io(f"-> Testing preprocessing-only with {num_subjects}"
                 " subjects on empty directory.")
        process_and_compare(num_subjects=num_subjects,
                            task_name=task_name,
                            extraction_style=extraction_style,
                            test_data_dir=test_data_dir)

    tests_io(f"-> Succeeded in testing on empty directory.")

    # Test reducing subject count
    process_and_compare(num_subjects=1,
                        task_name=task_name,
                        extraction_style=extraction_style,
                        test_data_dir=test_data_dir)
    tests_io(f"-> Succeeded in reducing number of subjects.")


@pytest.mark.parametrize("extraction_style", ["iterative", "compact"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_num_subjects_engineer_only(task_name: str, extraction_style: str):
    tests_io(f"Test case num subjects for engineering-only for task {task_name}.", level=0)
    # Test only engineering
    tests_io(f"Preparing fully preprocessed directory")
    copy_dataset("extracted")
    copy_dataset(Path("processed", task_name))

    test_data_dir = Path(TEST_GT_DIR, "engineered",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    # Load test data and extract ids
    test_df = pd.read_csv(Path(test_data_dir, "X.csv"))
    test_df = extract_test_ids(test_df)

    # Align unstructured frames
    test_df = test_df.sort_values(by=test_df.columns.to_list())
    test_df = test_df.reset_index(drop=True)

    # Test on existing directory
    for num_subjects in [1, 11, 16]:
        tests_io(f"-> Testing engineer-only with {num_subjects} subjects on existing directory.")
        engineer_and_compare(num_subjects=num_subjects,
                             task_name=task_name,
                             extraction_style=extraction_style,
                             test_df=test_df)

    tests_io(f"-> Succeeded in testing on existing directory.")

    # Test on empty directory
    engineered_dir = Path(TEMP_DIR, "engineered")
    for num_subjects in [11, 16]:
        if TEMP_DIR.is_dir():
            shutil.rmtree(engineered_dir)
        tests_io(f"-> Testing engineer-only with {num_subjects} subjects on empty directory.")
        engineer_and_compare(num_subjects=num_subjects,
                             task_name=task_name,
                             extraction_style=extraction_style,
                             test_df=test_df)

    tests_io(f"-> Succeeded in testing on empty directory.")

    # Test reducing subject count
    engineer_and_compare(num_subjects=1,
                         task_name=task_name,
                         extraction_style=extraction_style,
                         test_df=test_df)
    tests_io(f"-> Succeeded in reducing number of subjects.")


@pytest.mark.parametrize("extraction_style", ["iterative", "compact"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_num_subjects_process(task_name: str, extraction_style: str):
    tests_io(f"Test case num subjects for preprocessing from scratch for task {task_name}.",
             level=0)
    test_data_dir = Path(TEST_GT_DIR, "processed",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    # Test on existing directory
    for num_subjects in [1, 6, 11, 16, 21]:
        tests_io(
            f"-> Testing preprocessing {num_subjects} subjects on existing directory from scratch.")
        process_and_compare(num_subjects=num_subjects,
                            task_name=task_name,
                            extraction_style=extraction_style,
                            test_data_dir=test_data_dir)

    tests_io(f"-> Succeeded in testing on existing directory.")

    # Test on empty directory
    for num_subjects in [6, 11, 16, 21]:
        if TEMP_DIR.is_dir():
            shutil.rmtree(str(Path(TEMP_DIR)))
        tests_io(
            f"-> Testing preprocessing {num_subjects} subjects on empty director from scratch.")
        process_and_compare(num_subjects=num_subjects,
                            task_name=task_name,
                            extraction_style=extraction_style,
                            test_data_dir=test_data_dir)

    tests_io(f"-> Succeeded in testing on empty directory.")
    # Test reducing subject count
    process_and_compare(num_subjects=1,
                        task_name=task_name,
                        extraction_style=extraction_style,
                        test_data_dir=test_data_dir)
    tests_io(f"-> Succeeded in reducing number of subjects.")


@pytest.mark.parametrize("extraction_style", ["iterative", "compact"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_num_subjects_engineer(task_name: str, extraction_style: str):
    tests_io(f"Test case num subjects for engineering from scratch for task {task_name}.", level=0)

    test_data_dir = Path(TEST_GT_DIR, "engineered",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    # Load test data and extract ids
    test_df = pd.read_csv(Path(test_data_dir, "X.csv"))
    test_df = extract_test_ids(test_df)

    # Align unstructured frames
    test_df = test_df.sort_values(by=test_df.columns.to_list())
    test_df = test_df.reset_index(drop=True)

    # Test on existing directory
    for num_subjects in [1, 11, 16]:
        tests_io(
            f"-> Testing engineering {num_subjects} subjects on existing directory from scratch.")
        engineer_and_compare(num_subjects=num_subjects,
                             task_name=task_name,
                             extraction_style=extraction_style,
                             test_df=test_df)

    tests_io(f"-> Succeeded in testing on existing directory.")

    # Test on empty directory
    for num_subjects in [11, 16]:
        if TEMP_DIR.is_dir():
            shutil.rmtree(TEMP_DIR)
        tests_io(
            f"-> Testing preprocessing {num_subjects} subjects on empty directory from scratch.")
        engineer_and_compare(num_subjects=num_subjects,
                             task_name=task_name,
                             extraction_style=extraction_style,
                             test_df=test_df)

    tests_io(f"-> Succeeded in testing on empty directory.")

    # Test reducing subject count
    engineer_and_compare(num_subjects=1,
                         task_name=task_name,
                         extraction_style=extraction_style,
                         test_df=test_df)
    tests_io(f"-> Succeeded in reducing number of subjects.")


def extract_and_compare(num_subjects: int, extraction_style: str,
                        extracted_reader: ExtractedSetReader):
    return_entity = datasets.load_data(
        chunksize=(75835 if extraction_style == "iterative" else None),
        source_path=TEST_DATA_DEMO,
        num_subjects=num_subjects,
        storage_path=TEMP_DIR,
        extract=True)
    if extraction_style == "iterative":
        reader: ExtractedSetReader = return_entity
        assert len(reader.subject_ids) == num_subjects
        subject_ids = reader.subject_ids
        generated_dataset = reader.read_subjects(reader.subject_ids, read_ids=True)
    else:
        generated_dataset: dict = return_entity
        assert len(generated_dataset) == num_subjects
        subject_ids = list(generated_dataset.keys())
    test_dataset = extracted_reader.read_subjects(subject_ids, read_ids=True)

    compare_extracted_datasets(generated_dataset, test_dataset)


def process_and_compare(num_subjects: int, task_name: str, extraction_style: str,
                        test_data_dir: Path):
    return_entity: ProcessedSetReader = datasets.load_data(
        chunksize=(75835 if extraction_style == "iterative" else None),
        source_path=TEST_DATA_DEMO,
        storage_path=TEMP_DIR,
        num_subjects=num_subjects,
        preprocess=True,
        task=task_name)

    if extraction_style == "iterative":
        reader = return_entity
        assert len(reader.subject_ids) == num_subjects
        assert_reader_equals(reader, test_data_dir)
    else:
        generated_path = Path(TEMP_DIR, "processed", task_name)  # Outpath for task generation
        test_data_dir = Path(TEST_GT_DIR, "processed",
                             TASK_NAME_MAPPING[task_name])  # Ground truth data dir
        generated_samples = return_entity
        assert len(generated_samples["X"]) == num_subjects
        X = generated_samples["X"]
        y = generated_samples["y"]
        assert_dataset_equals(X, y, generated_path, test_data_dir)


def engineer_and_compare(num_subjects: int, task_name: str, extraction_style: str, test_df: Path):
    return_entity: ProcessedSetReader = datasets.load_data(
        chunksize=(75835 if extraction_style == "iterative" else None),
        source_path=TEST_DATA_DEMO,
        storage_path=TEMP_DIR,
        num_subjects=num_subjects,
        engineer=True,
        task=task_name)
    if extraction_style == "iterative":
        reader = return_entity
        generated_samples = reader.read_samples(read_ids=True)
    else:
        generated_samples = return_entity
    assert len(generated_samples["X"])  # Generated df is None if this doesn't pass
    generated_df = concatenate_dataset(generated_samples["X"])
    # Align unstructured frames
    generated_df = generated_df.sort_values(by=generated_df.columns.to_list())
    generated_df = generated_df.reset_index(drop=True)
    stay_ids = generated_df["ICUSTAY_ID"].unique()

    curr_test_df = test_df[test_df["ICUSTAY_ID"].isin(stay_ids.astype("str").tolist())]
    curr_test_df = curr_test_df.reset_index(drop=True)

    assert generated_df["SUBJECT_ID"].nunique() == num_subjects
    assert_dataframe_equals(generated_df,
                            curr_test_df,
                            rename={"hours": "Hours"},
                            normalize_by="groundtruth")
    tests_io("Engineered subjects are identical!")


if __name__ == "__main__":
    extraction_reader = datasets.load_data(chunksize=75835,
                                           source_path=TEST_DATA_DEMO,
                                           storage_path=SEMITEMP_DIR)
    for extraction_style in ["compact", "iterative"]:
        if TEMP_DIR.is_dir():
            shutil.rmtree(str(TEMP_DIR))
        test_num_subjects_extraction(extraction_reader, extraction_style)
        for task in TASK_NAMES:
            if not Path(SEMITEMP_DIR, "processed", task).is_dir():
                reader = datasets.load_data(chunksize=75835,
                                            source_path=TEST_DATA_DEMO,
                                            storage_path=SEMITEMP_DIR,
                                            preprocess=True,
                                            task=task)
            if TEMP_DIR.is_dir():
                shutil.rmtree(str(TEMP_DIR))
            test_num_subjects_preprocessing_only(task, extraction_style)
            if TEMP_DIR.is_dir():
                shutil.rmtree(str(TEMP_DIR))
            test_num_subjects_engineer_only(task, extraction_style)
            if TEMP_DIR.is_dir():
                shutil.rmtree(str(TEMP_DIR))
            test_num_subjects_process(task, extraction_style)
            if TEMP_DIR.is_dir():
                shutil.rmtree(str(TEMP_DIR))
            test_num_subjects_engineer(task, extraction_style)

    tests_io("All tests completed successfully!")


### FILE: .\tests\test_datasets\test_options\test_paths.py ###



### FILE: .\tests\test_datasets\test_options\test_subject_ids.py ###
import datasets
import random
import shutil
import pytest
import re
import pandas as pd
from copy import deepcopy
from pathlib import Path
from utils.IO import *
from tests.settings import *
from tests.pytest_utils import copy_dataset
from tests.pytest_utils.general import assert_dataframe_equals
from tests.pytest_utils.preprocessing import assert_reader_equals, assert_dataset_equals
from tests.pytest_utils.feature_engineering import extract_test_ids, concatenate_dataset
from tests.pytest_utils.extraction import compare_extracted_datasets
from datasets.readers import ExtractedSetReader, ProcessedSetReader


@pytest.mark.parametrize("extraction_style", ["iterative", "compact"])
def test_subject_ids_extraction(extracted_reader: ExtractedSetReader, subject_ids: list,
                                extraction_style: str):
    # Comapring subject event dfs for correctniss is costly due to unorderedness
    # These are subjects with a very low subject event count
    tests_io(f"Test case subject ids for extraction", level=0)

    # Retrive subject ids
    subjects = deepcopy(subject_ids)
    curr_subjects = list()
    # Sample subjects and test result by extending the subject id list
    for num_subjects in [1, 10, 10]:
        # Extend the subject id list
        curr_subjects.extend(random.sample(subjects, num_subjects - len(curr_subjects)))
        subjects = list(set(subjects) - set(curr_subjects))

        # Compare the extracted data with the test data
        tests_io(f"-> Testing extract-only with {len(curr_subjects)} subjects on empty directory.\n"
                 f"Subject IDs: {*curr_subjects,}")

        extract_and_compare(subject_ids=curr_subjects,
                            test_subject_ids=extracted_reader.subject_ids,
                            extraction_style=extraction_style,
                            extracted_reader=extracted_reader)

    tests_io(f"-> Succeeded in testing extending subject id.")

    # Sample subject from scratch and extract into empty dir
    extracted_dir = Path(TEMP_DIR, "extracted")
    for num_subjects in [11, 21]:
        # Remove the extracted directory
        if extracted_dir.is_dir():
            shutil.rmtree(str(extracted_dir))
        # Sample subject ids
        curr_subjects = random.sample(subjects, num_subjects)
        subjects = list(set(subjects) - set(curr_subjects))
        # Compare the extracted data with the test data
        tests_io(f"-> Testing extract-only with {len(curr_subjects)} subjects on empty directory.\n"
                 f"Subject IDs: {*curr_subjects,}")
        extract_and_compare(subject_ids=curr_subjects,
                            test_subject_ids=extracted_reader.subject_ids,
                            extraction_style=extraction_style,
                            extracted_reader=extracted_reader)

    tests_io(f"-> Succeeded in testing on empty directory.")

    # Test reducing subject count to a single subject
    curr_subjects = random.sample(subjects, 1)
    extract_and_compare(subject_ids=curr_subjects,
                        test_subject_ids=extracted_reader.subject_ids,
                        extraction_style=extraction_style,
                        extracted_reader=extracted_reader)

    tests_io(f"-> Succeeded in reducing number of subjects.")


@pytest.mark.parametrize("extraction_style", ["iterative", "compact"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_subject_ids_preprocessing_only(task_name: str, subject_ids: list, extraction_style: str):
    tests_io(f"Test case subject ids for preprocessing-only for task {task_name}.", level=0)
    test_data_dir = Path(TEST_GT_DIR, "processed",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    # Make sure extracted data is available
    tests_io(f"Preparing fully extracted directory")
    copy_dataset("extracted")

    # Retrive subject ids
    curr_subjects = list()
    subjects = deepcopy(subject_ids)

    # Sample subjects and test result by extending the subject id list
    for num_subjects in [1, 10, 10]:
        # Extend the subject id list
        curr_subjects.extend(random.sample(subjects, num_subjects - len(curr_subjects)))
        subjects = list(set(subjects) - set(curr_subjects))
        # Compare generated results
        tests_io(f"-> Testing preprocessing-only with {len(curr_subjects)}"
                 " subjects on existing directory.")
        process_and_compare(subject_ids=curr_subjects,
                            task_name=task_name,
                            extraction_style=extraction_style,
                            test_data_dir=test_data_dir)

    tests_io(f"-> Succeeded in testing on existing directory.")

    # Sample subject from scratch and extract into empty dir
    processed_dir = Path(TEMP_DIR, "processed")

    for num_subjects in [11, 21]:
        # Remove the processed directory
        if processed_dir.is_dir():
            shutil.rmtree(str(processed_dir))
        # Sample subject ids
        curr_subjects = random.sample(subjects, num_subjects)
        subjects = list(set(subjects) - set(curr_subjects))
        # Compare the processed data
        tests_io(f"-> Testing preprocessing-only with {len(curr_subjects)}"
                 " subjects on empty directory.")
        process_and_compare(subject_ids=curr_subjects,
                            task_name=task_name,
                            extraction_style=extraction_style,
                            test_data_dir=test_data_dir)

    tests_io(f"-> Succeeded in testing on empty directory.")
    # Test reducing subject count
    curr_subjects = random.sample(subjects, 1)
    process_and_compare(subject_ids=curr_subjects,
                        task_name=task_name,
                        extraction_style=extraction_style,
                        test_data_dir=test_data_dir)
    tests_io(f"-> Succeeded in reducing number of subjects.")


@pytest.mark.parametrize("extraction_style", ["iterative", "compact"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_subject_ids_engineer_only(task_name: str, subject_ids: list, extraction_style: str):
    tests_io(f"Test case subject ids for engineering-only for task {task_name}.", level=0)

    # Test only engineering
    tests_io(f"Preparing fully preprocessed directory")
    copy_dataset("extracted")
    copy_dataset(Path("processed", task_name))

    test_data_dir = Path(TEST_GT_DIR, "engineered",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    # Load test data and extract ids
    test_df = pd.read_csv(Path(test_data_dir, "X.csv"))
    test_df = extract_test_ids(test_df)

    copy_dataset("extracted")
    copy_dataset(Path("processed", task_name))
    # Retrive subject ids
    curr_subjects = list()
    subjects = deepcopy(subject_ids)

    # Align unstructured frames
    test_df = test_df.sort_values(by=test_df.columns.to_list())
    test_df = test_df.reset_index(drop=True)

    # Test on existing directory
    for num_subjects in [1, 10, 10]:
        # Extend the subject id list
        curr_subjects.extend(random.sample(subjects, num_subjects - len(curr_subjects)))
        subjects = list(set(subjects) - set(curr_subjects))
        # Compare generated results
        tests_io(
            f"-> Testing engineer-only with {len(curr_subjects)} subjects on existing directory.")
        engineer_and_compare(subject_ids=curr_subjects,
                             task_name=task_name,
                             extraction_style=extraction_style,
                             test_df=test_df)

    tests_io(f"-> Succeeded in testing on existing directory.")

    # Test on empty directory
    engineered_dir = Path(TEMP_DIR, "engineered")
    for num_subjects in [11, 16]:
        if TEMP_DIR.is_dir():
            shutil.rmtree(engineered_dir)
        # Sample subject ids
        curr_subjects = random.sample(subjects, num_subjects)
        subjects = list(set(subjects) - set(curr_subjects))
        # Compare the processed data
        tests_io(f"-> Testing engineer-only with {len(curr_subjects)} subjects on empty directory.")
        engineer_and_compare(subject_ids=curr_subjects,
                             task_name=task_name,
                             extraction_style=extraction_style,
                             test_df=test_df)

    tests_io(f"-> Succeeded in testing on empty directory.")

    # Test reducing subject count
    curr_subjects = random.sample(subjects, 1)
    engineer_and_compare(subject_ids=curr_subjects,
                         task_name=task_name,
                         extraction_style=extraction_style,
                         test_df=test_df)
    tests_io(f"-> Succeeded in reducing number of subjects.")


@pytest.mark.parametrize("extraction_style", ["iterative", "compact"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_subject_ids_preprocessing(task_name: str, subject_ids: list, extraction_style: str):
    # Test preprocessing from scratch, that means no fully extracted data in temp beforehand
    tests_io(f"Test case subject ids for preprocessing from scratch for task {task_name}.", level=0)
    test_data_dir = Path(TEST_GT_DIR, "processed",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    # Make sure extracted data is available
    tests_io(f"Preparing fully extracted directory")

    # Retrive subject ids
    curr_subjects = list()
    subjects = deepcopy(subject_ids)

    # Sample subjects and test result by extending the subject id list
    for num_subjects in [1, 10, 10]:
        # Extend the subject id list
        curr_subjects.extend(random.sample(subjects, num_subjects - len(curr_subjects)))
        subjects = list(set(subjects) - set(curr_subjects))
        # Compare generated results
        tests_io(f"-> Testing preprocessing-only with {len(curr_subjects)}"
                 " subjects on existing directory.")
        process_and_compare(subject_ids=curr_subjects,
                            task_name=task_name,
                            extraction_style=extraction_style,
                            test_data_dir=test_data_dir)

    tests_io(f"-> Succeeded in testing on existing directory.")

    # Sample subject from scratch and extract into empty dir
    processed_dir = Path(TEMP_DIR, "processed")

    for num_subjects in [11, 21]:
        # Remove the processed directory
        if processed_dir.is_dir():
            shutil.rmtree(str(processed_dir))
        # Sample subject ids
        curr_subjects = random.sample(subjects, num_subjects)
        subjects = list(set(subjects) - set(curr_subjects))
        # Compare the processed data
        tests_io(f"-> Testing preprocessing-only with {len(curr_subjects)}"
                 " subjects on empty directory.")
        process_and_compare(subject_ids=curr_subjects,
                            task_name=task_name,
                            extraction_style=extraction_style,
                            test_data_dir=test_data_dir)

    tests_io(f"-> Succeeded in testing on empty directory.")
    # Test reducing subject count
    curr_subjects = random.sample(subjects, 1)
    process_and_compare(subject_ids=curr_subjects,
                        task_name=task_name,
                        extraction_style=extraction_style,
                        test_data_dir=test_data_dir)
    tests_io(f"-> Succeeded in reducing number of subjects.")


@pytest.mark.parametrize("extraction_style", ["iterative", "compact"])
@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_subject_ids_engineer(task_name: str, subject_ids: list, extraction_style: str):
    tests_io(f"Test case subject ids for engineering from scratch for task {task_name}.", level=0)
    # Test engineering from scratch, that means no fully processed data in temp
    tests_io(f"Preparing fully preprocessed directory")
    test_data_dir = Path(TEST_GT_DIR, "engineered",
                         TASK_NAME_MAPPING[task_name])  # Ground truth data dir

    # Load test data and extract ids
    test_df = pd.read_csv(Path(test_data_dir, "X.csv"))
    test_df = extract_test_ids(test_df)

    # Align unstructured frames
    test_df = test_df.sort_values(by=test_df.columns.to_list())
    test_df = test_df.reset_index(drop=True)

    # Retrive subject ids
    curr_subjects = list()
    subjects = deepcopy(subject_ids)

    # Test on existing directory
    for num_subjects in [1, 10, 10]:
        # Extend the subject id list
        curr_subjects.extend(random.sample(subjects, num_subjects - len(curr_subjects)))
        subjects = list(set(subjects) - set(curr_subjects))
        # Compare generated results
        tests_io(
            f"-> Testing engineer-only with {len(curr_subjects)} subjects on existing directory.")
        engineer_and_compare(subject_ids=curr_subjects,
                             task_name=task_name,
                             extraction_style=extraction_style,
                             test_df=test_df)

    tests_io(f"-> Succeeded in testing on existing directory.")

    # Test on empty directory
    engineered_dir = Path(TEMP_DIR, "engineered")
    for num_subjects in [11, 16]:
        if TEMP_DIR.is_dir():
            shutil.rmtree(engineered_dir)
        # Sample subject ids
        curr_subjects = random.sample(subjects, num_subjects)
        subjects = list(set(subjects) - set(curr_subjects))
        # Compare the processed data
        tests_io(f"-> Testing engineer-only with {len(curr_subjects)} subjects on empty directory.")
        engineer_and_compare(subject_ids=curr_subjects,
                             task_name=task_name,
                             extraction_style=extraction_style,
                             test_df=test_df)

    tests_io(f"-> Succeeded in testing on empty directory.")
    # Test reducing subject count
    curr_subjects = random.sample(subjects, 1)
    engineer_and_compare(subject_ids=curr_subjects,
                         task_name=task_name,
                         extraction_style=extraction_style,
                         test_df=test_df)
    tests_io(f"-> Succeeded in reducing number of subjects.")


def extract_and_compare(subject_ids: list, test_subject_ids: list, extraction_style: str,
                        extracted_reader: ExtractedSetReader):
    return_entity: ExtractedSetReader = datasets.load_data(
        chunksize=(75835 if extraction_style == "iterative" else None),
        source_path=TEST_DATA_DEMO,
        subject_ids=subject_ids,
        storage_path=TEMP_DIR,
        extract=True)
    if extraction_style == "iterative":
        reader = return_entity
        # Some ids will not be processed due to no in unit subject events
        assert len(reader.subject_ids)
        missing_ids = list(set(subject_ids) - set(reader.subject_ids))
        # Make sure they are also not processed in the gt directory
        assert not set(missing_ids) & set(test_subject_ids)
        additional_ids = list(set(reader.subject_ids) - set(subject_ids))
        assert not additional_ids
        generated_dataset = reader.read_subjects(reader.subject_ids, read_ids=True)
    else:
        generated_dataset = return_entity
        assert len(generated_dataset)
        # Some ids will not be processed due to no in unit subject events
        missing_ids = list(set(subject_ids) - set(generated_dataset.keys()))
        # Make sure they are also not processed in the gt directory
        assert not set(missing_ids) & set(test_subject_ids)
        additional_ids = list(set(generated_dataset.keys()) - set(subject_ids))
        assert not additional_ids
    tests_io(f"-> The following IDs have not been extracted: {*missing_ids,}")
    test_dataset = extracted_reader.read_subjects(subject_ids, read_ids=True)
    compare_extracted_datasets(generated_dataset, test_dataset)


def process_and_compare(subject_ids: list, task_name: str, extraction_style: str,
                        test_data_dir: Path):
    # Process the dataset
    return_entity: ProcessedSetReader = datasets.load_data(
        chunksize=(75835 if extraction_style == "iterative" else None),
        source_path=TEST_DATA_DEMO,
        storage_path=TEMP_DIR,
        subject_ids=subject_ids,
        preprocess=True,
        task=task_name)

    listfile_df = pd.read_csv(Path(test_data_dir, "listfile.csv"))
    regex = r"(\d+)_episode(\d+)_timeseries\.csv"
    test_subject_ids = listfile_df.apply(lambda x: re.search(regex, x["stay"]).group(1), axis=1)
    test_subject_ids = test_subject_ids.astype(int)
    test_subject_ids = test_subject_ids.unique()

    if extraction_style == "iterative":
        reader = return_entity
        # Some ids will not be processed due to no in unit subject events
        missing_ids = list(set(subject_ids) - set(reader.subject_ids))
        # Make sure they are also not processed in the gt directory
        assert not set(missing_ids) & set(test_subject_ids)
        additional_ids = list(set(reader.subject_ids) - set(subject_ids))
        assert not additional_ids
        if reader.subject_ids:
            assert_reader_equals(reader, test_data_dir)

    else:
        generated_path = Path(TEMP_DIR, "processed", task_name)  # Outpath for task generation
        test_data_dir = Path(TEST_GT_DIR, "processed",
                             TASK_NAME_MAPPING[task_name])  # Ground truth data dir
        generated_samples = return_entity
        # Some ids will not be processed due to no in unit subject events
        missing_ids = list(set(subject_ids) - set(generated_samples["X"].keys()))
        # Make sure they are also not processed in the gt directory
        assert not set(missing_ids) & set(test_subject_ids)
        additional_ids = list(set(generated_samples["X"].keys()) - set(subject_ids))
        assert not additional_ids
        X = generated_samples["X"]
        y = generated_samples["y"]
        if len(X):
            assert_dataset_equals(X, y, generated_path, test_data_dir)


def engineer_and_compare(subject_ids: list, task_name: str, extraction_style: str, test_df: Path):
    return_entity: ProcessedSetReader = datasets.load_data(
        chunksize=(75835 if extraction_style == "iterative" else None),
        source_path=TEST_DATA_DEMO,
        storage_path=TEMP_DIR,
        subject_ids=subject_ids,
        engineer=True,
        task=task_name)
    if extraction_style == "iterative":
        reader = return_entity
        generated_samples = reader.read_samples(read_ids=True)
        # Some ids will not be processed due to no in unit subject events
        missing_ids = list(set(subject_ids) - set(generated_samples["X"].keys()))
        # Make sure they are also not processed in the gt directory
        assert not set(missing_ids) & set(test_df["SUBJECT_ID"].unique())
        additional_ids = list(set(generated_samples["X"].keys()) - set(subject_ids))
        assert not additional_ids
    else:
        generated_samples = return_entity
        # Some ids will not be processed due to no in unit subject events
        missing_ids = list(set(subject_ids) - set(generated_samples["X"].keys()))
        # Make sure they are also not processed in the gt directory
        assert not set(missing_ids) & set(test_df["SUBJECT_ID"].unique())
        additional_ids = list(set(generated_samples["X"].keys()) - set(subject_ids))
        assert not additional_ids
    generated_df = concatenate_dataset(generated_samples["X"])

    # Align unstructured frames
    if generated_df is None:
        tests_io(f"-> No data generated for {task_name} task.")
        return
    columns = generated_df.columns.to_list()

    generated_samples = return_entity

    generated_df = generated_df.sort_values(by=columns)
    generated_df = generated_df.reset_index(drop=True)
    stay_ids = generated_df["ICUSTAY_ID"].unique()

    curr_test_df = test_df[test_df["ICUSTAY_ID"].isin(stay_ids.astype("str").tolist())]
    curr_test_df = curr_test_df.sort_values(by=columns)
    curr_test_df = curr_test_df.reset_index(drop=True)

    assert_dataframe_equals(generated_df,
                            curr_test_df,
                            rename={"hours": "Hours"},
                            normalize_by="groundtruth")


if __name__ == "__main__":
    extraction_reader = datasets.load_data(chunksize=75835,
                                           source_path=TEST_DATA_DEMO,
                                           storage_path=SEMITEMP_DIR)
    icu_history = extraction_reader._read_csv("icu_history.csv")
    subjects = icu_history["SUBJECT_ID"].astype(int).unique().tolist()
    for extraction_style in ["iterative"]:  #"compact", "iterative"]:
        if TEMP_DIR.is_dir():
            shutil.rmtree(str(TEMP_DIR))
        test_subject_ids_extraction(extraction_reader, subjects, extraction_style)
        for task in TASK_NAMES:
            if not Path(SEMITEMP_DIR, "processed", task).is_dir():
                reader = datasets.load_data(chunksize=75835,
                                            source_path=TEST_DATA_DEMO,
                                            storage_path=SEMITEMP_DIR,
                                            preprocess=True,
                                            task=task)

            if TEMP_DIR.is_dir():
                shutil.rmtree(str(TEMP_DIR))
            test_subject_ids_preprocessing_only(task, subjects, extraction_style)
            if TEMP_DIR.is_dir():
                shutil.rmtree(str(TEMP_DIR))
            test_subject_ids_engineer_only(task, subjects, extraction_style)
            if TEMP_DIR.is_dir():
                shutil.rmtree(str(TEMP_DIR))
            test_subject_ids_preprocessing(task, subjects, extraction_style)
            if TEMP_DIR.is_dir():
                shutil.rmtree(str(TEMP_DIR))
            test_subject_ids_engineer(task, subjects, extraction_style)


### FILE: .\tests\test_datasets\test_readers\test_event_reader.py ###
import shutil
import numpy as np
import pandas as pd
import datasets
from time import sleep
from pathlib import Path
from datasets.trackers import ExtractionTracker
from datasets.readers import EventReader
from tests.settings import *
from tests.pytest_utils.general import assert_dataframe_equals
from utils.IO import *

DTYPES = DATASET_SETTINGS["subject_events"]["dtype"]

columns = ["SUBJECT_ID", "HADM_ID", "ICUSTAY_ID", "CHARTTIME", "ITEMID", "VALUE", "VALUEUOM"]


def assert_dtypes(dataframe: pd.DataFrame):
    assert all([
        dataframe.dtypes[column] == "object"
        if dtype == "str" else dataframe.dtypes[column] == dtype  # Might be translated to obj
        for column, dtype in DTYPES.items()
        if column in dataframe
    ])
    return True


def test_get_full_chunk():
    tests_io("Test case getting full chunk", level=0)
    tracker = ExtractionTracker(TEMP_DIR, num_samples=None)
    event_reader = EventReader(chunksize=900000, dataset_folder=TEST_DATA_DEMO, tracker=tracker)

    samples, frame_lengths = event_reader.get_chunk()
    assert len(samples) == 3
    for csv_name, frame in samples.items():
        assert len(frame) == len(frame)
        assert len(frame) == frame_lengths[csv_name]
        assert not (set(frame.columns) - set(columns))
        assert not (set(columns) - set(frame.columns))
        assert_dtypes(frame)
    tests_io("Test case getting full chunk succeeded")


def test_get_mutlitple_chunks():
    tests_io("Test case getting multiple chunks", level=0)

    tracker = ExtractionTracker(TEMP_DIR, num_samples=None)
    event_reader = EventReader(chunksize=5000, dataset_folder=TEST_DATA_DEMO, tracker=tracker)

    samples, frame_lengths = event_reader.get_chunk()

    assert len(samples) == 3
    for csv_name, frame in samples.items():
        assert len(frame) == 5000
        assert frame_lengths[csv_name] == 5000
        assert frame.index[-1] == 4999
        assert not (set(frame.columns) - set(columns))
        assert not (set(columns) - set(frame.columns))
        assert_dtypes(frame)

    samples, frame_lengths = event_reader.get_chunk()

    assert len(samples) == 3
    for csv_name, frame in samples.items():
        assert len(frame) == 5000
        assert frame_lengths[csv_name] == 5000
        assert frame.index[0] == 5000
        assert frame.index[-1] == 9999
        assert not (set(frame.columns) - set(columns))
        assert not (set(columns) - set(frame.columns))
        assert_dtypes(frame)

    tests_io("Test case getting multiple chunks succeeded")


def test_resume_get_chunk():
    tests_io("Test case resuming get_chunk", level=0)
    orig_tracker = ExtractionTracker(TEMP_DIR, num_samples=None)
    orig_event_reader = EventReader(chunksize=5000,
                                    dataset_folder=TEST_DATA_DEMO,
                                    tracker=orig_tracker)

    samples, frame_lengths = orig_event_reader.get_chunk()

    assert len(samples) == 3
    for csv_name, frame in samples.items():
        assert len(frame) == 5000
        assert frame_lengths[csv_name] == 5000
        assert frame.index[0] == 0
        assert frame.index[-1] == 4999
        assert not (set(frame.columns) - set(columns))
        assert not (set(columns) - set(frame.columns))
        assert_dtypes(frame)

    # Tracker register the frames as read
    orig_tracker.count_subject_events += frame_lengths

    restored_tracker = ExtractionTracker(TEMP_DIR)
    restored_event_reader = EventReader(chunksize=5000,
                                        dataset_folder=TEST_DATA_DEMO,
                                        tracker=restored_tracker)

    samples, frame_lengths = restored_event_reader.get_chunk()

    assert len(samples) == 3
    for csv_name, frame in samples.items():
        assert len(frame) == 5000
        assert frame_lengths[csv_name] == 5000
        assert frame.index[0] == 5000
        assert frame.index[-1] == 9999
        assert not (set(frame.columns) - set(columns))
        assert not (set(columns) - set(frame.columns))
        assert_dtypes(frame)

    # Event producer only read 200 samples then crashed
    orig_tracker.count_subject_events += {
        "CHARTEVENTS.csv": 200,
        "LABEVENTS.csv": 200,
        "OUTPUTEVENTS.csv": 200
    }

    restored_tracker = ExtractionTracker(TEMP_DIR)
    restored_event_reader = EventReader(chunksize=5000,
                                        dataset_folder=TEST_DATA_DEMO,
                                        tracker=restored_tracker)

    # Resume reading after crash from same chunk
    samples, frame_lengths = restored_event_reader.get_chunk()

    assert len(samples) == 3
    for csv_name, frame in samples.items():
        assert len(frame) == 4800
        assert frame_lengths[csv_name] == 4800
        assert frame.index[0] == 5200
        assert frame.index[-1] == 9999
        assert not (set(frame.columns) - set(columns))
        assert not (set(columns) - set(frame.columns))
        assert_dtypes(frame)

    tests_io("Test case resuming get_chunk succeeded")


def test_switch_chunk_sizes():
    tests_io("Test case switching chunk sizes", level=0)
    orig_tracker = ExtractionTracker(TEMP_DIR, num_samples=None)
    orig_event_reader = EventReader(chunksize=4000,
                                    dataset_folder=TEST_DATA_DEMO,
                                    tracker=orig_tracker)

    samples, frame_lengths = orig_event_reader.get_chunk()
    orig_tracker.count_subject_events += frame_lengths

    assert len(samples) == 3
    for csv_name, frame in samples.items():
        assert len(frame) == 4000
        assert frame_lengths[csv_name] == 4000
        assert frame.index[0] == 0
        assert frame.index[-1] == 3999
        assert not (set(frame.columns) - set(columns))
        assert not (set(columns) - set(frame.columns))
        assert_dtypes(frame)

    # Crash and resume with half the chunk size
    orig_tracker = ExtractionTracker(TEMP_DIR, num_samples=None)
    orig_event_reader = EventReader(chunksize=2000,
                                    dataset_folder=TEST_DATA_DEMO,
                                    tracker=orig_tracker)

    samples, frame_lengths = orig_event_reader.get_chunk()
    orig_tracker.count_subject_events += frame_lengths

    assert len(samples) == 3
    for csv_name, frame in samples.items():
        assert len(frame) == 2000
        assert frame_lengths[csv_name] == 2000
        assert frame.index[0] == 4000
        assert frame.index[-1] == 5999
        assert not (set(frame.columns) - set(columns))
        assert not (set(columns) - set(frame.columns))
        assert_dtypes(frame)

    # Crash and resume with half the chunk size
    orig_tracker = ExtractionTracker(TEMP_DIR, num_samples=None)
    orig_event_reader = EventReader(chunksize=1000,
                                    dataset_folder=TEST_DATA_DEMO,
                                    tracker=orig_tracker)

    samples, frame_lengths = orig_event_reader.get_chunk()
    orig_tracker.count_subject_events += frame_lengths

    assert len(samples) == 3
    for csv_name, frame in samples.items():
        assert len(frame) == 1000
        assert frame_lengths[csv_name] == 1000
        assert frame.index[0] == 6000
        assert frame.index[-1] == 6999
        assert not (set(frame.columns) - set(columns))
        assert not (set(columns) - set(frame.columns))
        assert_dtypes(frame)

    # Crash and resume with half the chunk size
    orig_tracker = ExtractionTracker(TEMP_DIR, num_samples=None)
    orig_event_reader = EventReader(chunksize=500,
                                    dataset_folder=TEST_DATA_DEMO,
                                    tracker=orig_tracker)

    samples, frame_lengths = orig_event_reader.get_chunk()
    orig_tracker.count_subject_events += frame_lengths

    assert len(samples) == 3
    for csv_name, frame in samples.items():
        assert len(frame) == 500
        assert frame_lengths[csv_name] == 500
        assert frame.index[0] == 7000
        assert frame.index[-1] == 7499
        assert not (set(frame.columns) - set(columns))
        assert not (set(columns) - set(frame.columns))
        assert_dtypes(frame)

    tests_io("Test case switching chunk sizes succeeded")


def test_subject_ids():
    """Test if the event reader stops on last subject occurence.
    """
    tests_io("Test case with subject ids", level=0)
    tracker = ExtractionTracker(TEMP_DIR, num_samples=None)
    subject_ids = ["40124"]  # int version 40124
    event_reader = EventReader(chunksize=1000000,
                               subject_ids=subject_ids,
                               dataset_folder=TEST_DATA_DEMO,
                               tracker=tracker)
    # Takes only a second on my machine
    sleep(2)
    frame_lengths = True
    previous_samples = {}
    test_csv = ["CHARTEVENTS.csv", "LABEVENTS.csv", "OUTPUTEVENTS.csv"]

    sample_buffer = {"CHARTEVENTS.csv": list(), "LABEVENTS.csv": list(), "OUTPUTEVENTS.csv": list()}

    while frame_lengths:
        samples, frame_lengths = event_reader.get_chunk()
        for csv in frame_lengths:
            if frame_lengths[csv]:
                previous_samples[csv] = samples[csv]
                sample_buffer[csv].append(samples[csv])
    assert sample_buffer
    for csv in previous_samples.keys():
        last_occurence = event_reader._last_occurrence[csv]
        last_sample = previous_samples[csv].index[-1]

        # Test if all events have been read
        assert np.isclose(
            last_occurence, last_sample, atol=1
        ) and last_occurence >= last_sample, f"Last occurence line: {last_occurence} and last sample line: {last_sample}"
        tests_io(
            f"{csv}: Last occurence line: {last_occurence} and last sample line: {last_sample}")
        test_csv.remove(csv)

    all_data = event_reader.get_all()
    all_data = all_data[all_data["SUBJECT_ID"].isin([40124])]
    all_data = all_data.sort_values(
        by=["SUBJECT_ID", "HADM_ID", "ICUSTAY_ID", "CHARTTIME", "ITEMID"])
    all_data = all_data.reset_index(drop=True)
    chunk_data = pd.concat([pd.concat(buffer) for buffer in sample_buffer.values()])
    chunk_data = chunk_data.sort_values(
        by=["SUBJECT_ID", "HADM_ID", "ICUSTAY_ID", "CHARTTIME", "ITEMID"])
    chunk_data = chunk_data.reset_index(drop=True)
    assert_dataframe_equals(chunk_data.astype("object"), all_data.astype("object"))
    assert_dtypes(chunk_data)
    assert_dtypes(all_data)

    assert not test_csv, f"Test csvs not empty: {test_csv}"
    tests_io("Test case with subject ids succeeded")


if __name__ == '__main__':
    # if TEMP_DIR.is_dir():
    #     shutil.rmtree(str(TEMP_DIR))
    import shelve
    reader = datasets.load_data(chunksize=75837, source_path=TEST_DATA_DEMO, storage_path=TEMP_DIR)

    def reset():
        with shelve.open(str(Path(TEMP_DIR, "extracted"))) as db:
            db.clear()

    reset()
    test_subject_ids()
    reset()
    test_get_full_chunk()
    reset()
    test_get_mutlitple_chunks()
    reset()
    test_switch_chunk_sizes()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))


### FILE: .\tests\test_datasets\test_readers\test_extracted_set_reader.py ###
import datasets
import os
import pandas as pd
import re
import shutil
import random
import pytest
from pathlib import Path
from utils.IO import *
from tests.settings import *
from datasets.readers import ExtractedSetReader
from datasets.mimic_utils import convert_dtype_dict

ground_truth_subject_ids = [
    int(subject_dir.name)
    for subject_dir in Path(TEST_GT_DIR, "extracted").iterdir()
    if subject_dir.name.isnumeric()
]

ground_truth_stay_ids = [
    int(icustay) for subject_dir in Path(TEST_GT_DIR, "extracted").iterdir()
    if subject_dir.name.isnumeric()
    for icustay in pd.read_csv(Path(subject_dir, "stays.csv")).ICUSTAY_ID.to_numpy().tolist()
]

# Consider storing this somewhere else
frame_properties = {
    "timeseries": {
        "columns": [
            'Capillary refill rate', 'Diastolic blood pressure', 'Fraction inspired oxygen',
            'Glascow coma scale eye opening', 'Glascow coma scale motor response',
            'Glascow coma scale total', 'Glascow coma scale verbal response', 'Glucose',
            'Heart Rate', 'Height', 'Mean blood pressure', 'Oxygen saturation', 'pH',
            'Respiratory rate', 'Systolic blood pressure', 'Temperature', 'Weight'
        ],
        "index": "hours"
    },
    "episodic_data": {
        "columns": ['AGE', 'LOS', 'MORTALITY', 'GENDER', 'ETHNICITY', 'Height', 'Weight'],
        "index": "Icustay"
    },
    "subject_events": {
        "columns": [
            'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'CHARTTIME', 'ITEMID', 'VALUE', 'VALUEUOM'
        ],
        "index": None
    },
    "subject_diagnoses": {
        "columns": [
            'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'ICD9_CODE', 'SHORT_TITLE', 'LONG_TITLE',
            'ICUSTAY_ID', 'HCUP_CCS_2015', 'USE_IN_BENCHMARK'
        ],
        "index": None
    },
    "subject_icu_history": {
        "columns": [
            'SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'LAST_CAREUNIT', 'DBSOURCE', 'INTIME', 'OUTTIME',
            'LOS', 'ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'ETHNICITY', 'DIAGNOSIS', 'GENDER', 'DOB',
            'DOD', 'AGE', 'MORTALITY_INUNIT', 'MORTALITY', 'MORTALITY_INHOSPITAL'
        ],
        "index": None
    }
}

idx_to_file_type = {
    0: "timeseries",
    1: "episodic_data",
    2: "subject_events",
    3: "subject_diagnoses",
    4: "subject_icu_history"
}

FILE_TYPE_KEYS = ("timeseries", "episodic_data", "subject_events", "subject_diagnoses",
                  "subject_icu_history")

DTYPES = {
    file_type: DATASET_SETTINGS[file_index]["dtype"]
    for file_type, file_index in zip(FILE_TYPE_KEYS, [
        "timeseries",
        "episodic_data",
        "subject_events",
        "diagnosis",
        "icu_history",
    ])
}

READER_TO_FILE_TYPE = {
    "read_episodic_data": "episodic_data",
    "read_events": "subject_events",
    "read_diagnoses": "subject_diagnoses",
    "read_icu_history": "subject_icu_history"
}


def test_properties(extracted_reader: ExtractedSetReader):
    # TODO! test dtypes
    tests_io("Test case properties for ExtractedSetReader", level=0)
    reader = extracted_reader
    assert reader.root_path == Path(
        SEMITEMP_DIR, "extracted"
    ), f"Expected root path is {str(Path(SEMITEMP_DIR, 'extracted'))}, but reader root path is {str(reader.root_path)}"

    # Should not be able to set root path
    with pytest.raises(AttributeError) as error:
        reader.root_path = Path("test")
        assert error.info == "can't set attribute"
    tests_io("Root path is correctly set")

    assert reader.subject_ids == ground_truth_subject_ids, f"Subjects are not in the ground truth: {list(set(reader.subject_ids) - set(ground_truth_subject_ids))}\n Subjects are missing from the reader attribute: {list(set(ground_truth_subject_ids) - set(reader.subject_ids))}"

    # Should not be able to set subject ids
    with pytest.raises(AttributeError) as error:
        reader.subject_ids = []
        assert error.info == "can't set attribute"
    tests_io("Subject ids are correct")


def assert_dtypes(dataframe: pd.DataFrame, dtypes: dict):
    assert all([
        dataframe.dtypes[column] == "object"
        if dtype == "str" else dataframe.dtypes[column] == dtype  # Might be translated to obj
        for column, dtype in dtypes.items()
        if column in dataframe
    ])
    return True


def test_read_csv(extracted_reader: ExtractedSetReader):
    # TODO! test dtypes
    tests_io("Test case read csv for ExtractedSetReader", level=0)
    reader = extracted_reader

    # Test dtypes of different bites sizes
    dtype_mapping_template = {
        'ROW_ID': 'Int32',
        'SUBJECT_ID': 'Int64',
        'HADM_ID': 'Int32',
        'SEQ_NUM': 'Int8',
        'ICD9_CODE': 'str',
        'ICUSTAY_ID': 'float'
    }
    dtype_mapping = convert_dtype_dict(dtype_mapping_template, add_lower=False)

    absolute_diagnoses_path = Path(SEMITEMP_DIR, "extracted", "diagnoses.csv")
    absolute_df = reader.read_csv(absolute_diagnoses_path, dtypes=dtype_mapping)
    assert not absolute_df.empty, f"The file {str(absolute_diagnoses_path)} could not be found using absolute resolution!"
    assert_dtypes(absolute_df, dtype_mapping)

    relative_df = reader.read_csv("diagnoses.csv", dtypes=dtype_mapping)
    assert not relative_df.empty, f"The file {str(absolute_diagnoses_path)} could not be found using relative resolution!"
    assert_dtypes(relative_df, dtype_mapping)

    tests_io("Read CSV working with file name and absolute path")

    # Test dtype too small error
    dtype_mapping_template['ROW_ID'] = 'Int8'
    dtype_mapping = convert_dtype_dict(dtype_mapping_template, add_lower=False)
    with pytest.raises(TypeError) as error:
        absolute_df = reader.read_csv(absolute_diagnoses_path, dtypes=dtype_mapping)

    # Test for file types this is used on
    for file_name in ["episodic_info_df.csv", "icu_history.csv", "subject_info.csv"]:
        absolute_diagnoses_path = Path(SEMITEMP_DIR, "extracted", file_name)
        # Get dtypes
        settings_index_name = file_name.rstrip(".csv").rstrip("_df")
        dtypes = convert_dtype_dict(DATASET_SETTINGS[settings_index_name]["dtype"], add_lower=False)
        # Test absoulte read
        absolute_df = reader.read_csv(absolute_diagnoses_path, dtypes=dtypes)
        assert not absolute_df.empty, f"The file {str(absolute_diagnoses_path)} could not be found using absolute resolution!"
        assert_dtypes(absolute_df, dtypes)
        # Test relative read
        relative_df = reader.read_csv(file_name, dtypes=dtypes)
        assert not relative_df.empty, f"The file {str(absolute_diagnoses_path)} could not be found using relative resolution!"
        assert_dtypes(relative_df, dtypes)
        tests_io(f"Read CSV working correctly with {file_name}")
    tests_io("Read CSV tested successfully")


def test_read_timeseries(extracted_reader: ExtractedSetReader):
    tests_io("Test case read timeseries for ExtractedSetReader", level=0)
    reader = extracted_reader

    # --- test correct structure ---
    data = reader.read_timeseries(read_ids=True)
    # Make sure all subjects have stays dict
    assert all([isinstance(stays_dict, dict) for _, stays_dict in data.items()])

    # Make sure all indices are integers
    assert all([
        isinstance(subj_id, int) and isinstance(stay_id, int)
        for subj_id, stays_dict in data.items()
        for stay_id, _ in stays_dict.items()
    ])

    # Make sure all stays are frames
    assert all([
        isinstance(frame, pd.DataFrame)
        for _, stays_dict in data.items()
        for _, frame in stays_dict.items()
    ])
    # Ensure dtype correcteness
    assert all([
        assert_dtypes(frame, DTYPES["timeseries"])
        for _, stays_dict in data.items()
        for _, frame in stays_dict.items()
    ])
    # Make sure all frames are read
    assert all(
        [not frame.empty for _, stays_dict in data.items() for _, frame in stays_dict.items()])
    tests_io("Correct structure of timeseries data with ids")

    # --- test correct num subjects ---
    data = reader.read_timeseries(read_ids=True, num_subjects=10)

    # Make sure all indices are integers
    assert all([
        isinstance(subj_id, int) and isinstance(stay_id, int)
        for subj_id, stays_dict in data.items()
        for stay_id, _ in stays_dict.items()
    ])
    assert [isinstance(stays_dict, dict) for _, stays_dict in data.items()]
    # Make sure all stays are frames
    assert all([
        isinstance(frame, pd.DataFrame)
        for _, stays_dict in data.items()
        for _, frame in stays_dict.items()
    ])
    # Assert no empty frames
    assert all(
        [not frame.empty for _, stays_dict in data.items() for _, frame in stays_dict.items()])
    assert len(data) == 10
    assert all([
        assert_dtypes(frame, DTYPES["timeseries"])
        for _, stays_dict in data.items()
        for _, frame in stays_dict.items()
    ])
    tests_io("Correct num subjects when sepcified for timeseries data")

    data = reader.read_timeseries()
    assert len(data) == len(ground_truth_stay_ids)
    assert all([isinstance(frame, pd.DataFrame) for frame in data])
    assert all([not frame.empty for frame in data])
    assert all([assert_dtypes(frame, DTYPES["timeseries"]) for frame in data])
    tests_io("Correct dimension of timeseries data")
    tests_io("Timeseries read tested successfully")


@pytest.mark.parametrize(
    "reader_name", ["read_episodic_data", "read_events", "read_diagnoses", "read_icu_history"])
def test_read_remaining_file_types(reader_name: str, extracted_reader: ExtractedSetReader):
    tests_io(f"Test case {reader_name} for ExtractedSetReader", level=0)
    reader_method = getattr(extracted_reader, reader_name)
    file_dtypes = DTYPES[READER_TO_FILE_TYPE[reader_name]]

    # --- test correct structure ---
    data = reader_method(read_ids=True)
    assert all([
        isinstance(subj_id, int)
        for subj_id, stays_dict in data.items()
        if isinstance(stays_dict, dict)
    ])
    # Make sure all subjects have stays dict
    assert [isinstance(stays_dict, pd.DataFrame) for _, stays_dict in data.items()]
    # Make sure all stays are frames
    assert all([not frame.empty for _, frame in data.items()])
    assert all([assert_dtypes(frame, file_dtypes) for frame in data.values()])
    tests_io(f"Correct structure of {reader_name} data with ids")

    # --- test correct num subjects ---
    data = reader_method(read_ids=True, num_subjects=10)
    assert [isinstance(stays_dict, pd.DataFrame) for _, stays_dict in data.items()]
    assert all([not frame.empty for _, frame in data.items()])
    assert all([assert_dtypes(frame, file_dtypes) for frame in data.values()])
    assert len(data) == 10
    tests_io(f"Correct num subjects when sepcified for {reader_name} data")

    data = reader_method()
    assert len(data) == len(ground_truth_subject_ids)
    assert all([not frame.empty for frame in data])
    assert all([assert_dtypes(frame, file_dtypes) for frame in data])
    tests_io(f"Correct dimension of {reader_name} data")
    tests_io(f"{reader_name} read tested successfully")


def test_read_subjects(extracted_reader: ExtractedSetReader):
    tests_io("Test case read subjects for ExtractedSetReader", level=0)

    reader = extracted_reader
    data_with_ids = reader.read_subjects(read_ids=True)

    # Make sure no subjects missing or additional
    gt_subject_ids = [
        int(directory.name)
        for directory in Path(TEST_GT_DIR, "extracted").iterdir()
        if directory.name.isnumeric()
    ]
    assert not set(gt_subject_ids) - set(data_with_ids.keys()) and not set(
        data_with_ids.keys()) - set(gt_subject_ids)
    tests_io("Correct dimension of dataset relative to subject ids")

    # Make sure no stays missing or additional
    gt_stay_ids = [
        re.findall('[0-9]+', file.name).pop()
        for directory in Path(TEST_GT_DIR, "extracted").iterdir() if directory.name.isnumeric()
        for file in directory.iterdir() if re.findall('[0-9]+', file.name)
    ]
    extracted_stay_ids = [
        str(stay_id)
        for subject_data in data_with_ids.values()
        for stay_id in subject_data["timeseries"].keys()
    ]
    assert not set(gt_stay_ids) - set(extracted_stay_ids) and \
           not set(extracted_stay_ids) - set(gt_stay_ids)
    tests_io("Correct dimension of dataset relative to stay ids")

    # Make sure the correct columns and indices are read for every file:
    for subject_data in data_with_ids.values():
        validate_subject_data(subject_data, file_type_keys=True, read_ids=True)

    data_without_ids = reader.read_subjects()
    assert len(data_without_ids) == len(gt_subject_ids)
    assert sum([len(subject_data["timeseries"]) for subject_data in data_without_ids
               ]) == len(extracted_stay_ids)

    for subject_data in data_without_ids:
        validate_subject_data(subject_data, file_type_keys=True, read_ids=False)

    data_without_keys = reader.read_subjects(file_type_keys=False)
    assert len(data_without_keys) == len(gt_subject_ids)
    assert sum([len(subject_data[0]) for subject_data in data_without_keys
               ]) == len(extracted_stay_ids)

    for subject_data in data_without_keys:
        validate_subject_data(subject_data, file_type_keys=False, read_ids=False)
    tests_io("Correct structure of dataset relative to file type keys")

    # Test the num samples option
    data_with_num_subjects = reader.read_subjects(read_ids=True, num_subjects=10)
    assert len(data_with_num_subjects) == 10
    for subject_data in data_with_num_subjects.values():
        validate_subject_data(subject_data, file_type_keys=True, read_ids=True)
    tests_io("Correct number of subjects when specified")

    # Test the subject ids option
    data_with_subject_ids = reader.read_subjects(read_ids=True,
                                                 subject_ids=["10006", "10011", "10036", "10088"])
    assert len(data_with_subject_ids) == 4
    assert list(data_with_subject_ids.keys()) == [10006, 10011, 10036, 10088]
    for subject_data in data_with_subject_ids.values():
        validate_subject_data(subject_data, file_type_keys=True, read_ids=True)
    tests_io("Correct subjects when specified")
    tests_io("Read subjects tested successfully")


def test_read_subjects_dir(extracted_reader: ExtractedSetReader):
    tests_io("Test case read subjects ExtractedSetReader", level=0)
    reader = extracted_reader
    ## With ids
    # Absolute
    data_with_ids = reader.read_subject(Path(reader.root_path, "10019"), read_ids=True)
    validate_subject_data(data_with_ids, file_type_keys=True, read_ids=True)
    # Relative
    data_with_ids = reader.read_subject("10019", read_ids=True)
    validate_subject_data(data_with_ids, file_type_keys=True, read_ids=True)
    tests_io("Correct relative and absolute resolution of subject data with ids")

    ## Without ids
    # Absolute
    data_without_ids = reader.read_subject(Path(reader.root_path, "10019"))
    validate_subject_data(data_without_ids, file_type_keys=True, read_ids=False)
    # Relative
    data_without_ids = reader.read_subject("10019")
    validate_subject_data(data_without_ids, file_type_keys=True, read_ids=False)
    tests_io("Correct relative and absolute resolution of subject data without ids")

    ## Without ids without file type keys
    # Absolute
    data_without_keys = reader.read_subject(Path(reader.root_path, "10019"), file_type_keys=False)
    validate_subject_data(data_without_keys, file_type_keys=False, read_ids=False)
    # Relative
    data_without_keys = reader.read_subject(Path(reader.root_path, "10019"), file_type_keys=False)
    validate_subject_data(data_without_keys, file_type_keys=False, read_ids=False)
    tests_io("Correct relative and absolute resolution of "
             "subject data without ids and file type keys")

    file_types = ("episodic_data", "subject_events", "subject_diagnoses", "subject_icu_history",
                  "timeseries")
    with pytest.raises(ValueError) as error:
        reader.read_subject(Path(reader.root_path, "10019"), file_types=("episodic_data"))
        assert error.value == f'file_types must be a tuple but is {type("")}'

    for _ in range(10):
        random_filetypes = random.sample(file_types, random.randint(1, 5))
        data = reader.read_subject("10019", file_types=random_filetypes, read_ids=True)
        validate_subject_data(data, file_type_keys=True, read_ids=True)

        data = reader.read_subject("10019", file_types=random_filetypes, read_ids=False)
        validate_subject_data(data, file_type_keys=True, read_ids=False)

        file_type_map = dict(enumerate(list(data.keys())))
        data = reader.read_subject("10019",
                                   file_types=random_filetypes,
                                   read_ids=False,
                                   file_type_keys=False)
        validate_subject_data(data,
                              file_type_keys=False,
                              read_ids=False,
                              file_type_mapping=file_type_map)
    tests_io("Correct file types subsampling when specified")
    tests_io("Read subjects tested successfully")


def validate_subject_data(subject_data,
                          file_type_keys,
                          read_ids,
                          file_type_mapping=idx_to_file_type):
    assert subject_data, "Subject data is empty!"
    # Make sure all file types are present in the data by checking their column names
    for idx, frame in subject_data.items() if file_type_keys else enumerate(subject_data):
        if not file_type_keys:
            idx = file_type_mapping[idx]
        if isinstance(frame, pd.DataFrame):
            filtered_colnames = [
                col for col in frame.columns if not col.isnumeric() and not col[1:].isnumeric()
            ]
            assert_dtypes(frame, DTYPES[idx])
            assert filtered_colnames == frame_properties[idx]["columns"]
            assert frame.index.name == frame_properties[idx]["index"]
        else:
            for value in frame.values() if read_ids else frame:
                filtered_colnames = [
                    col for col in value.columns if not col.isnumeric() and not col[1:].isnumeric()
                ]
                assert_dtypes(value, DTYPES[idx])
                assert filtered_colnames == frame_properties[idx]["columns"]
                assert value.index.name == frame_properties[idx]["index"]
    return


if __name__ == "__main__":
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    reader = datasets.load_data(chunksize=75835,
                                source_path=TEST_DATA_DEMO,
                                storage_path=SEMITEMP_DIR,
                                task="PHENO")
    test_properties(reader)
    test_read_csv(reader)
    test_read_timeseries(reader)
    for reader_mname in ["read_episodic_data", "read_events", "read_diagnoses", "read_icu_history"]:
        test_read_remaining_file_types(reader_mname, reader)
    test_read_subjects_dir(reader)
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))


### FILE: .\tests\test_datasets\test_readers\test_processed_set_reader.py ###
import datasets
import pytest
import pandas as pd
import numpy as np
from copy import deepcopy
from utils.IO import *
from tests.settings import *
from datasets.readers import ProcessedSetReader
from datasets.mimic_utils import upper_case_column_names

timeseries_label_cols = {
    "IHM":
        "Y",
    "DECOMP":
        "Y",
    "LOS":
        "Y",
    "PHENO": [
        'y0', 'y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8', 'y9', 'y10', 'y11', 'y12', 'y13',
        'y14', 'y15', 'y16', 'y17', 'y18', 'y19', 'y20', 'y21', 'y22', 'y23', 'y24'
    ]
}

DTYPES = {
    column_name.upper(): np.float64 if dtype == "float64" else object
    for column_name, dtype in DATASET_SETTINGS["timeseries"]["dtype"].items()
}


@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_read_sample(task_name: str, preprocessed_readers: ProcessedSetReader):
    tests_io(f"Test case read sample for task {task_name}", level=0)
    reader = preprocessed_readers[task_name]

    # 10017: Single stay
    # 40124: Multiple stays

    # Testing without reading ids and timestamps
    check_sample(reader.read_sample(10017),
                 read_ids=False,
                 read_timestamps=False,
                 task_name=task_name)
    check_sample(reader.read_sample(40124),
                 read_ids=False,
                 read_timestamps=False,
                 task_name=task_name)
    tests_io(f"Suceeded testing read sample for task {task_name} without ids and timestamps passed")

    # Testing with reading ids and without timestamps
    check_sample(reader.read_sample(40124, read_ids=True),
                 read_ids=True,
                 read_timestamps=False,
                 task_name=task_name)
    check_sample(reader.read_sample(10017, read_ids=True),
                 read_ids=True,
                 read_timestamps=False,
                 task_name=task_name)
    tests_io(
        f"Suceeded testing read sample for task {task_name} with ids and without timestamps passed")

    # Testing with reading ids and timestamps
    check_sample(reader.read_sample(10017, read_ids=True, read_timestamps=True),
                 read_ids=True,
                 read_timestamps=True,
                 task_name=task_name)
    check_sample(reader.read_sample(10017, read_ids=True, read_timestamps=True),
                 read_ids=True,
                 read_timestamps=True,
                 task_name=task_name)
    tests_io(f"Suceeded testing read sample for task {task_name} with ids and timestamps passed")

    # Testing convert to numpy on read
    check_sample(reader.read_sample(40124, data_type=np.ndarray),
                 read_ids=False,
                 read_timestamps=False,
                 data_type=np.ndarray,
                 task_name=task_name)

    check_sample(reader.read_sample(40124, data_type=pd.DataFrame),
                 read_ids=False,
                 read_timestamps=False,
                 data_type=pd.DataFrame,
                 task_name=task_name)
    tests_io(f"Suceeded testing read sample for task {task_name} with numpy conversion passed")


@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_read_samples(task_name: str, preprocessed_readers: ProcessedSetReader):
    tests_io(f"Test case read samples for task {task_name}", level=0)
    reader = preprocessed_readers[task_name]
    # Testing without reading ids and timestamps
    check_sample(reader.read_samples([10017, 40124]),
                 read_ids=False,
                 read_timestamps=False,
                 task_name=task_name)
    check_sample(reader.read_samples([10017]),
                 read_ids=False,
                 read_timestamps=False,
                 task_name=task_name)
    check_sample(reader.read_samples(reader.subject_ids),
                 read_ids=False,
                 read_timestamps=False,
                 task_name=task_name)
    tests_io(
        f"Suceeded testing read samples for task {task_name} without ids and timestamps passed")

    # Testing with reading ids but without timestamps
    check_samples(reader.read_samples([10017, 40124], read_ids=True),
                  read_timestamps=False,
                  task_name=task_name)
    check_samples(reader.read_samples([10017], read_ids=True),
                  read_timestamps=False,
                  task_name=task_name)
    check_samples(reader.read_samples(reader.subject_ids, read_ids=True),
                  read_timestamps=False,
                  task_name=task_name)
    tests_io(
        f"Suceeded testing read samples for task {task_name} with ids and without timestamps passed"
    )

    # Testing with reading ids and timestamps
    check_samples(reader.read_samples([10017, 40124], read_ids=True, read_timestamps=True),
                  read_timestamps=True,
                  task_name=task_name)
    check_samples(reader.read_samples([10017], read_ids=True, read_timestamps=True),
                  read_timestamps=True,
                  task_name=task_name)
    check_samples(reader.read_samples(reader.subject_ids, read_ids=True, read_timestamps=True),
                  read_timestamps=True,
                  task_name=task_name)
    tests_io(f"Suceeded testing read samples for task {task_name} with ids and timestamps passed")

    # Testing convert to numpy on read
    check_sample(reader.read_samples([40124, 10017], data_type=np.ndarray),
                 read_ids=False,
                 read_timestamps=False,
                 data_type=np.ndarray,
                 task_name=task_name)

    check_sample(reader.read_samples([40124, 10017], data_type=pd.DataFrame),
                 read_ids=False,
                 read_timestamps=False,
                 data_type=pd.DataFrame,
                 task_name=task_name)
    tests_io(f"Suceeded testing read samples for task {task_name} with numpy conversion passed")


@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_random_sample(task_name: str, preprocessed_readers: ProcessedSetReader):
    tests_io(f"Test case random samples for task {task_name}", level=0)
    reader = preprocessed_readers[task_name]
    # Testing without reading ids and timestamps
    check_sample(reader.random_samples(10),
                 read_ids=False,
                 read_timestamps=False,
                 task_name=task_name)
    check_sample(reader.random_samples(),
                 read_ids=False,
                 read_timestamps=False,
                 task_name=task_name)
    check_sample(reader.random_samples(n_samples=10),
                 read_ids=False,
                 read_timestamps=False,
                 task_name=task_name)
    tests_io(
        f"Suceeded testing random samples for task {task_name} without ids and timestamps passed")

    # Testing with reading ids but without timestamps
    check_samples(reader.random_samples(10, read_ids=True),
                  read_timestamps=False,
                  task_name=task_name)
    check_samples(reader.random_samples(read_ids=True), read_timestamps=False, task_name=task_name)
    check_samples(reader.random_samples(n_samples=10, read_ids=True),
                  read_timestamps=False,
                  task_name=task_name)
    tests_io(
        f"Suceeded testing random samples for task {task_name} with ids and without timestamps passed"
    )

    # Testing with reading ids and timestamps
    check_samples(reader.random_samples(10, read_ids=True, read_timestamps=True),
                  read_timestamps=True,
                  task_name=task_name)
    check_samples(reader.random_samples(read_ids=True, read_timestamps=True),
                  read_timestamps=True,
                  task_name=task_name)
    check_samples(reader.random_samples(n_samples=10, read_ids=True, read_timestamps=True),
                  read_timestamps=True,
                  task_name=task_name)
    tests_io(f"Suceeded testing random samples for task {task_name} with ids and timestamps passed")

    # Test without replacement property
    samples = reader.random_samples(len(reader.subject_ids), read_ids=True)
    check_samples(samples=samples, read_timestamps=False, task_name=task_name)
    assert len(set(samples["X"].keys())) == len(samples["X"].keys())
    assert len(set(samples["X"].keys())) == len(reader.subject_ids)

    # Exceeding the set size results in warning and set sized sample collection
    samples = reader.random_samples(2 * len(reader.subject_ids), read_ids=True)
    check_samples(samples, read_timestamps=False, task_name=task_name)
    assert len(set(samples["X"].keys())) == len(samples["X"].keys())
    assert len(set(samples["X"].keys())) == len(reader.subject_ids)
    tests_io(f"Suceeded testing random samples for task {task_name} without replacement passed")


def check_samples(samples: dict, read_timestamps: bool, data_type=None, task_name=None):
    assert isinstance(samples, dict)
    assert set(samples.keys()) == set(["X", "y", "t"] if read_timestamps else ["X", "y"])
    for subject_id in samples["X"]:
        assert isinstance(subject_id, int)
        sample = {"X": samples["X"][subject_id], "y": samples["y"][subject_id]}
        if read_timestamps:
            sample.update({"t": samples["t"][subject_id]})
        check_sample(sample,
                     read_ids=True,
                     read_timestamps=read_timestamps,
                     data_type=data_type,
                     task_name=task_name)


def check_sample(sample: dict,
                 read_ids: bool,
                 read_timestamps: bool,
                 data_type=None,
                 task_name=None):
    assert set(sample.keys()) == set(["X", "y", "t"] if read_timestamps else ["X", "y"])
    X, Y = sample["X"], sample["y"]

    if read_ids:
        assert isinstance(X, dict)
        assert isinstance(Y, dict)
        assert len(X) == len(Y)
        assert all([
            isinstance(stay_id, int) and
            (isinstance(timeseries, np.ndarray) if data_type == np.ndarray else isinstance(
                timeseries, pd.DataFrame)) for stay_id, timeseries in X.items()
        ])
    else:
        assert isinstance(X, list)
        assert isinstance(Y, list)
        assert len(X) == len(Y)
        assert all([(isinstance(timeseries, np.ndarray) if data_type == np.ndarray else isinstance(
            timeseries, pd.DataFrame)) for timeseries in X])
    if data_type == pd.DataFrame or data_type is None:
        for X_sample, Y_sample in (zip(X.values(), Y.values()) if read_ids else zip(X, Y)):
            X_sample = upper_case_column_names(deepcopy(X_sample))

            assert set([
                column.upper() for column in DATASET_SETTINGS["timeseries"]["dtype"].keys()
            ]) == set(X_sample)
            assert set([column.upper() for column in timeseries_label_cols[task_name]
                       ]) == set(Y_sample.columns.str.upper())

            assert all([X_sample[column].dtype == DTYPES[column] for column in X_sample.columns])
    else:
        for X_sample, Y_sample in (zip(X.values(), Y.values()) if read_ids else zip(X, Y)):
            assert isinstance(X_sample, np.ndarray)
            assert isinstance(Y_sample, np.ndarray)
            assert X_sample.shape[1] == len(DATASET_SETTINGS["timeseries"]["dtype"])
            assert Y_sample.shape[1] == len(timeseries_label_cols[task_name])


if __name__ == "__main__":
    import shutil
    # if SEMITEMP_DIR.is_dir():
    #     shutil.rmtree(str(SEMITEMP_DIR))
    reader_dict = dict()
    for task in TASK_NAMES:
        reader = datasets.load_data(chunksize=75835,
                                    source_path=TEST_DATA_DEMO,
                                    storage_path=SEMITEMP_DIR,
                                    preprocess=True,
                                    task=task)
        reader_dict[task] = reader
    for task in TASK_NAMES:
        test_read_sample(task, reader_dict)
        test_read_samples(task, reader_dict)
        test_random_sample(task, reader_dict)

    print("All tests passed!")
    # if TEMP_DIR.is_dir():
    #     shutil.rmtree(str(TEMP_DIR))


### FILE: .\tests\test_datasets\test_readers\__init__.py ###


### FILE: .\tests\test_datasets\test_trackers.py\test_extraction_tracker.py ###
import pytest
import shutil
from datasets.trackers import ExtractionTracker
from pathlib import Path
from utils.IO import *
from tests.settings import *

TRACKER_STATE = {
    "count_subject_events": {
        "OUTPUTEVENTS.csv": 0,
        "LABEVENTS.csv": 0,
        "CHARTEVENTS.csv": 0
    },
    "has_episodic_data": False,
    "has_timeseries": False,
    "subject_ids": list(),
    "has_subject_events": False,
    "count_total_samples": 0,
    "has_icu_history": False,
    "has_diagnoses": False,
    "has_bysubject_info": False,
    "is_finished": False,
    "num_samples": None
}

EVENT_BOOLS = [
    "has_episodic_data", "has_subject_events", "has_timeseries", "has_bysubject_info", "is_finished"
]


def test_extraction_tracker_basics():
    tests_io("Test case basic capabilities of ExtractionTracker.", level=0)
    # Create an instance of ExtractionTracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_samples=None)

    # Test correct initialization
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        assert getattr(tracker, attribute) == value

    tests_io("Succeeded testing initialization.")
    # Assignment bools and nums
    for attribute, value in TRACKER_STATE.items():
        # These states influence the state of the tracker from within the ExtractionTracker __init__
        # and are therefore not part of basic funcitonalities
        if attribute not in ["count_subject_events", "subject_ids", "num_samples", "num_subjects"]:
            setattr(tracker, attribute,
                    True if isinstance(getattr(tracker, attribute), bool) else 10)

    tracker.count_subject_events = {
        "OUTPUTEVENTS.csv": 10,
        "LABEVENTS.csv": 10,
        "CHARTEVENTS.csv": 10
    }

    # Test correct assignment
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute not in ["count_subject_events", "subject_ids", "num_samples", "num_subjects"]:
            assert getattr(
                tracker,
                attribute) == (True if isinstance(getattr(tracker, attribute), bool) else 10)
            assert tracker._progress[attribute] == (True if isinstance(
                getattr(tracker, attribute), bool) else 10)

    assert tracker.count_subject_events == {
        "OUTPUTEVENTS.csv": 10,
        "LABEVENTS.csv": 10,
        "CHARTEVENTS.csv": 10
    }
    tests_io("Succeeded testing assignment.")

    # Test correct restoration after assignment
    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_samples=None)
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute not in ["count_subject_events", "subject_ids", "num_samples", "num_subjects"]:
            assert getattr(
                tracker,
                attribute) == (True if isinstance(getattr(tracker, attribute), bool) else 10)
            assert tracker._progress[attribute] == (True if isinstance(
                getattr(tracker, attribute), bool) else 10)

    assert tracker.count_subject_events == {
        "OUTPUTEVENTS.csv": 10,
        "LABEVENTS.csv": 10,
        "CHARTEVENTS.csv": 10
    }
    tests_io("Succeeded testing restoration.")

    # Test correct __iadd__ implementation
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if isinstance(getattr(tracker, attribute), (int, float)):
            setattr(tracker, attribute,
                    getattr(tracker, attribute) +
                    10)  #This is how iadd is perfomed for simple properties

    tracker.count_subject_events += {
        "OUTPUTEVENTS.csv": 10,
        "LABEVENTS.csv": 10,
        "CHARTEVENTS.csv": 10
    }

    for attribute, value in TRACKER_STATE.items():
        if isinstance(attribute, (int, float)):
            assert getattr(tracker, attribute) == 20
            assert tracker._progress[attribute] == 20

    assert tracker.count_subject_events == {
        "OUTPUTEVENTS.csv": 20,
        "LABEVENTS.csv": 20,
        "CHARTEVENTS.csv": 20
    }

    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_samples=None)

    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if isinstance(attribute, (int, float)):
            assert getattr(tracker, attribute) == 20

    assert tracker.count_subject_events == {
        "OUTPUTEVENTS.csv": 20,
        "LABEVENTS.csv": 20,
        "CHARTEVENTS.csv": 20
    }
    tests_io("Succeeded testing __iadd__ implementation.")


def test_num_samples_option():
    # Test the logic of increasing and decreasing num sapmles upon reinstantiation
    tests_io("Test case sample target option of ExtractionTracker.", level=0)
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_samples=10)

    assert tracker.num_samples == 10
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute != "num_samples":
            assert getattr(tracker, attribute) == value

    # Simulate extraction done
    for attribute in EVENT_BOOLS:
        setattr(tracker, attribute, True)
    tracker.count_total_samples = 10

    # Test decreasing the number of samples upon reinstantiation
    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_samples=5)
    # Check init
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute not in EVENT_BOOLS + ["num_samples", "count_total_samples"]:
            assert getattr(tracker, attribute) == value
    # Asser total samples unchanged
    assert tracker.count_total_samples == 10

    # Assert still done
    for attribute in EVENT_BOOLS:
        assert getattr(tracker, attribute) == True

    tests_io("Succeeded testing sample target reduction")
    # Test increasing the number of samples upon reinstantiation
    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_samples=15)
    # Asser total samples unchanged
    assert tracker.count_total_samples == 10

    # Assert not done
    for attribute in EVENT_BOOLS:
        assert getattr(tracker, attribute) == False

    # Check set correctly
    assert tracker.num_samples == 15

    tests_io("Succeeded testing sample target increase")

    # Simulate extraction done
    for attribute in EVENT_BOOLS:
        setattr(tracker, attribute, True)
    tracker.count_total_samples = 15
    # Test setting sample_target to None
    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_samples=None)
    # Check init
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute not in EVENT_BOOLS + ["num_samples", "count_total_samples"]:
            assert getattr(tracker, attribute) == value

    # Check extraction done
    for attribute in EVENT_BOOLS:
        assert getattr(tracker, attribute) == False

    # Check event read not done
    assert tracker.num_samples is None
    tests_io("Succeeded testing sample target set to None")


def test_num_subjects_option():
    # Test the logic of increasing and decreasing num sapmles upon reinstantiation
    tests_io("Test case sample target option of ExtractionTracker.", level=0)
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_subjects=10)
    # Check init
    assert tracker.num_subjects == 10
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute != "num_subjects":
            assert getattr(tracker, attribute) == value

    # Simulate extraction done
    for attribute in EVENT_BOOLS:
        setattr(tracker, attribute, True)
    tracker.subject_ids.extend(list(range(10)))

    # Test decreasing the number of samples upon reinstantiation
    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_subjects=5)
    # Check init
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute not in EVENT_BOOLS + ["num_samples", "subject_ids"]:
            assert getattr(tracker, attribute) == value
    # Assert subject ids unchanged
    assert tracker.subject_ids == list(range(10))
    # Assert done
    for attribute in EVENT_BOOLS:
        assert getattr(tracker, attribute) == True

    tests_io("Succeeded testing num subject reduction")

    # Test increasing the number of samples upon reinstantiation
    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_subjects=15)
    # Check init
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute not in EVENT_BOOLS + ["num_samples", "subject_ids"]:
            assert getattr(tracker, attribute) == value
    # Assert subject ids unchanged
    assert tracker.subject_ids == list(range(10))
    # Assert done
    for attribute in EVENT_BOOLS:
        assert getattr(tracker, attribute) == False
    tracker.subject_ids.extend(list(range(10, 15)))

    tests_io("Succeeded testing num subject increase")

    # Simulate extraction done
    for attribute in EVENT_BOOLS:
        setattr(tracker, attribute, True)

    # Test setting sample_target to None
    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), num_subjects=None)
    # Check init
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute not in EVENT_BOOLS + ["num_samples", "subject_ids"]:
            assert getattr(tracker, attribute) == value

    for attribute in EVENT_BOOLS:
        assert getattr(tracker, attribute) == False
    # Assert subject ids unchanged
    assert tracker.subject_ids == list(range(15))

    tests_io("Succeeded testing num subject set to None")


def test_subject_ids_option():
    # Test the logic of passing subject ids to the tracker
    # If all ids have already been extracted is_finished should be set to True
    # If some ids have not yet been extracted is_finished should be set to False
    # The tracker does not know all subject ids in advance, so it cannot provide
    # the list of still to be processed subjects
    tests_io("Test case subject ids option of ExtractionTracker.", level=0)

    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"),
                                subject_ids=list(range(10)))
    # Check init
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute != "subject_ids":
            assert getattr(tracker, attribute) == value

    # Simulate extraction done
    for attribute in EVENT_BOOLS:
        setattr(tracker, attribute, True)
    tracker.count_total_samples = 10
    tracker.subject_ids.extend(list(range(10)))

    # Test decreasing the number of subjects upon reinstantiation
    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), subject_ids=list(range(5)))
    # Check init
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute not in EVENT_BOOLS + ["num_samples", "count_total_samples", "subject_ids"]:
            assert getattr(tracker, attribute) == value
    # Assert subject ids unchanged
    assert tracker.count_total_samples == 10
    assert tracker.subject_ids == list(range(10))
    # Assert done
    for attribute in EVENT_BOOLS:
        assert getattr(tracker, attribute) == True

    tests_io("Succeeded testing subject ids reduction")
    # Test increasing the number of subjects upon reinstantiation
    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"),
                                subject_ids=list(range(15)))
    # Check init
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute not in EVENT_BOOLS + ["num_samples", "count_total_samples", "subject_ids"]:
            assert getattr(tracker, attribute) == value
    # Assert subject ids unchanged
    assert tracker.count_total_samples == 10
    assert tracker.subject_ids == list(range(10))
    # Assert not done
    for attribute in EVENT_BOOLS:
        assert getattr(tracker, attribute) == False

    tracker.subject_ids.extend(list(range(10, 15)))
    tracker.count_total_samples += 5
    assert tracker.subject_ids == list(range(15))

    # Test setting subject_ids to None
    del tracker
    tracker = ExtractionTracker(storage_path=Path(TEMP_DIR, "progress"), subject_ids=None)
    # Check init
    for attribute, value in TRACKER_STATE.items():
        assert attribute in tracker._progress
        if attribute not in EVENT_BOOLS + ["num_samples", "count_total_samples", "subject_ids"]:
            assert getattr(tracker, attribute) == value
    # Assert subject ids unchanged
    assert tracker.count_total_samples == 15
    assert tracker.subject_ids == list(range(15))

    # Assert not done
    for attribute in EVENT_BOOLS:
        assert getattr(tracker, attribute) == False


if __name__ == "__main__":
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    test_subject_ids_option()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    test_num_subjects_option()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    test_extraction_tracker_basics()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    test_num_samples_option()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    print("All tests passed!")


### FILE: .\tests\test_datasets\test_trackers.py\test_processing_tracker.py ###
# TODO! subject ids may be string or numbers this might have some effect on the tracker
import shutil
from pathlib import Path
from datasets.trackers import PreprocessingTracker
from utils.IO import *
from tests.settings import *

tracker_state = {"subjects": {}, "finished": False, "num_subjects": None}


def test_processing_tracker_basics():
    tests_io("Test case basic capabilities of PreprocessingTracker.", level=0)
    # Create an instance of PreprocessingTracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=None,
                                   subject_ids=None)

    # Test correct initialization
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress
        if attribute == "subjects":
            assert getattr(tracker, attribute) == {"total": 0}
        else:
            assert getattr(tracker, attribute) == value

    # Assignment attriubutes
    tracker.subjects = {
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4
        }
    }
    assert tracker._progress["subjects"] == {
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2,
            "total": 3
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4,
            "total": 6
        },
        "total": 9
    }
    # Test correct assignment
    assert tracker.subjects == {
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2,
            "total": 3
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4,
            "total": 6
        },
        "total": 9
    }
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2"])
    assert not set(["subject_1", "subject_2"]) - set(tracker.subject_ids)

    tests_io("Succeeded testing initialization.")

    # Test correct restoration after assignment
    del tracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=None,
                                   subject_ids=None)
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress

    assert tracker.subjects == {
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2,
            "total": 3
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4,
            "total": 6
        },
        "total": 9
    }
    assert tracker.num_subjects == None
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2"])
    assert not set(["subject_1", "subject_2"]) - set(tracker.subject_ids)
    tests_io("Succeeded testing restoration.")

    # Test custom update implementation
    tracker.subjects.update({"subject_3": {"stay_1": 3, "stay_2": 6}})
    assert tracker._progress["subjects"] == {
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2,
            "total": 3
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4,
            "total": 6
        },
        "subject_3": {
            "stay_1": 3,
            "stay_2": 6,
            "total": 9
        },
        "total": 18
    }
    tracker.subjects == {
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2,
            "total": 3
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4,
            "total": 6
        },
        "subject_3": {
            "stay_1": 3,
            "stay_2": 6,
            "total": 9
        },
        "total": 18
    }
    tracker.subjects.update({"subject_1": {"stay_3": 3}})
    assert tracker._progress["subjects"] == {
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2,
            "stay_3": 3,
            "total": 6
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4,
            "total": 6
        },
        "subject_3": {
            "stay_1": 3,
            "stay_2": 6,
            "total": 9
        },
        "total": 21
    }
    tracker.subjects == {
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2,
            "stay_3": 3,
            "total": 6
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4,
            "total": 6
        },
        "subject_3": {
            "stay_1": 3,
            "stay_2": 6,
            "total": 9
        },
        "total": 21
    }
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2", "subject_3"])
    assert not set(["subject_1", "subject_2", "subject_3"]) - set(tracker.subject_ids)

    del tracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=None,
                                   subject_ids=None)
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress
    assert tracker.subjects == {
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2,
            "stay_3": 3,
            "total": 6
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4,
            "total": 6
        },
        "subject_3": {
            "stay_1": 3,
            "stay_2": 6,
            "total": 9
        },
        "total": 21
    }
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2", "subject_3"])
    assert not set(["subject_1", "subject_2", "subject_3"]) - set(tracker.subject_ids)
    tests_io("Succeeded testing custom __iadd__ implementation.")


def test_finishing_mechanism():
    tests_io("Test case finishing mechanism of PreprocessingTracker.", level=0)
    # Create an instance of PreprocessingTracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=None,
                                   subject_ids=None)

    # Test correct initialization
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress
        if attribute == "subjects":
            assert getattr(tracker, attribute) == {"total": 0}
        else:
            assert getattr(tracker, attribute) == value

    # Simulate processing of two subjects
    tracker.subjects.update({
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4
        }
    })
    # Make sure subjects are set
    assert tracker.subjects == {
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2,
            "total": 3
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4,
            "total": 6
        },
        "total": 9
    }
    # Finish tracker
    tracker.is_finished = True
    assert tracker.is_finished == True
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2"])
    assert not set(["subject_1", "subject_2"]) - set(tracker.subject_ids)
    for key, stays in tracker.subjects.items():
        if key == "total":
            continue
        # Make sure total length is set per subject
        assert stays["total"] == sum([count for key, count in stays.items() if key != "total"])
    tests_io("Succeeded testing total length creation.")

    # Test correct restoration after finishing
    del tracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=None,
                                   subject_ids=None)
    assert tracker.is_finished == True
    for key, stays in tracker.subjects.items():
        if key == "total":
            continue
        # Make sure total length is set per subject
        assert stays["total"] == sum([count for key, count in stays.items() if key != "total"])
    tests_io("Succeeded testing restoration after finishing.")


def test_num_subject_option():
    tests_io("Test case num_subjects option of PreprocessingTracker.", level=0)
    # Create an instance of PreprocessingTracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=2,
                                   subject_ids=None)

    # Test correct initialization
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress
        if attribute == "num_subjects":
            assert getattr(tracker, attribute) == 2
        elif attribute == "subjects":
            assert getattr(tracker, attribute) == {"total": 0}
        else:
            assert getattr(tracker, attribute) == value

    # Simulate processing of two subjects
    tracker.subjects.update({
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2,
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4,
        }
    })
    tracker.is_finished = True

    # Test decrease of num_subjects
    del tracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=1,
                                   subject_ids=None)
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress

    # Make sure state is restored and finished is set to True
    assert tracker.num_subjects == 2
    assert tracker.is_finished == True
    assert len(tracker.subjects) - 1 == 2
    assert tracker.subjects["total"] == 9
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2"])
    assert not set(["subject_1", "subject_2"]) - set(tracker.subject_ids)

    tests_io("Succeeded testing decrease of num_subjects.")

    # Test increase to original num_subjects
    del tracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=2,
                                   subject_ids=None)
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress

    # Make sure state is restored and finished is set to True
    assert tracker.num_subjects == 2
    assert tracker.is_finished == True
    assert len(tracker.subjects) - 1 == 2
    assert tracker.subjects["total"] == 9
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2"])
    assert not set(["subject_1", "subject_2"]) - set(tracker.subject_ids)

    tests_io("Succeeded testing increase to original num_subjects.")

    # Test increase above original num_subjects
    del tracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=3,
                                   subject_ids=None)
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress

    # Make sure state is restored and finished is set to True
    assert tracker.num_subjects == 3
    assert tracker.is_finished == False
    assert len(tracker.subjects) - 1 == 2
    assert tracker.subjects["total"] == 9
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2"])
    assert not set(["subject_1", "subject_2"]) - set(tracker.subject_ids)

    tests_io("Succeeded testing increase above original num_subjects.")

    # Simulate processing of one more subject
    tracker.subjects.update({"subject_3": {"stay_1": 3, "stay_2": 6}})
    tracker.is_finished = True
    assert tracker.num_subjects == 3
    assert tracker.is_finished == True
    assert len(tracker.subjects) - 1 == 3
    assert tracker.subjects["subject_3"]["total"] == 9
    assert tracker.subjects["total"] == 18
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2", "subject_3"])
    assert not set(["subject_1", "subject_2", "subject_3"]) - set(tracker.subject_ids)
    # Test switch to None
    del tracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=None,
                                   subject_ids=None)
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress

    # Make sure state is restored and finished is set to False
    assert tracker.num_subjects == None
    assert tracker.finished == False
    assert len(tracker.subjects) - 1 == 3
    assert tracker.subjects["total"] == 18
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2", "subject_3"])
    assert not set(["subject_1", "subject_2", "subject_3"]) - set(tracker.subject_ids)
    tests_io("Succeeded testing switch to None.")


def test_subject_ids_option():
    tests_io("Test case subject_ids option of PreprocessingTracker.", level=0)
    # Create an instance of PreprocessingTracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=None,
                                   subject_ids=None)
    # Test correct initialization
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress
        if attribute == "subjects":
            assert getattr(tracker, attribute) == {"total": 0}
        else:
            assert getattr(tracker, attribute) == value

    # Simulate processing of two subjects
    tracker.subjects.update({
        "subject_1": {
            "stay_1": 1,
            "stay_2": 2
        },
        "subject_2": {
            "stay_1": 2,
            "stay_2": 4
        }
    })
    tracker.is_finished = True

    # Test truthy subject_ids
    del tracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=None,
                                   subject_ids=["subject_1", "subject_2"])

    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress

    # All necessary subjects are processed
    assert tracker.is_finished == True
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2"])
    assert not set(["subject_1", "subject_2"]) - set(tracker.subject_ids)

    # Test decrease of num_subjects
    del tracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=None,
                                   subject_ids=["subject_1"])
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress
    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2"])
    assert not set(["subject_1", "subject_2"]) - set(tracker.subject_ids)
    assert tracker.is_finished == True
    tests_io("Succeeded testing truthy subject_ids.")

    # Test falsey subject_ids
    del tracker
    tracker = PreprocessingTracker(storage_path=Path(TEMP_DIR, "progress"),
                                   num_subjects=None,
                                   subject_ids=["subject_1", "subject_2", "subject_3"])
    for attribute, value in tracker_state.items():
        assert attribute in tracker._progress

    assert not set(tracker.subject_ids) - set(["subject_1", "subject_2"])
    assert not set(["subject_1", "subject_2"]) - set(tracker.subject_ids)
    assert tracker.is_finished == False
    tests_io("Succeeded testing falsey subject_ids.")


if __name__ == "__main__":
    if TEMP_DIR.exists():
        shutil.rmtree(TEMP_DIR)
    test_processing_tracker_basics()
    if TEMP_DIR.exists():
        shutil.rmtree(TEMP_DIR)
    test_finishing_mechanism()
    if TEMP_DIR.exists():
        shutil.rmtree(TEMP_DIR)
    test_num_subject_option()
    if TEMP_DIR.exists():
        shutil.rmtree(TEMP_DIR)
    test_subject_ids_option()
    tests_io("Succeeded testing PreprocessingTracker.")


### FILE: .\tests\test_datasets\test_trackers.py\test_storage_decorator.py ###
import pytest
import shutil
from pathlib import Path
from utils.IO import *
from tests.settings import *
from storable import storable


@storable
class TestClass:
    num_samples: int = 0
    time_elapsed: float = 0.0
    finished: bool = False
    subjects: dict = {"a": 0, "b": 0}
    names: dict = {"a": "a", "b": "b"}


@storable
class CountTestClass:
    num_samples: int = 0
    time_elapsed: float = 0.0
    finished: bool = False
    subject_ids: list = list()
    subjects: dict = {"a": 0, "b": 0}
    _store_total: bool = True


# Test the storable
def test_storable_basics():
    tests_io("Test case basic capabilities of Storable.", level=0)

    # Test the default values
    assert TestClass.num_samples == 0
    assert TestClass.time_elapsed == 0.0
    assert TestClass.finished == False
    assert TestClass.subjects == {"a": 0, "b": 0}
    assert TestClass.names == {"a": "a", "b": "b"}

    # Test the correct recreation of its attributes with the correct types
    check_dtypes(TestClass)
    tests_io("Succeeded testing initialization.")

    # Test the storage path
    test_instance = TestClass(Path(TEMP_DIR, "progress"))

    assert Path(TEMP_DIR, "progress.dat").is_file()

    # Test assignment
    test_instance.num_samples = 10
    test_instance.time_elapsed = 1.0
    test_instance.finished = True
    test_instance.subjects = {"a": 1, "b": 2}
    test_instance.names = {"a": "b", "b": "a"}

    assert test_instance.num_samples == 10
    assert test_instance.time_elapsed == 1.0
    assert test_instance.finished == True
    assert test_instance.subjects == {"a": 1, "b": 2}
    assert test_instance.names == {"a": "b", "b": "a"}

    check_dtypes(test_instance)
    tests_io("Succeeded testing assignment.")

    # Test restorable assignment
    del test_instance
    test_instance = TestClass(Path(TEMP_DIR, "progress"))

    assert test_instance.num_samples == 10
    assert test_instance.time_elapsed == 1.0
    assert test_instance.finished == True
    assert test_instance.subjects == {"a": 1, "b": 2}
    assert test_instance.names == {"a": "b", "b": "a"}

    check_dtypes(test_instance)

    tests_io("Succeeded testing restoration.")

    # Test __iadd__
    test_instance.num_samples += 10
    test_instance.time_elapsed += 1.1

    assert test_instance.num_samples == 20
    assert test_instance.time_elapsed == 2.1

    check_dtypes(test_instance)

    # Test restorable __iadd__
    del test_instance
    test_instance = TestClass(Path(TEMP_DIR, "progress"))

    assert test_instance.num_samples == 20
    assert test_instance.time_elapsed == 2.1
    check_dtypes(test_instance)

    tests_io("Succeeded testing __iadd__.")


def test_dictionary_iadd():
    tests_io("Test case dictionary iadd.", level=0)
    test_instance = TestClass(Path(TEMP_DIR, "progress"))
    test_instance.subjects == {"a": 0, "b": 0}

    test_instance.subjects += {"a": 1, "b": 2}
    assert test_instance.subjects == {"a": 1, "b": 2}

    test_instance.subjects += {"a": 1}
    assert test_instance.subjects == {"a": 2, "b": 2}

    tests_io("Succeeded testing numerical dictionary iadd.")

    # Test restorable dictionary iadd
    del test_instance
    test_instance = TestClass(Path(TEMP_DIR, "progress"))

    assert test_instance.subjects == {"a": 2, "b": 2}
    tests_io("Succeeded testing restoration of numerical dictionary iadd.")


def test_dictionary_update():
    tests_io("Test case dictionary update.", level=0)
    test_instance = TestClass(Path(TEMP_DIR, "progress"))
    test_instance.names = {"a": {"a": 1, "b": 1}, "b": {"a": 2, "b": 2}}
    test_instance.names.update({"a": {"c": 1, "d": 1}, "c": {"a": 1, "b": 1}})
    assert test_instance.names == {
        "a": {
            "a": 1,
            "b": 1,
            "c": 1,
            "d": 1
        },
        "b": {
            "a": 2,
            "b": 2
        },
        "c": {
            "a": 1,
            "b": 1
        }
    }
    tests_io("Succeeded testing dictionary update.")
    del test_instance
    test_instance = TestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.names == {
        "a": {
            "a": 1,
            "b": 1,
            "c": 1,
            "d": 1
        },
        "b": {
            "a": 2,
            "b": 2
        },
        "c": {
            "a": 1,
            "b": 1
        }
    }
    tests_io("Succeeded testing restoration of dictionary update.")


def test_total_count():
    # Only implemented for single nestation and not for iadd
    tests_io("Test case total count for storable decorator.", level=0)
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))

    test_instance.subjects = {"a": {"a": 1, "b": 2}, "b": {"a": 2, "b": 4}}
    assert test_instance.subjects == {
        "a": {
            "a": 1,
            "b": 2,
            "total": 3
        },
        "b": {
            "a": 2,
            "b": 4,
            "total": 6
        },
        "total": 9
    }
    tests_io("Succeeded dictionary assignment total count.")
    test_instance.subjects.update({"a": {"c": 3}})
    assert test_instance.subjects == {
        "a": {
            "a": 1,
            "b": 2,
            "c": 3,
            "total": 6
        },
        "b": {
            "a": 2,
            "b": 4,
            "total": 6
        },
        "total": 12
    }
    tests_io("Succeeded dictionary nested update total count.")

    test_instance.subjects = {"a": 0, "b": 0}
    assert test_instance.subjects == {"a": 0, "b": 0, "total": 0}
    tests_io("Succeeded dictionary reassignment total count.")

    test_instance.subjects = {}
    assert test_instance.subjects == {"total": 0}
    tests_io("Succeeded dictionary empty reassignment total count.")

    test_instance.subjects.update({"a": 1, "b": 2})
    assert test_instance.subjects == {"a": 1, "b": 2, "total": 3}
    tests_io("Succeeded dictionary simple data type overwrite total count.")

    test_instance.subjects.update({"a": 1, "b": 1})
    assert test_instance.subjects == {"a": 1, "b": 1, "total": 2}

    test_instance.subjects.update({"a": 2, "b": 2})
    assert test_instance.subjects == {"a": 2, "b": 2, "total": 4}

    test_instance.subjects.update({"c": 1, "d": 1})
    assert test_instance.subjects == {"a": 2, "b": 2, "c": 1, "d": 1, "total": 6}

    test_instance.subjects.update({"a": {"a": 1, "b": 2}, "b": {"a": 2, "b": 4}})
    assert test_instance.subjects == {
        "a": {
            "a": 1,
            "b": 2,
            "total": 3
        },
        "b": {
            "a": 2,
            "b": 4,
            "total": 6
        },
        "c": 1,
        "d": 1,
        "total": 11
    }
    tests_io("Succeeded dictionary complex data type overwrite total count.")
    test_instance.subjects.update({"c": 3})
    assert test_instance.subjects == {
        "a": {
            "a": 1,
            "b": 2,
            "total": 3
        },
        "b": {
            "a": 2,
            "b": 4,
            "total": 6
        },
        "c": 3,
        "d": 1,
        "total": 13
    }

    test_instance.subjects.update({"d": {"a": 1, "b": 1}})
    assert test_instance.subjects == {
        "a": {
            "a": 1,
            "b": 2,
            "total": 3
        },
        "b": {
            "a": 2,
            "b": 4,
            "total": 6
        },
        "c": 3,
        "d": {
            "a": 1,
            "b": 1,
            "total": 2
        },
        "total": 14
    }


def test_list_basic():
    tests_io("Test case basic list for storable decorator.", level=0)
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))

    test_instance.subject_ids = ["a", "b", "c"]
    assert test_instance.subject_ids == ["a", "b", "c"]
    assert test_instance._progress["subjects"] == ["a", "b", "c"]

    tests_io("Succeeded testing basic list assignment.")
    del test_instance
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.subject_ids == ["a", "b", "c"]
    assert test_instance._progress["subjects"] == ["a", "b", "c"]
    tests_io("Succeeded testing restoration of basic list assignment.")
    # We are tempering with the cls so need to make sure nothing is permanent
    del test_instance
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.subject_ids == []
    assert test_instance._progress["subjects"] == []
    tests_io("Succeeded testing list non permanency on database deletion.")


def test_list_extend():
    tests_io("Test case extend list for storable decorator.", level=0)
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))

    test_instance.subject_ids.extend(["a", "b", "c"])
    assert test_instance.subject_ids == ["a", "b", "c"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c"]
    tests_io("Succeeded testing extend list assignment.")

    del test_instance
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.subject_ids == ["a", "b", "c"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c"]
    tests_io("Succeeded testing restoration of extend list assignment.")

    test_instance.subject_ids.extend(["d", "e", "f"])
    assert test_instance.subject_ids == ["a", "b", "c", "d", "e", "f"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c", "d", "e", "f"]
    tests_io("Succeeded testing extend list assignment.")

    del test_instance
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.subject_ids == ["a", "b", "c", "d", "e", "f"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c", "d", "e", "f"]
    tests_io("Succeeded testing restoration of extend list assignment.")


def test_list_pop():
    tests_io("Test case pop list for storable decorator.", level=0)
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))

    # Assign and make sure
    test_instance.subject_ids = ["a", "b", "c"]
    assert test_instance.subject_ids == ["a", "b", "c"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c"]

    # Restore to see if permament
    del test_instance
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.subject_ids == ["a", "b", "c"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c"]

    # Now pop
    popper = test_instance.subject_ids.pop()
    assert popper == "c"
    assert test_instance.subject_ids == ["a", "b"]
    assert test_instance._progress["subject_ids"] == ["a", "b"]
    tests_io("Succeeded testing pop list assignment.")
    # Restore to see if permament
    del test_instance
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.subject_ids == ["a", "b"]
    assert test_instance._progress["subject_ids"] == ["a", "b"]
    tests_io("Succeeded testing restoration of pop list assignment.")


def test_list_remove():
    tests_io("Test case remove list for storable decorator.", level=0)
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))

    # Assign and make sure
    test_instance.subject_ids = ["a", "b", "c"]
    assert test_instance.subject_ids == ["a", "b", "c"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c"]

    # Restore to see if permament
    del test_instance
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.subject_ids == ["a", "b", "c"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c"]

    # Now remove
    test_instance.subject_ids.remove("b")
    assert test_instance.subject_ids == ["a", "c"]
    assert test_instance._progress["subject_ids"] == ["a", "c"]
    tests_io("Succeeded testing remove list assignment.")

    # Restore to see if permament
    del test_instance
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.subject_ids == ["a", "c"]
    assert test_instance._progress["subject_ids"] == ["a", "c"]
    tests_io("Succeeded testing restoration of remove list assignment.")


def test_list_append():
    tests_io("Test case append list for storable decorator.", level=0)
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))

    # Assign and make sure
    test_instance.subject_ids = ["a", "b", "c"]
    assert test_instance.subject_ids == ["a", "b", "c"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c"]

    # Restore to see if permament
    del test_instance
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.subject_ids == ["a", "b", "c"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c"]

    # Now append
    test_instance.subject_ids.append("d")
    assert test_instance.subject_ids == ["a", "b", "c", "d"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c", "d"]
    tests_io("Succeeded testing append list assignment.")

    # Restore to see if permament
    del test_instance
    test_instance = CountTestClass(Path(TEMP_DIR, "progress"))
    assert test_instance.subject_ids == ["a", "b", "c", "d"]
    assert test_instance._progress["subject_ids"] == ["a", "b", "c", "d"]
    tests_io("Succeeded testing restoration of append list assignment.")


def check_dtypes(instance):
    assert isinstance(instance.num_samples, int)
    assert isinstance(instance.time_elapsed, float)
    assert isinstance(instance.finished, bool)
    assert isinstance(instance.subjects, dict)
    assert isinstance(instance.names, dict)


if __name__ == "__main__":
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    test_list_extend()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    test_list_append()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    test_list_pop()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    test_list_remove()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    test_total_count()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    TEMP_DIR.mkdir(exist_ok=True, parents=True)
    test_storable_basics()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    TEMP_DIR.mkdir(exist_ok=True, parents=True)
    test_dictionary_iadd()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))
    TEMP_DIR.mkdir(exist_ok=True, parents=True)
    test_dictionary_update()
    if TEMP_DIR.is_dir():
        shutil.rmtree(str(TEMP_DIR))

    tests_io("All tests passed")


### FILE: .\tests\test_pipelines\test_regression_pipeline.py ###
'''
import datasets
import os
import pandas as pd
import json
import shutil
from pathlib import Path
from utils.IO import *
from settings import *
from datasets.readers import ProcessedSetReader
import numpy as np
from pipelines.nn import MIMICPipeline as MIMICNNPipeline
from pipelines.regression import MIMICPipeline as MIMICRegPipeline
from model.sklearn.standard.linear_models import StandardLogReg
from model.tf2.logistic_regression import IncrementalLogReg

settings = json.load(Path(os.getenv("CONFIG"), "test.json").open())


def test_regression_pipeline_ihm():
    data_path = Path(os.getenv("WORKINGDIR"), "test", "temp")
    storage_path = Path(os.getenv("WORKINGDIR"), "test", "temp", "regression", "ihm")
    if storage_path.is_file():
        shutil.rmtree(str(storage_path))
    data_path = datasets.load_data(chunksize=5000000,
                                   source_path=TEST_DATA_GTRAW,
                                   storage_path=data_path,
                                   preprocess=True,
                                   engineer=True,
                                   task="IHM")
    pipeline_config = {
        'framework': 'sklearn',
        'output_type': 'sparse',
        'metrics': ['auc_roc', 'auc_pr'],
        'split_config': {
            'test_fraction_split': 0.2
        },
        'model_name': 'test',
        'root_path': Path(storage_path, "sklearn"),
        'task': 'in_hospital_mortality'
    }
    pipeline = MIMICRegPipeline(StandardLogReg('in_hospital_mortality'), **pipeline_config)
    pipeline.fit(data_path=data_path)

    pipeline_config = {
        'framework': 'tf2',
        'output_type': 'sparse',
        'metrics': ['auc_roc', 'auc_pr'],
        'patience': 4,
        'split_config': {
            'test_fraction_split': 0.2,
            'validation_fraction_split': 0.2
        },
        'model_name': 'test',
        'root_path': Path(storage_path, "tf2"),
        'task': 'in_hospital_mortality'
    }
    pipeline = MIMICRegPipeline(IncrementalLogReg('in_hospital_mortality'), **pipeline_config)
    pipeline.fit(data_path=data_path)


def test_regression_pipeline_phenotyping():
    data_path = Path(os.getenv("WORKINGDIR"), "test", "temp")
    storage_path = Path(os.getenv("WORKINGDIR"), "test", "temp", "regression", "PHENO")
    if storage_path.is_file():
        shutil.rmtree(str(storage_path))
    data_path = datasets.load_data(chunksize=5000000,
                                   source_path=TEST_DATA_GTRAW,
                                   storage_path=data_path,
                                   preprocess=True,
                                   engineer=True,
                                   task="PHENO")
    pipeline_config = {
        'framework': 'sklearn',
        'output_type': 'sparse',
        'metrics': ["auc_roc_micro", "auc_roc_macro"],
        'split_config': {
            'test_fraction_split': 0.2
        },
        'model_name': 'test',
        'root_path': Path(storage_path, "sklearn"),
        'task': "PHENO"
    }
    pipeline = MIMICRegPipeline(StandardLogReg("PHENO"), **pipeline_config)
    pipeline.fit(data_path=data_path)

    pipeline_config = {
        'framework': 'tf2',
        'output_type': 'sparse',
        'metrics': ["auc_roc_micro", "auc_roc_macro"],
        'patience': 4,
        'split_config': {
            'test_fraction_split': 0.2,
            'validation_fraction_split': 0.2
        },
        'model_name': 'test',
        'root_path': Path(storage_path, "tf2"),
        'task': "PHENO"
    }
    pipeline = MIMICRegPipeline(IncrementalLogReg("PHENO"), **pipeline_config)
    pipeline.fit(data_path=data_path)


def test_regression_pipeline_decomp():
    data_path = Path(os.getenv("WORKINGDIR"), "test", "temp")
    storage_path = Path(os.getenv("WORKINGDIR"), "test", "temp", "regression", "DECOMP")
    if storage_path.is_file():
        shutil.rmtree(str(storage_path))
    data_path = datasets.load_data(chunksize=5000000,
                                   source_path=TEST_DATA_GTRAW,
                                   storage_path=data_path,
                                   preprocess=True,
                                   engineer=True,
                                   task="DECOMP")
    """
    pipeline_config = {
        'framework': 'sklearn',
        'output_type': 'sparse',
        'metrics': ['auc_roc', 'auc_pr'],
        'split_config': {
            'test_fraction_split': 0.2
        },
        'model_name': 'test',
        'root_path': Path(storage_path, "sklearn"),
        'task': 'decompensation'
    }
    pipeline = MIMICRegPipeline(StandardLogReg('decompensation'), **pipeline_config)
    pipeline.fit(data_path=data_path)
    """
    pipeline_config = {
        'framework': 'tf2',
        'output_type': 'sparse',
        'metrics': ['auc_roc', 'auc_pr'],
        'patience': 4,
        'split_config': {
            'test_fraction_split': 0.2,
            'validation_fraction_split': 0.2
        },
        'model_name': 'test',
        'root_path': Path(storage_path, "tf2"),
        'task': 'decompensation'
    }
    pipeline = MIMICRegPipeline(IncrementalLogReg('decompensation'), **pipeline_config)
    pipeline.fit(data_path=data_path)


def test_regression_pipeline_los():
    data_path = Path(os.getenv("WORKINGDIR"), "test", "temp")
    storage_path = Path(os.getenv("WORKINGDIR"), "test", "temp", "regression", "LOS")
    if storage_path.is_file():
        shutil.rmtree(str(storage_path))
    data_path = datasets.load_data(chunksize=5000000,
                                   source_path=TEST_DATA_GTRAW,
                                   storage_path=data_path,
                                   preprocess=True,
                                   engineer=True,
                                   task="LOS")
    # One shot optimization not supported for los
    """
    pipeline_config = {
        'framework': 'sklearn',
        'output_type': 'sparse',
        'metrics': ['accuracy'],
        'split_config': {
            'test_fraction_split': 0.2
        },
        'model_name': 'test',
        'root_path': Path(storage_path, "sklearn"),
        'task': 'length_of_stay'
    }
    pipeline = MIMICRegPipeline(StandardLogReg('length_of_stay'), **pipeline_config)
    pipeline.fit(data_path=data_path)
    """

    pipeline_config = {
        'framework': 'tf2',
        'output_type': 'sparse',
        'metrics': ['accuracy'],
        'patience': 4,
        'split_config': {
            'test_fraction_split': 0.2,
            'validation_fraction_split': 0.2
        },
        'model_name': 'test',
        'root_path': Path(storage_path, "tf2"),
        'task': 'length_of_stay'
    }
    pipeline = MIMICRegPipeline(IncrementalLogReg('length_of_stay'), **pipeline_config)
    pipeline.fit(data_path=data_path)


if __name__ == "__main__":
    test_regression_pipeline_ihm()
    test_regression_pipeline_phenotyping()
    test_regression_pipeline_decomp()
    test_regression_pipeline_los()
'''


### FILE: .\tests\test_pipelines\__init__.py ###


### FILE: .\tests\test_preprocessing\test_imputer.py ###
# Test the preprocessing.imputer class using the preprocessing_readers from conftest.py. You can find the imputer use case in preprocessing.discretizer and preprocessing.normalizer
import pytest
from preprocessing.imputers import PartialImputer
from tests.settings import *


@pytest.mark.parametrize("task_name", TASK_NAMES)
def test_imputer_fit_dataset(preprocessing_readers, task_name):
    # Arrange
    reader = preprocessing_readers()
    data = reader.get_random()  # assuming the reader has a read method that returns data
    imp = PartialImputer()  # replace with actual imputer initialization if different

    # Act
    imputed_data = imp.fit_transform(discretized_data)

    # Assert
    # replace with actual assertions based on your expectations
    assert imputed_data is not None
    assert imputed_data.isnull().sum().sum() == 0  # assuming imputer fills all NaN values


def test_imputer_with_normalizer():
    # Arrange
    reader = preprocessing_readers()
    data = reader.read()  # assuming the reader has a read method that returns data
    norm = normalizer.Normalizer()  # replace with actual normalizer initialization if different
    imp = imputer.Imputer()  # replace with actual imputer initialization if different

    # Act
    normalized_data = norm.fit_transform(data)
    imputed_data = imp.fit_transform(normalized_data)

    # Assert
    # replace with actual assertions based on your expectations
    assert imputed_data is not None
    assert imputed_data.isnull().sum().sum() == 0  # assuming imputer fills all NaN values


### FILE: .\tests\test_pytest_suite.py\test_pytest_utils.py ###
import pytest
import pandas as pd
import numpy as np
from tests.pytest_utils.general import assert_dataframe_equals
from copy import deepcopy


def generate_random_frame(n_rows, n_cols, numeric=False):
    # Generate random data of different types for the DataFrame

    data = {
        f"col_{i}":
            np.where(
                np.random.rand(n_rows) < 0.2,
                np.nan,  # Inject NaNs
                np.random.randint(0, 100, size=n_rows) if i % 3 == 0 else np.random.rand(n_rows) *
                100 if i %
                3 == 1 or numeric else np.random.choice(['A', 'B', 'C', 'D'], size=n_rows))
        for i in range(n_cols)
    }

    # Create the DataFrame
    df_random = pd.DataFrame(data)
    return df_random


def generate_random_series(dtypes: pd.Series):
    series_data = pd.Series(index=dtypes.index, dtype=object)

    for col, dtype in dtypes.items():
        dtype_name = dtype.name
        if dtype_name.startswith('int'):
            series_data[col] = np.random.randint(0, 100)
        elif dtype_name.startswith('float'):
            series_data[col] = np.random.random() * 100
        elif dtype_name == 'object':
            series_data[col] = ''.join(
                np.random.choice(list('abcdefghijklmnopqrstuvwxyz')) for _ in range(5))
        elif dtype_name == 'datetime64[ns]':
            series_data[col] = pd.Timestamp('2020-01-01') + pd.to_timedelta(
                np.random.randint(0, 365), unit='D')
        elif dtype_name == 'category':
            categories = ['A', 'B', 'C', 'D']
            series_data[col] = pd.Categorical([np.random.choice(categories)], categories=categories)
        else:
            series_data[col] = np.nan

    return series_data


def test_compare_dataframes():
    nested_mock_columns = {
        "int": {
            "1": [1, 2, 3],
            "2": [1, 4, 3]
        },
        "float": {
            "1": [1.0, 2.5, 3.5],
            "2": [1.0, 2.4, 3.5]
        },
        "string1": {
            "1": ["a", "b", "c"],
            "2": ["a", "B", "c"]
        },
        "string2": {
            "1": ["a", "b", "c"],
            "2": ["a", "bb", "c"]
        },
        "np_int": {
            "1": np.array([1, 2, 3]),
            "2": np.array([1, 4, 3])
        },
        "np_float": {
            "1": np.array([1.0, 2.5, 3.5]),
            "2": np.array([1.0, 2.4, 3.5])
        },
        "pandas_categorical": {
            "1": pd.Categorical(["test", "train", "test"], categories=["test", "train", "val"]),
            "2": pd.Categorical(["test", "train", "val"], categories=["test", "train", "val"])
        }
    }

    # Test single entry differences
    base_frame_df = generate_random_frame(3, 3, True)
    assert_dataframe_equals(base_frame_df, deepcopy(base_frame_df))
    for noramlized_by in ["generated", "groundtruth"]:
        for dtype, columns in nested_mock_columns.items():
            type_1_df = deepcopy(base_frame_df)
            type_1_df["col_3"] = columns["1"]
            type_2_df = deepcopy(base_frame_df)
            type_2_df["col_3"] = columns["2"]
            assert_dataframe_equals(type_1_df,
                                    type_1_df,
                                    normalize_by=noramlized_by,
                                    compare_mode="single_entry")
            assert_dataframe_equals(type_2_df,
                                    type_2_df,
                                    normalize_by=noramlized_by,
                                    compare_mode="single_entry")
            with pytest.raises(AssertionError) as error:
                assert_dataframe_equals(type_1_df,
                                        type_2_df,
                                        normalize_by=noramlized_by,
                                        compare_mode="single_entry")
                assert str(
                    error.value) == "Diffs detected between generated and ground truth files: 1!"

            with pytest.raises(AssertionError) as error:
                assert_dataframe_equals(type_2_df,
                                        type_1_df,
                                        normalize_by=noramlized_by,
                                        compare_mode="single_entry")
                assert str(
                    error.value) == "Diffs detected between generated and ground truth files: 1!"

        # Test single entry dimension mismatch
        long_frame = deepcopy(base_frame_df)
        long_frame.loc[len(long_frame)] = generate_random_series(long_frame.dtypes)
        assert long_frame.shape == (4, 3)
        with pytest.raises(AssertionError) as error:
            assert_dataframe_equals(base_frame_df,
                                    long_frame,
                                    normalize_by=noramlized_by,
                                    compare_mode="single_entry")
            assert str(error.value) == ("Generated and ground truth dataframes do not have"
                                        " the same amount of rows. Generated: 3, Ground truth: 4.")

        short_frame = deepcopy(base_frame_df)
        short_frame = short_frame.drop(short_frame.index[-1])
        assert short_frame.shape == (2, 3)
        with pytest.raises(AssertionError) as error:
            assert_dataframe_equals(base_frame_df,
                                    short_frame,
                                    normalize_by=noramlized_by,
                                    compare_mode="single_entry")
            assert str(error.value) == ("Generated and ground truth dataframes do not have"
                                        " the same amount of rows. Generated: 3, Ground truth: 2.")

        wide_frame = deepcopy(base_frame_df)
        wide_frame["col_3"] = generate_random_series(pd.Series([long_frame.dtypes[0]] * 3))
        assert wide_frame.shape == (3, 4)
        with pytest.raises(AssertionError) as error:
            assert_dataframe_equals(base_frame_df,
                                    wide_frame,
                                    normalize_by=noramlized_by,
                                    compare_mode="single_entry")
            assert str(error.value) == ("Generated and ground truth dataframes do not have the "
                                        "same amount of columns. Generated: 3, Ground truth: 4.")

        narrow_frame = deepcopy(base_frame_df)
        narrow_frame = narrow_frame.iloc[:, :-1]
        assert narrow_frame.shape == (3, 2)
        with pytest.raises(AssertionError) as error:
            assert_dataframe_equals(base_frame_df,
                                    narrow_frame,
                                    normalize_by=noramlized_by,
                                    compare_mode="single_entry")
            assert str(error.value) == ("Generated and ground truth dataframes do not have the "
                                        "same amount of columns. Generated: 3, Ground truth: 2.")

    # Test multi entry differences
    base_frame_df = pd.concat([generate_random_frame(3, 3) for _ in range(3)])
    base_frame_df.index = [f"{idx}_episode{idx}_timeseries.csv" for idx in base_frame_df.index]
    assert_dataframe_equals(base_frame_df, deepcopy(base_frame_df))
    for noramlized_by in ["generated", "groundtruth"]:
        for _, columns in nested_mock_columns.items():
            if "string" in dtype or "categorical" in dtype:
                # This is done for numeric dfs from the feature engine so no objects
                continue
            type_1_df = deepcopy(base_frame_df)
            type_1_df["col_3"] = columns["1"] * 3
            type_2_df = deepcopy(base_frame_df)
            type_2_df["col_3"] = columns["2"] * 3
            assert_dataframe_equals(type_1_df,
                                    type_1_df,
                                    normalize_by=noramlized_by,
                                    compare_mode="multiline")
            assert_dataframe_equals(type_2_df,
                                    type_2_df,
                                    normalize_by=noramlized_by,
                                    compare_mode="multiline")
            with pytest.raises(AssertionError) as error:
                assert_dataframe_equals(type_1_df,
                                        type_2_df,
                                        normalize_by=noramlized_by,
                                        compare_mode="multiline")
                assert str(
                    error.value) == "Diffs detected between generated and ground truth files: 1!"

            with pytest.raises(AssertionError) as error:
                assert_dataframe_equals(type_2_df,
                                        type_1_df,
                                        normalize_by=noramlized_by,
                                        compare_mode="multiline")
                assert str(
                    error.value) == "Diffs detected between generated and ground truth files: 1!"

        # Test single entry dimension mismatch
        long_frame = deepcopy(base_frame_df)
        long_frame.loc[len(long_frame)] = generate_random_series(long_frame.dtypes)
        assert long_frame.shape == (10, 3)
        with pytest.raises(AssertionError) as error:
            assert_dataframe_equals(base_frame_df,
                                    long_frame,
                                    normalize_by=noramlized_by,
                                    compare_mode="multiline")
            assert str(error.value) == ("Generated and ground truth dataframes do not have"
                                        " the same amount of rows. Generated: 9, Ground truth: 10.")

        short_frame = deepcopy(base_frame_df)
        short_frame = short_frame.iloc[0:len(short_frame) - 1]
        assert short_frame.shape == (8, 3)
        with pytest.raises(AssertionError) as error:
            assert_dataframe_equals(base_frame_df,
                                    short_frame,
                                    normalize_by=noramlized_by,
                                    compare_mode="multiline")
            assert str(error.value) == ("Generated and ground truth dataframes do not have"
                                        " the same amount of rows. Generated: 9, Ground truth: 8.")

        wide_frame = deepcopy(base_frame_df)
        wide_frame["col_3"] = generate_random_series(pd.Series([long_frame.dtypes[0]] * 3))
        assert wide_frame.shape == (9, 4)
        with pytest.raises(AssertionError) as error:
            assert_dataframe_equals(base_frame_df,
                                    wide_frame,
                                    normalize_by=noramlized_by,
                                    compare_mode="multiline")
            assert str(error.value) == ("Generated and ground truth dataframes do not have the "
                                        "same amount of columns. Generated: 9, Ground truth: 4.")

        narrow_frame = deepcopy(base_frame_df)
        narrow_frame = narrow_frame.iloc[:, :-1]
        assert narrow_frame.shape == (9, 2)
        with pytest.raises(AssertionError) as error:
            assert_dataframe_equals(base_frame_df,
                                    narrow_frame,
                                    normalize_by=noramlized_by,
                                    compare_mode="multiline")
            assert str(error.value) == ("Generated and ground truth dataframes do not have the "
                                        "same amount of columns. Generated: 9, Ground truth: 2.")


if __name__ == "__main__":
    test_compare_dataframes()


### FILE: .\tests\test_pytest_suite.py\__init__.py ###


